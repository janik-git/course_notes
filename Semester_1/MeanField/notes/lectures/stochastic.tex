\chapter{Stochastic Mean Field Particle Systems}
From now on let the underlying probability space be given by $(\Omega ,\mathcal{F},\mathbb{P})$.
\section{Basics of probability}
\begin{definition}[Brownian Motion]
  Real valued stochastic process $W(*)$ is called a Brownian motion (Wiener process) if 
  \begin{enumerate}
    \item $W(0)= 0  \text{a.s.}$ 
    \item $W(t) - W(s) \sim \mathcal{N}(0,t-s)$ , for all $t,s\ge 0$ 
    \item $\forall  0< t_{1} < t_{2} < \ldots  < t_n$  , $W(t_{1}),W(t_{2})-W(t_{1}),\ldots,W(t_n) - W(t_{n-1})$ are independent 
    \item $W(t)$ is continuous a.s (sample paths) 
  \end{enumerate}
\end{definition}
\begin{remark}[Properties]
  \begin{enumerate}
    \item $\E[W(t)] = 0$  , $\E[W(t)^2] = t$ , for all $t>0$
    \item $\E[W(t)W(s)]=t \land s$ a.s
    \item $W(t) \in  \mathcal{C}^{\gamma }[0,T] $ , $\forall 0 < \gamma < \frac{1}{2}$.
    \item $W(t)$ is nowhere differentiable  a.s
  \end{enumerate}
  additionally Brownian motions are martingales and satisfy the Markov property 
\end{remark}
\begin{definition}[Progressively measurable]
  In addition to adaptation of a Stochastic process $X_t$  we say it is progressively measurable w.r.t $\mathcal{F}_t$
  if $X(s,\omega ) : [0,t] \times  \Omega  \to  \mathbb{R}$ is $\mathcal{B}[0,t] \times  \mathcal{F}_t$ measurable, i.e the t is included 
\end{definition}
\begin{definition}[Simple functions]
  Instead of $\mathcal{H}^{2} $  she uses $\mathbb{L}^2(0,T)$ is the space of all real-valued progressively measurable proceses $G(*)$ s.t
  \begin{align*}
    \E[\int_0^{T} G^2 dt] < \infty 
  .\end{align*}
  define $\mathbb{L}$ analog
\end{definition}
\begin{definition}[Step Process]
  $G \in  \mathbb{L}^2(0,T)$  is called a step process when Partition of $[0,T]$ exists 
  s.t $G(t) = G_k$ for all $t_k \le  t \le t_{k+1}$, $k=0,\ldots ,m-1$ note $G_k$ is $\mathcal{F}_{t_k}$ measurable R.V.
\end{definition}
For step process we define the ito integral as a simple sum 
\begin{definition}[Ito integral for step process]
 Let $G \in  \mathbb{L}^2(0,T)$ be a step process  is given by 
 \begin{align*}
   \int_0^{T}  G(t) dW_t = \sum_{k=0}^{m-1}  G_k(W(t_{k+1}-W(t_k)))
 .\end{align*}
 We take the left value of the process such that we converge against the right integral later
\end{definition}
\begin{remark}
 For two step processes $G,H \in  \mathbb{L}^2(0,T)$  for all $a,b \in  \mathbb{R}$,
 we have linearity (note they may have two different partitions, so we need to make a bigger (finer) one to include both,)\\[1ex]
 \begin{enumerate}
   \item $\int_0^{T} (aG+bH) dW_t = a\int G + b \int  H$ 
   \item $\E[\int_0^{T} G dW_t] = 0$ , because the Brownian motion has EV of 0
   \item $\E[(\int_0^{T} G dW_t )^2 ]= \E[\int_{0}^{T}G^2 dt ]$  Ito isometry
 \end{enumerate}
\end{remark}
\begin{proof}
  First property is just defining a new partition that includes both process.
  Second property, the Idea of the proof is that
  \begin{align*}
    \E[\int_0^{t} G d W_t ] &= \E[\sum_{k=0}^{m-1}G_k(W_{t_{k+1}}-W_{t_k}) ]  \\
                            &= \sum_{k=0}^{m-1}\E[G_k(W(t_{k+1})-W(t_k))]  \\
  .\end{align*}
  Remember $G_k \sim \mathcal{F}_{t_k}$ m.b. and $W(t_{k+1}) - W(t_k)$ is mb. wrt to $W^{t}(t_k) $ future sigma algebra
  and it is independent of $\mathcal{F}_{t_k}$ s.t the expectation decomposes 
  \begin{align*}
    \sum_{k=0}^{m-1}\E[G_k(W(t_{k+1})-W(t_k))]  &= \sum_{k=0}^{m-1} \E[G_k]\E[W(t_{k+1}-W(t_{k}))]  = C*0 = 0 
  .\end{align*}
  For the variance decompose into square and non square terms , the non square terms dissapear by property 2 the rest follows by the variance of 
  Brownian motion, be careful of which terms are actually independent , at leas one will always be independent of the other 3
\end{proof}
\begin{definition}[Ito Formula]
  If $u \in  \mathcal{C}^{2,1}(\mathbb{R} \times  [0,T] ; R) $   then 
  \begin{align*}
    d u(x(t),t) &= \frac{\partial u}{\partial t} (x(t),t) dt + \frac{\partial u}{\partial x}(x(t),t) dx + \frac{1}{2} \frac{\partial^2 u}{\partial x^2}G^2dt \\ 
    &= \frac{\partial u}{\partial x}(x(t),t)GdW_t +(\frac{\partial u}{\partial t}(x(t),t) ) + \frac{\partial u}{\partial x}(x(t),t) F + \frac{1}{2} \frac{\partial ^2 u}{\partial x^2}G^2 dt 
  .\end{align*}
  For $dX = F dt + G dW_t$ for $F \in  L^{1}([0,T]) $ , $G \in  L^2([0,T])$
\end{definition}
\begin{proof}
  The proof is split into the steps
  \begin{enumerate}
   \item  
     \begin{align*}
       d(W_t^2) &= 2W_t  dW_t + dt\\
       d(tW_t) &= W_t dt + t d W_t
     .\end{align*}
    \item 
      \begin{align*}
        dX_i &= F_i dt + G_i dW_t \\
        d(X_{1},X_{2}) &= X_2 dX_1 + X_{1} dX_{2} + G_{1}G_{2}dt \\
      .\end{align*}
    \item 
      \begin{align*}
        u(x) =  x^{m}  \quad m\ge 2
      .\end{align*} 
    \item Itos formula for $u(x,t) = f(x)g(t)$ where $f$ is a polynomial
  \end{enumerate}
  I.e we prove the Ito formula for functions of the form $u(x) = x^{m} $ and then 
 Step 1 : 
 \begin{enumerate}
  \item $d(W_t^2) = 2 W_t dW_t + dt$ which is equivalent to $W^2(t) = W_0^{2} + \int_0^{t} 2 W_s dW_t + \int_0^{t} ds  $ 
  \item $d(tW_t) = W_t dt + t dW_t$ which is equivalent to $tW(t) - sW(0) = \int_0^{t} W_s ds + \int_0^{t} s dW_s  $
 \end{enumerate}
 Actually $\forall \text{ a.e } \omega  \in  \Omega $ : 
 \begin{align*}
   2 \int_0^{t} W_s dW_s = 2 \lim_{n \to \infty}  
 .\end{align*}
 Now we prove (2) $tW_t - 0 W_0  = \int_0^{t} W_s ds + \int_0^{t} s dW_s  $ 
 \begin{align*}
   \int_0^{t} s   dW_s + \int_0^{t} W_s ds = \lim_{n \to \infty} \sum_{k=0}^{n-1} t_k^{n} (W(t_{k+1}^{n} )- W(t_{k}^{n} ))    + \sum_{k=0}^{n-1} W(t_{k+1}^{n}(t_{k+1}^{n} -t_{k}^{n}  ) ) 
 .\end{align*}
 We choose the right value for the second integral
 \begin{align*}
   = \lim_{n\to \infty} \sum_{k=0}^{n-1} (-t_k^{n}W(t_k)^{n} + t_{k+1}^{n}W(t_{k+1}^{n} )   )   =W(t)t - W(0)*0
 .\end{align*}
 Its product rule 
 \begin{align*}
   dX_{1} &= F_{1} dt + G_{1}dW_t \\
   dX_2 &= F_{2} dt + G_{2} dW_t
 .\end{align*}
 This can be written as 
 \begin{align*}
  d(X_{1},X_{2}) = X_{2}dX_{1} + X_{1}dX_{2}
 .\end{align*}
 this shorthand notation actually means 
 \begin{align*}
   X_1(t)X_{2}(t) - X_{1}(0)X_{2}(0) = &\int_0^{t} X_{2}F_{1} ds + \int_0^{t} X_{2}G_{1} dW_s \\
                                       &+ \int_0^{t} X_{1}F_{2} ds + \int_0^{t} X_{1}G_{2}dW_s \\
                                       &+ \int_0^{t}G_{1}G_{2}     
 .\end{align*}
 We prove for $F_{1},F_{2},G_{1},G_{2}$ are time independent 
 \begin{align*}
   &\int_0^{t} (X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2}ds)  \\
   &= \int_0^{t} (X_{2}F_{1}+X_{1}F_{2} + G_{1}G_{2}) ds + \int_0^{t} (X_{2}G_{1} + X_{1}G_{2})  dW_s \\
   &= \int_0^{t} (\underbrace{F_{2}F_{1}s + F_{1}G_{2}W_s}_{=X_{2}} + \underbrace{F_{1}F_{2}s+F_{2}G_{1}W_s}_{=X_{1}} +G_{1}G_{2}) ds \\ 
   &+ \int_0^{t} (F_{2}G_{1}s + G_{2}G_{1}W_s+F_{1}G_{2}s + G_{1}G_{2}W_s) dW_s \\
   &= G_{1}G_{2}t + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1})\underbrace{\left(\int_0^{t} W_s ds + \int_0^{t}  s dW_s\right)}_{tW_t} + 2G_{1}G_{2} \underbrace{\int_0^{t} W_s dW_s}_{W_t^2-t} \\
   &=G_{1}G_{2}t + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1}) tW_t +G_{1}G_{2}W_t^2 - G_{1}G_{2}t \\
   &= X_{1}(t)*X_{2}(t)
 .\end{align*}
 Where $X_{2}(t) = \int_0^{t} F_{2} ds + \int_0^{t}G_{2}dW_s   \myS{Cons.}{=} F_{2}t + G_{2}W_t$ \\[1ex]
 Extend the above idea by considering step processes ($F_{1},F_{2},G_{1},G_{2}$)instead of time independent.
 Step processes are constant (related to time) and we can use the above prove for every time step t and just consider
 a summation over it. \\
 For general $F_{1},F_{2} \in  L^{1}(0,T) , G_{1},G_{2} \in  L^2(0,T) $ then we take step processes
 to approximate them
 \begin{align*}
   &\E[\int_0^{T} \abs{F_{i}^{n} - F_{i}} dt  ] \to  0 \\
   &\E[\int_0^{T} \abs{G_{i}^{n} - G_{i}}^2 dt  ] \to  0 \\
 .\end{align*}
 \begin{align*}
  X_i(t)^{n} = X_i(0) + \int_0^{t} F_i^{n}    ds + \int_0^{t} G_i^{n}  dW_s
 .\end{align*}
 It holds 
 \begin{align*}
   X_{1}^{n}(t) X_{2}(t)^{n}   - X_1(0)X_{2}(0)  &= \int_0^{t} X_2(s)^{n}  F_1^{n}(s) ds + \int_0^{t} X_2(s)G_1(s)^{n}    dW_s \\
                                                 &+ \int_0^{t} X_1^{n}  (s) F_2^{n}(s) ds + \int_0^{t} X_1(s)^{n}G_2^{n}(s) dW_s + \int_0^{t} G_1(s)^{n}G_2^{n}(s) ds
 .\end{align*}
 Only thing left is a convergence result (i.e DCT) sinc the processes are bounded or smth like that. \\[1ex]
 Step 3 if $u(x) = x^{m} , \ \forall  m=0,\ldots$  to prove 
 \begin{align*}
  d(X^{m} ) = mX^{m-1} dX + \frac{1}{2}m(m-1) X^{m-2} G^2dt
 .\end{align*}
 For $m=2$ the result is obtained by the product rule, By induction we prove for arbitrary $m$ \\[1ex]
 \textbf{(IV)} Suppose the statement hold for $m-1$ \\
 \textbf{(IS)} $m-1 \to  m$
 \begin{align*}
   d(X^{m} ) &= d(X*X^{m-1} ) = XdX^{m-1}  + X^{m-1} dx + (m-1)X^{m-2} G^2 dt\\
             &\myS{IS}{=} X(m-1)X^{m-2}  dx + X*\frac{1}{2}(m-1)(m-2)X^{m-3}G^2dt +X^{m-1} dx + (m-1)X^{m-2}G^2 dt  \\
             &= mX^{m-1} dx + (m-1)(\frac{m}{2}-1+1)X^{m-2}G^2   dt \\
             &= \underbrace{mX^{m-1}}_{\partial_x u}dx + \frac{1}{2} \underbrace{m(m-1)X^{m-2}}_{\partial^2_x u} G^2dt  
 .\end{align*}
 Now $u(x) =x^{m}  $
 \begin{align*}
  dX  =F dt + G dW_t
 .\end{align*}
 Step 4  If $u(x,t) = f(x)g(t)$ where  $f$ is a polynomial
  \begin{align*}
    d(u(x,t)) &= d(f(x)g(t)) = f(x)dg + g df(x) + G*0 dt  \\
              &\myS{S3}{=} f(x)g'(t) dt + gf'(x) dx + \frac{1}{2}gf^{''}(x) G^2 dt 
  .\end{align*}
  Itos formula is true for $f(x)g(t)$, it should thus also be true for functions $u(x,t) = \sum_{i=1}^{m} g^{i}(t)f^{i}(x) $ \\[1ex]
  Step 5: if   $u \in \mathcal{C}^{2,1} $ then we know there exists a sequence of polynomials $f^{i}(x) $  s.t
  \begin{align*}
    u_n(x,t) = \sum_{i=1}^{n} f^{i}(x)g^{i}(t)
  .\end{align*}
  Then $u_n \to u$ uniformly for any compact set $K \subset  \mathbb{R} \times  [0,T] $, we can thus apply 
  Itos formula for each of the $u_n$ and take the limit term wise  
\end{proof}
\begin{remark}
 One can get the existence of the polynomial sequence by using Hermetian polynomials 
 \begin{align*}
  H_n(x) = (-1)^{n}  e^{\frac{x^2}{2}} \frac{d^{n}}{d x^{n} } e^{-\frac{x^2}{2}} 
 .\end{align*}
\end{remark}
\begin{exercise}
  If $u \in  \mathcal{C}^{\infty} $  , $\frac{\partial u}{\partial x} \in  \mathcal{C}_b$ then prove Step 4 $\implies$ Step 5 \\[1ex]
  \textit{Use Taylor expansion and use the uniform convergence of the Taylor series on any compact support }
\end{exercise}
\begin{remark}[Multi Dimensional Brownian Motion]
 Multi dimensional  Brownian motion 
 \begin{align*}
  W(t) &= (W^{1}(t),\ldots ,W^{m}(t)  ) \in  \mathbb{R}^{m} \\
 .\end{align*}
 In each direction we should have a 1 dimensional Brownian motion and any two directions should be independent.
 We use the natural filtration $\mathcal{F}_t = \sigma(W(s) ; 0\le s\le t)$
\end{remark}
\begin{definition}[Multi-Dimensional Ito's Integral]
  We the define the $n$ dimensional integral for $G \in  L^{2}_{n*m}([0,T]) $ , $G_{ij} \in  L^{2}([0,T])$ $1\le i\le n \ , \ 1 \le j \le m$
  \begin{align*}
    \int_0^{T} G d W_t = \begin{pmatrix} \vdots \\ \int_0^{T} G_{ij} d W^{j}_t \\ \vdots    \end{pmatrix}_{n \times 1}
  .\end{align*}
  With the Properties 
  \begin{align*}
    \E[\int_0^{T} G dW_t ] &= 0  \\
    \E[(\int_0^{T} G dW_t )^2] = \E[\int_0^{T} \abs{G}^2 dt ]
  .\end{align*}
  Where $\abs{G}^2 = \sum_{i,j}^{n,m} \abs{G_{ij}}^2 $ 
\end{definition}
\begin{definition}[Multi-Dimensional Ito process]
 We define the $n$ dimensional Ito process as  
 \begin{align*}
   X(t) &= X(s) + \int_s^{t} F_{n \times  1}(r) dr   + \int_0^{t} G_{n \times  m}(r) dW_{m \times  1}(r)  \\
   dX^{i} &= F^{i} dt + \sum_{j=1}^{m} G^{ij} dW_t^i      \qquad 1\le i \le n
 .\end{align*}
\end{definition}
\begin{theorem}[Multi Dimensional Ito's formula]
  We define the $n$ dimensional Ito's formula as $u \in  \mathcal{C}^{2,1}(\mathbb{R}^{n} \times [0,T],\mathbb{R} ) $
  \begin{align*}
    du(x(t),t) &= \frac{\partial u}{\partial t}(x(t),t) dt + \triangledown u(x(t),t) * dx(t) \\
               &+ \frac{1}{2} \sum \frac{\partial ^2 u}{\partial x_i \partial x_j}(x(t),t) \sum_{l=1}^{m}  G^{il} G^{il}dt 
  .\end{align*}
\end{theorem}
\begin{prop}
  For real valued processes $X_{1},X_{2}$
 \begin{align*}
  \begin{cases}
    dX_{1} &= F_{1} dt + G_1 dW_1 \\
    d X_2 &= F_{2} dt + G_{2} dW_2
  \end{cases} \implies d(X_{1},X_{2}) = XdX_{2} + X_{2}dX_{1} + \sum_{k=1}^{m} G_1^{k} G_2^{k} dt   
 .\end{align*} 
\end{prop}
Working with SDEs relies on a lot of notational rules as seen in the differential notation is just shorthand for the Integral form 
\begin{definition}
 Formal multiplication rules for SDEs
 \begin{align*}
   (dt)^2 = 0 \ , \ dt dW^{k} = 0 \ , \ dW^{k}dW^{l} = \delta_{kl} dt    
 .\end{align*}
\end{definition}
Using this notation we can simply itos formula as follows 
\begin{align*}
  du(X,t) &= \frac{\partial u}{\partial t} dt + \triangledown_x u*dX + \frac{1}{2}\sum_{i,j=1}^{n} \frac{\partial ^2  u}{\partial x_i \partial x_j}   dX^{i}dX^{j}   \\ 
          &= \frac{\partial u}{\partial t} dt + \sum_{i=1}^{n} \frac{\partial u}{\partial X^{i} } F^{i} dt + \sum_{i=1}^{n} \frac{\partial u }{\partial X_i}      \sum_{i=1}^{m} G^{ik} d W_k   \\
          &+  \frac{1}{2} \sum_{i,j=1}^{n}  \frac{\partial ^2  u}{\partial x_i \partial x_j} \left(F^{i} dt + \sum_{k=1}^{m} G^{ik} dW_k   \right)\left( F^{j} dt + \sum_{l=1}^{m} G^{i;} dW_l    \right)   \\
          &= (\frac{\partial u}{\partial t} + F*\triangledown u + \frac{1}{2} H*D^2 u) dt + \sum_{i=1}^{n} \frac{\partial u}{\partial x_i} \sum_{k=1}^{m} G^{ik} dW_{k}
.\end{align*}
Where 
\begin{align*}
  dX^{i} &= F^{i} dt + \sum_{k=1}^{m}  G^{ik} dW_k   \\
  H_{ij} &= \sum_{k=1}^{m} G^{ik}G^{jk}  \ , \ A *B = \sum_{i,j=1}^{m} A_{ij} B_{ij} 
.\end{align*}
Typical example 
\begin{align*}
G^{T}G = \sigma  I_{n \times  n} 
.\end{align*}
\begin{example}
 If $F$ and $G$ are deterministic 
 \begin{align*}
   dX_{n \times  1} F(t)_{n \times  1} dt + G_{n \times  m} dW_t{m \times  1}
 .\end{align*}
 Then for arbitrary test function $u \in  \mathcal{C}_0^{\infty}(\mathbb{R}^{n} ) $ then by Ito's formula 
 \begin{align*}
   u(x(t)) - u(x(0)) &= \int_0^{t} \triangledown u (x(s)) * F(s) ds + \int_0^{t}  \frac{1}{2}(G^{T}G ) : D^2u(x(s)) ds \\
                     &+ \int_0^{t} \triangledown u(x(s)) * G(s) dW_{s} 
 .\end{align*}
 Let $\mu(s,*)$  be the law of $X(s)$ then we take the expectation of the above integral 
 \begin{align*}
   &\int_{\mathbb{R}^{n} } u(x) d\mu(s,x) - \int_{\mathbb{R}^{n} } u(x) d\mu_0(x)  =  \int_{0}^{t} \int_{\mathbb{R}^{n} }  \triangledown u(x) * F(s) d\mu(s,x)  \\
   &+ \int_0^{t} \int_{\mathbb{R}^{n} }  \frac{1}{2}(G^{T}(s)G(s)) : D^2 u(x) * d\mu (s,x) + 0
 .\end{align*}
\end{example}
\begin{definition}[Parabolic Operator]
 \begin{align*}
   \partial_t u  - \frac{1}{2} \sum_{i,j=1}^{n}  D_{ij} (\sum_{k=1}^{m}  G^{ik}G^{kj}  )\mu  + \triangledown * (F \mu )  = 0 
 .\end{align*} 
\end{definition}
\begin{example}
  If $F=0$  $m=n$ and $G=\sqrt{2}I_{n \times  n} $ then 
  \begin{align*}
    dX = \sqrt{2} dW_t 
  .\end{align*}
  And the law of $X$ , $\mu $ fulfills the heat equation
  \begin{align*}
    \mu_t = \triangle \mu  = 0
  .\end{align*}
\end{example}
How does this all translate to our Mean field Limit, consider a particle system given  by 
\begin{align*}
  \begin{cases}  
  dX_N &= F(X_N) dt + \sqrt{2} dW_{dN \times 1} \\
  d x_i &=  \frac{1}{N} \sum K(x_i,x_j) dt + \sqrt{2} dW_t^1  \qquad 1\le i\le N \ N\to \infty\\
  x_i(0)    &= x_{0,i} \\
  \mu_N(t) &= \frac{1}{N} \sum_{i=1}^{N} \delta_{x_i(t)} 
  \end{cases}
.\end{align*}
At time $t = 0$ the $x_i$ are independent random variables at any time $t>0$ they are dependent and the particles have joint law 
\begin{align*}
  (x_{1}(t),\ldots ,x_N(t)) \sim  u(x_{1},\ldots ,x_n)
.\end{align*}
Where $u \in  \mu(\mathbb{R}^{dN}$ by Ito's formula we get for arbitrary test function $\forall  \phi  \in  \mathcal{C}_0^{\infty}(\mathbb{R}^{dN} ) $ 
\begin{align*}
  \phi(X_N) &=  \phi(X_N(0)) + \int_0^{t} \triangledown _{dN}\phi  *\begin{pmatrix} \vdots \\ \frac{1}{n} \sum_{j=1}^{N} K(x_i,x_j) \\ \vdots  \end{pmatrix}  X_N \\
            &+ \int_0^{t}  \triangle_{X_N} \phi  dt + \int_0^{t} \sqrt{2} \triangledown \phi  dW_t^{i} 
.\end{align*}
Taking the expectation on both sides, then the last term disappears by definition of Ito processes 
\begin{align*}
  \partial_t - \sum_{i=1}^{N} \triangle_i u  + \sum_{i=1}^{N} \triangledown_{x_i} \left( \frac{1}{N} \sum_{j=1}^{N} K(x_i,x_j) u \right)  = 0
.\end{align*}
Now consider the Mean-Field-Limit, if the joint particle law can be rewritten as the tensor product of a single $\overline{u}$ 
\begin{align*}
 u(x_{1},\ldots ,x_N)  = \overline{u}^{\otimes N}  
.\end{align*}
the equation simplifies
\begin{align*}
  \partial_t - \sum_{i=1}^{N} \triangle_i u  + \sum_{i=1}^{N} \triangledown_{x_i} \left( \overline{u}^{\otimes N}k \star \overline{u}(x_i)   \right)  = 0
.\end{align*}
\section{Solving Stochastic Differential Equations}
The setup of the following section will be the following 
\begin{definition}[Basic Setup]
 We consider the probability space $(\Omega ,\mathcal{F},\mathbb{P})$, With a $m-D$ dimensional Brownian motion $W(*)$. Let $X_0$ be an 
 $n-D$ dimensional random variable independent of $W(0)$, then our Filtration is given by
 \begin{align*}
  \mathcal{F}_t = \sigma(X_{0}) \cup \sigma(W(s) , 0\le s\le t)
 .\end{align*}
\end{definition}
Note for better understanding the dimensions will be included in the following definition, but we generally leave them out.
\begin{definition}[SDE]
 Given the above basic setup we are trying to solve equations of the type 
 \begin{align*}
  \begin{cases}
    d\underbrace{X_t}_{n \times 1} &= \underbrace{b}_{n \times 1}(X_t,t) dt + \underbrace{B}_{n \times m}(X_t,t) d\underbrace{W_t}_{m \times 1} \quad 0\le t\le T \\
    X_{t}\rvert_{t=0} &= X_{0} \quad X \ : \ (t,\omega ) \to  \mathbb{R}^{n} 
  \end{cases}
 .\end{align*}
 Where 
 \begin{align*}
   b &: (x,t) \in  \mathbb{R}^{n} \times [0,T] \to \mathbb{R}^{n}   \\
   B &: (x,t) \in  \mathbb{R}^{n} \times [0,T] \to  M^{nxm}  
 .\end{align*}
\end{definition}
\begin{remark}
 The differential equation should always be understood as the Integral equation 
 \begin{align*}
  X_t - X_{0} = \int_0^{t}  b(X_s,s) ds + \int_0^{t} B(X_s,s) dW_s 
 .\end{align*}
\end{remark}
\begin{definition}[Solution]
 We say an $\mathbb{R}^{n} $-valued stochastic process $X(*)$ is a solution of the SDE if 
 \begin{enumerate}
  \item $X_t$ is progressively measurable w.r.t $\mathcal{F}_t$
  \item (drift) $F\coloneqq b(X_t,t) \in  L_{n}^{1}([0,T]) \ \Leftrightarrow \  \int_0^{t} \E[F_s] ds < \infty $ 
  \item (diffusion) $G\coloneqq B(X_t,t) \in  L_{n \times m}^{2}([0,T]) \Leftrightarrow \  \int_0^{t} \E[\abs{G_s}^2] ds < \infty $ 
 \end{enumerate}
\end{definition}
Reminder that (1) implies that for any given $t \in  [0,T]$ $X_t$ is random variable measurable with respect to $\mathcal{F}_t$\\[1ex]
The goal from now on is to prove the existence and uniqueness of such solutions, we formulate the following theorem,
one should remember that if the diffusion term $B(X_t,t)$ is 0 then we get a unique solution iff $b(X_t,t)$ is Lipschitz
\begin{theorem}[Existence and Solution]
  Suppose $b \ : \ \mathbb{R}^{n} \times  [0,T] \to  \mathbb{R}^{n}  $ and $B : \mathbb{R}^{n} \times  [0,T] \to  M^{n \times m}  $,
  then we get the necessary condition that they are continuous and (globally) Lipschitz continuous with respect to $x$ i.e $\exists  \ L > 0$ such that
  for arbitrary $\forall  \ x , \tilde{x} \in  \mathbb{R}^{n}  $ and $t \in  [0,T]$ it holds
  \begin{align*}
    \abs{b(x,t) - b(\tilde{x},t )} +  \abs{B(x ,t) - B(\tilde{x},t )} \le  L \abs{x - \tilde{x} }
  .\end{align*}
  and  the linear growth condition 
  \begin{align*}
    \abs{b(x,t)} + \abs{B(x,t)} \le  L(1+\abs{x}) 
  .\end{align*}
  The initial data $X_{0}$ should be square integrable $x_{0} \in  L_n^{2}(\Omega ) $ and that $X_0$ is independent of $W^{t}(0)$ \\[1ex]
  Whenever the above conditions hold then there exists a unique solution $X \in  L^2_n([0,T])$ of the SDE.
\end{theorem}
\begin{proof}
  We begin by proving the uniqueness of solution. \\[1ex] 
  Suppose we have two solutions $X$ and $\tilde{X} $ to the SDE then we need to show that they are indistinguishable,
  then by using the definition of a solution 
  \begin{align*}
    X_t - \tilde{X}_t = \int_0^{t} (b(X_s,s) - b(\tilde{X}_s,s )) ds + \int_0^{t} B(X_s,s) - B(\tilde{X}(s),s )  dW_s
  .\end{align*}
  If the diffusion term were to be 0 we could use a Grönwall type inequality and get the uniqueness. 
  To work with the diffusion term we consider the square of the above and apply Itos isometry. Note that
  generally $\abs{a+b}^2 \nleqslant  (a^2+b^2)$  which is why we need the extra 2.
  \begin{align*}
    \abs{X_t - \tilde{X}_t}^2 \le  2\abs{\int_0^{t} (b(X_s,s) - b(\tilde{X}_s,s )) ds}^2 + \abs{\int_0^{t} B(X_s,s) - B(\tilde{X}(s),s )  dW_s}^2
  .\end{align*}
  Now consider the following 
  \begin{align*}
    \E[\abs{X_t-\tilde{X}_t}^2] &\le 2\E[\abs{\int_0^{t} \abs{b(X_s,s) - b(\tilde{X}_s,x )} ds}^2 ]  \\
                                & \qquad + 2 \E[\abs{\int_0^{t} B(X_s,s) - B(\tilde{X}_s,s ) dW_s}^2]\\
                                &\myS{Hold.}{\le } 2t \E[\int_0^{t} \abs{b(X_s,s) - b(\tilde{X})s,s )}^2 ds ] + 2\E[\int_0^{t} \abs{B(X_s,s)-B(\tilde{X}_s,s )}^2 ds ] \\
                                &\myS{Lip.}{\le } 2(t+1)L^2 E[\int_0^{t} \abs{X_s-\tilde{X}_s }^2 ds ]\\
                                &= 2(t+1)L^2 \int_0^{t} E[\abs{X_s-\tilde{X}_s }^2  ]ds\\
  .\end{align*}
 Where the following Hoelders inequality was used 
 \begin{align*}
   \left( \int_0^{t} 1 \abs{f} ds  \right)^2 &\le  \left( \int_0^{t} 1^2 ds  \right)^{\frac{1}{2}*2}*\left( \int_0^{t} \abs{f}^2 ds  \right)^{\frac{1}{2}*2}  \\
                                             &\le t \int_0^{t} \abs{f}^2 ds 
 .\end{align*}
 Now by Gronwalls inequality we have 
 \begin{align*}
   \E[\abs{X_t-\tilde{X}_t }^2] = 0
 .\end{align*}
 i.e $X_t$ and $\tilde{X}_t $ are modifications of each other and it remains to show that they are actually
 indistinguishable.\\[1ex]
 Define 
 \begin{align*}
  A_t = \{ \omega  \in  \Omega  \ | \ \abs{X_t - \tilde{X}_t  } > 0\}   \qquad \P(A_t) = 0
 .\end{align*}
 \begin{align*}
   \P(\max_{t \in  \mathbb{Q} \cap [0,T]} \abs{X_t-\tilde{X}_t } > 0 ) = \P(\bigcup_{k=1}^{\infty} A_{t_k} ) = 0
 .\end{align*}
 Now since $X_t(\omega )$ is continuous in $t$ we can extend the maximum over the entire interval $[0,T]$ 
 \begin{align*}
   \max_{t \in  \mathbb{Q} \cap [0,T]} \abs{X_t - \tilde{X}_t} = \max_{t \in  [0,T]} \abs{X_t - \tilde{X}_t}
 .\end{align*}
 Then the probability over the entire interval must also be 0 
 \begin{align*}
   \P(\max_{t \in  [0,T]} \abs{X_t - \tilde{X}_t} >0)  = 0 \quad \text{ i.e. } X_t = \tilde{X}_t \ \forall  t \text{ a.s.} 
 .\end{align*}
 This concludes the uniqueness proof, for existence as in the deterministic case we use Picard iteration.\\[1ex]
 Define the Picard iteration by  
 \begin{align*}
   X_t^{0} &= X_0  \\
           &\vdots\\
   X_t^{n+1} &= X_0 + \int_0^{t} b(X_s^{n},s ) ds + \int_0^{t} B(X_s^{n},s ) dW_s   
 .\end{align*}
 Let $d(t)^{n} = \E[\abs{X_t^{n+1}-X_t^{n}}^2] $ we claim that by induction $d^{n}(t) \le  \frac{(Mt)^{n+1} }{(n+1)!} $ for some $M>0$.\\
  \textbf{IA:} For $n=0$ we have
  \begin{align*}
    d(t)^{0} = \E[\abs{X_t^1-X_t^0}^2] &\le  \E[2 (\int_0^{t} b(X_0,s) ds )^2 + 2 (\int_0^{t} B(X_0,s )dW_s )^2]  \\
                                       &\le  2t \E[\int_0^{t} L^2(1+X_{0}^2) ds ] + 2\E[\int_0^{t} L^2(1+X_{0}) ds ] \\
                                       &\le  tM \qquad \text{ where } M\ge 2L^2(1+\E[X_{0}^2]) +2L^2(1+T)
  .\end{align*}
  \textbf{IV:} suppose the assumption holds for $n-1 \in  \mathbb{N}$\\
  \textbf{IS:} Take $n-1 \to n$ then 
  \begin{align*}
    d^{n}(t) &= \E[\abs{X_t^{n+1} - X^{n}_t }^2] \le  2 L^2 T \E[\int_0^{t} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ]  + 2L^2\E[\int_0^{t} \abs{X_s^{n} - X_s^{n-1}  }^2  ds] \\
             &\myS{IV}{\le } 2L^2(1+T) \int_0^{t} \frac{(Ms)^n}{n!} ds \\
             &= 2L^2(1+t) \frac{M^n}{(n+1)!} t^{n+1} \le \frac{M^{n+1}t^{n+1}}{(n+1)!} 
  .\end{align*}
  Issue now is that because of $\Omega $ we cannot use completeness to argue the convergence, instead
  we use a similar argument to the uniqueness proof. 
  \begin{align*}
    &\E[\max_{0\le t \le T} \abs{X^{n+1}_t - X^{n}_t  }^2] \\
    &\le \E[\max_{0\le t\le T} 2\abs*{\int_0^{t} b(X_s^{n},s )-b(X_s^{n-1},s ) ds}^2 + 2\abs*{\int_0^{t}B(X_s^{n},s )-B(X_s^{n-1},s ) dW_s}^2] \\
    &\le 2TL^2 \E[\int_0^{T} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ] + 2\E[\max_{0\le t\le T} \abs*{\int_0^{t} B(X_s^{n},s )- B(X_s^{n-1},s ) ds W_s}]\\
    &\le  2TL^2 \E[\int_0^{T} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ] + 8\E[\int_0^{T} \abs{B(X_s^{n},s  )-B(X_s^{n-1},s )}^2 ds ]\\
    &\le C*\E[\int_0^{T} \abs{X_s^{n}-X_s^{n-1}  }^2 ds ]
  .\end{align*}
  Where we used the following Doobs martingales Lp inequality 
  \begin{align*}
    \E[\max_{0\le s\le t} \abs{X(s)}^{p} ] \le  (\frac{p}{p-1})^{p} \E[\abs{X(t)}^{p}  ] 
  .\end{align*}
  By Picard iteration we know the distance $d^{n}(t) = \E[\abs{X_s^{n}-X_s^{n-1}  }^2] $ is bounded by 
  \begin{align*}
    C*\E[\int_0^{T} \abs{X_s^{n}-X_s^{n-1}  }^2 ds ] &= C* \int_0^{T} \E[\abs{X_s^{n} - X_s^{n-1}  }^2] ds \\
                                                     &\le \int_0^{T} \frac{(Mt)^{n} }{(n)!} \\
                                                     &= C \frac{M^{n} T^{n+1} }{(n+1)!}
  .\end{align*}
  Further more we get with a Markovs inequality
  \begin{align*}
    \P(\underbrace{\max_{0\le t\le T} \abs{X_t^{n+1}-X_t^n}^2 > \frac{1}{2^{n} }}_{A_n}) &\le 2^{2n} \E[\max_{0\le t\le T} \abs{X_t^{n+1}-X_t^n}^2]\\
                                                                      &\le 2^{2n} \frac{CM^{n}T^{n+1}  }{(n+1)!} 
  .\end{align*}
  Then by Borel-Cantelli we know
  \begin{align*}
    \sum_{n=0}^{\infty} \P(A_n) \le  C \sum_{n=0}^{\infty}2^{2n} \frac{(MT)^{n} }{(n+1)!}    <\infty \implies \P(\bigcap_{n=0}^{\infty} \bigcup_{m=n}^{\infty} A_m ) = 0
   .\end{align*}
   Define by a telescope argument 
   \begin{align*}
     X_t^n = X_t^0 + \sum_{j=1}^{n-1}(X_t^{j+1} - X_t^{j}) 
   .\end{align*}
   Then the above converges to 
   \begin{align*}
     X_t = X_0 + \int_0^{t} b(X_s,s) ds + \int_0^{t} B(X_s,s)  dW_s
   .\end{align*}
\end{proof}
\begin{remark}
 Uniqueness in a stochastic sense means that for two solution $X,\tilde{X} $ we have
 \begin{align*}
   \P(X(t) = \tilde{X}(t), \ \forall  t \in  [0,T] ) = 1 \Leftrightarrow \max_{0\le t \le T} \abs{x(t)-\tilde{x}(t) }  = 0 \text{ a.s.}
 .\end{align*}
 I.e they are indistinguishable 
\end{remark}
As a small side note we consider this example to distinguish modifications and indistinguishable.
\begin{example}
  First note that for any $t \in  [0,T]$  we have the following inclusion 
  \begin{align*}
    A \coloneqq \{X(t)=\tilde{X}(t) ,\ \forall \ t \in  [0,T] \}  \subset  \{X(t) = \tilde{X}(t) \} \coloneqq  A_t   
  .\end{align*}
  i.e 
  \begin{align*}
   \P(A) \le  P(A_t) 
  .\end{align*}
  Such that indistinguishability implies modification where modification means 
  \begin{align*}
    \forall \ t  \in  [0,T] \ : \ \P(A_t) = 1
  .\end{align*}
\end{example}

