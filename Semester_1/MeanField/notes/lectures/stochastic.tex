\chapter{Stochastic Mean Field Particle Systems}
From now on let the underlying probability space be given by $(\Omega ,\mathcal{F},\mathbb{P})$.
\section{Basics of probability}
\begin{definition}[Brownian Motion]
  Real valued stochastic process $W(*)$ is called a Brownian motion (Wiener process) if 
  \begin{enumerate}
    \item $W(0)= 0  \text{a.s.}$ 
    \item $W(t) - W(s) \sim \mathcal{N}(0,t-s)$ , for all $t,s\ge 0$ 
    \item $\forall  0< t_{1} < t_{2} < \ldots  < t_n$  , $W(t_{1}),W(t_{2})-W(t_{1}),\ldots,W(t_n) - W(t_{n-1})$ are independent 
    \item $W(t)$ is continuous a.s (sample paths) 
  \end{enumerate}
\end{definition}
\begin{remark}[Properties]
  \begin{enumerate}
    \item $\E[W(t)] = 0$  , $\E[W(t)^2] = t$ , for all $t>0$
    \item $\E[W(t)W(s)]=t \land s$ a.s
    \item $W(t) \in  \mathcal{C}^{\gamma }[0,T] $ , $\forall 0 < \gamma < \frac{1}{2}$.
    \item $W(t)$ is nowhere differentiable  a.s
  \end{enumerate}
  additionally Brownian motions are martingales and satisfy the Markov property 
\end{remark}
\begin{definition}[Progressively measurable]
  In addition to adaptation of a Stochastic process $X_t$  we say it is progressively measurable w.r.t $\mathcal{F}_t$
  if $X(s,\omega ) : [0,t] \times  \Omega  \to  \mathbb{R}$ is $\mathcal{B}[0,t] \times  \mathcal{F}_t$ measurable, i.e the t is included 
\end{definition}
\begin{definition}[Simple functions]
  Instead of $\mathcal{H}^{2} $  she uses $\mathbb{L}^2(0,T)$ is the space of all real-valued progressively measurable proceses $G(*)$ s.t
  \begin{align*}
    \E[\int_0^{T} G^2 dt] < \infty 
  .\end{align*}
  define $\mathbb{L}$ analog
\end{definition}
\begin{definition}[Step Process]
  $G \in  \mathbb{L}^2(0,T)$  is called a step process when Partition of $[0,T]$ exists 
  s.t $G(t) = G_k$ for all $t_k \le  t \le t_{k+1}$, $k=0,\ldots ,m-1$ note $G_k$ is $\mathcal{F}_{t_k}$ measurable R.V.
\end{definition}
For step process we define the ito integral as a simple sum 
\begin{definition}[Ito integral for step process]
 Let $G \in  \mathbb{L}^2(0,T)$ be a step process  is given by 
 \begin{align*}
   \int_0^{T}  G(t) dW_t = \sum_{k=0}^{m-1}  G_k(W(t_{k+1}-W(t_k)))
 .\end{align*}
 We take the left value of the process such that we converge against the right integral later
\end{definition}
\begin{remark}
 For two step processes $G,H \in  \mathbb{L}^2(0,T)$  for all $a,b \in  \mathbb{R}$,
 we have linearity (note they may have two different partitions, so we need to make a bigger (finer) one to include both,)\\[1ex]
 \begin{enumerate}
   \item $\int_0^{T} (aG+bH) dW_t = a\int G + b \int  H$ 
   \item $\E[\int_0^{T} G dW_t] = 0$ , because the Brownian motion has EV of 0
   \item $\E[(\int_0^{T} G dW_t )^2 ]= \E[\int_{0}^{T}G^2 dt ]$  Ito isometry
 \end{enumerate}
\end{remark}
\begin{proof}
  First property is just defining a new partition that includes both process.
  Second property, the Idea of the proof is that
  \begin{align*}
    \E[\int_0^{t} G d W_t ] &= \E[\sum_{k=0}^{m-1}G_k(W_{t_{k+1}}-W_{t_k}) ]  \\
                            &= \sum_{k=0}^{m-1}\E[G_k(W(t_{k+1})-W(t_k))]  \\
  .\end{align*}
  Remember $G_k \sim \mathcal{F}_{t_k}$ m.b. and $W(t_{k+1}) - W(t_k)$ is mb. wrt to $W^{t}(t_k) $ future sigma algebra
  and it is independent of $\mathcal{F}_{t_k}$ s.t the expectation decomposes 
  \begin{align*}
    \sum_{k=0}^{m-1}\E[G_k(W(t_{k+1})-W(t_k))]  &= \sum_{k=0}^{m-1} \E[G_k]\E[W(t_{k+1}-W(t_{k}))]  = C*0 = 0 
  .\end{align*}
  For the variance decompose into square and non square terms , the non square terms dissapear by property 2 the rest follows by the variance of 
  Brownian motion, be careful of which terms are actually independent , at leas one will always be independent of the other 3
\end{proof}
\begin{definition}[Ito Formula]
  
\end{definition}
\begin{proof}
 Step 1 : 
 \begin{enumerate}
  \item $d(W_t^2) = 2 W_t dW_t + dt$ which is equivalent to $W^2(t) = W_0^{2} + \int_0^{t} 2 W_s dW_t + \int_0^{t} ds  $ 
  \item $d(tW_t) = W_t dt + t dW_t$ which is equivalent to $tW(t) - sW(0) = \int_0^{t} W_s ds + \int_0^{t} s dW_s  $
 \end{enumerate}
 Actually $\forall \text{ a.e } \omega  \in  \Omega $ : 
 \begin{align*}
   2 \int_0^{t} W_s dW_s = 2 \lim_{n \to \infty}  
 .\end{align*}
 Now we prove (2) $tW_t - 0 W_0  = \int_0^{t} W_s ds + \int_0^{t} s dW_s  $ 
 \begin{align*}
   \int_0^{t} s   dW_s + \int_0^{t} W_s ds = \lim_{n \to \infty} \sum_{k=0}^{n-1} t_k^{n} (W(t_{k+1}^{n} )- W(t_{k}^{n} ))    + \sum_{k=0}^{n-1} W(t_{k+1}^{n}(t_{k+1}^{n} -t_{k}^{n}  ) ) 
 .\end{align*}
 We choose the right value for the second integral
 \begin{align*}
   = \lim_{n\to \infty} \sum_{k=0}^{n-1} (-t_k^{n}W(t_k)^{n} + t_{k+1}^{n}W(t_{k+1}^{n} )   )   =W(t)t - W(0)*0
 .\end{align*}
 Its product rule 
 \begin{align*}
   dX_{1} &= F_{1} dt + G_{1}dW_t \\
   dX_2 &= F_{2} dt + G_{2} dW_t
 .\end{align*}
 This can be written as 
 \begin{align*}
  d(X_{1},X_{2}) = X_{2}dX_{1} + X_{1}dX_{2}
 .\end{align*}
 this shorthand notation actually means 
 \begin{align*}
   X_1(t)X_{2}(t) - X_{1}(0)X_{2}(0) = &\int_0^{t} X_{2}F_{1} ds + \int_0^{t} X_{2}G_{1} dW_s \\
                                       &+ \int_0^{t} X_{1}F_{2} ds + \int_0^{t} X_{1}G_{2}dW_s \\
                                       &+ \int_0^{t}G_{1}G_{2}     
 .\end{align*}
 We prove for $F_{1},F_{2},G_{1},G_{2}$ are time independent 
 \begin{align*}
   &\int_0^{t} (X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2}ds)  \\
   &= \int_0^{t} (X_{2}F_{1}+X_{1}F_{2} + G_{1}G_{2}) ds + \int_0^{t} (X_{2}G_{1} + X_{1}G_{2})  dW_s \\
   &= \int_0^{t} (\underbrace{F_{2}F_{1}s + F_{1}G_{2}W_s}_{=X_{2}} + \underbrace{F_{1}F_{2}s+F_{2}G_{1}W_s}_{=X_{1}} +G_{1}G_{2}) ds \\ 
   &+ \int_0^{t} (F_{2}G_{1}s + G_{2}G_{1}W_s+F_{1}G_{2}s + G_{1}G_{2}W_s) dW_s \\
   &= G_{1}G_{2}t + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1})\underbrace{\left(\int_0^{t} W_s ds + \int_0^{t}  s dW_s\right)}_{tW_t} + 2G_{1}G_{2} \underbrace{\int_0^{t} W_s dW_s}_{W_t^2-t} \\
   &=G_{1}G_{2}t + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1}) tW_t +G_{1}G_{2}W_t^2 - G_{1}G_{2}t \\
   &= X_{1}(t)*X_{2}(t)
 .\end{align*}
 Where $X_{2}(t) = \int_0^{t} F_{2} ds + \int_0^{t}G_{2}dW_s   \myS{Cons.}{=} F_{2}t + G_{2}W_t$ \\[1ex]
 Extend the above idea by considering step processes ($F_{1},F_{2},G_{1},G_{2}$)instead of time independent.
 Step processes are constant (related to time) and we can use the above prove for every time step t and just consider
 a summation over it. \\
 For general $F_{1},F_{2} \in  L^{1}(0,T) , G_{1},G_{2} \in  L^2(0,T) $ then we take step processes
 to approximate them
 \begin{align*}
   &\E[\int_0^{T} \abs{F_{i}^{n} - F_{i}} dt  ] \to  0 \\
   &\E[\int_0^{T} \abs{G_{i}^{n} - G_{i}}^2 dt  ] \to  0 \\
 .\end{align*}
 \begin{align*}
  X_i(t)^{n} = X_i(0) + \int_0^{t} F_i^{n}    ds + \int_0^{t} G_i^{n}  dW_s
 .\end{align*}
 It holds 
 \begin{align*}
   X_{1}^{n}(t) X_{2}(t)^{n}   - X_1(0)X_{2}(0)  &= \int_0^{t} X_2(s)^{n}  F_1^{n}(s) ds + \int_0^{t} X_2(s)G_1(s)^{n}    dW_s \\
                                                 &+ \int_0^{t} X_1^{n}  (s) F_2^{n}(s) ds + \int_0^{t} X_1(s)^{n}G_2^{n}(s) dW_s + \int_0^{t} G_1(s)^{n}G_2^{n}(s) ds
 .\end{align*}
 Only thing left is a convergence result (i.e DCT) sinc the processes are bounded or smth like that. \\[1ex]
 Step 3 if $u(x) = x^{m} , \ \forall  m=0,\ldots$  to prove 
 \begin{align*}
  d(X^{m} ) = mX^{m-1} dX + \frac{1}{2}m(m-1) X^{m-2} G^2dt
 .\end{align*}
 For $m=2$ the result is obtained by the product rule, By induction we prove for arbitrary $m$ \\[1ex]
 \textbf{(IV)} Suppose the statement hold for $m-1$ \\
 \textbf{(IS)} $m-1 \to  m$
 \begin{align*}
   d(X^{m} ) &= d(X*X^{m-1} ) = XdX^{m-1}  + X^{m-1} dx + (m-1)X^{m-2} G^2 dt\\
             &\myS{IS}{=} X(m-1)X^{m-2}  dx + X*\frac{1}{2}(m-1)(m-2)X^{m-3}G^2dt +X^{m-1} dx + (m-1)X^{m-2}G^2 dt  \\
             &= mX^{m-1} dx + (m-1)(\frac{m}{2}-1+1)X^{m-2}G^2   dt \\
             &= \underbrace{mX^{m-1}}_{\partial_x u}dx + \frac{1}{2} \underbrace{m(m-1)X^{m-2}}_{\partial^2_x u} G^2dt  
 .\end{align*}
 Now $u(x) =x^{m}  $
 \begin{align*}
  dX  =F dt + G dW_t
 .\end{align*}
\end{proof}
\section{Bad K}
\section{Convergence}
