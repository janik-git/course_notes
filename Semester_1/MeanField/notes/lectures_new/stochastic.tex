\chapter{MEAN FIELD LIMIT FOR SDE SYSTEM}
\section{Basics On Probability Theory}
This section is dedicated to a small review of basic concepts 
in probability theory in preparations of SDE's
\subsection{Probability Spaces and Random Variables}
\begin{definition}[$\sigma$-Algebra]
 Let $\Omega $  be a given set, then a $\sigma-$algebra $\mathcal{F}$ on $\Omega $ is a
 family of subsets of $\Omega $ s.t.
 \begin{enumerate}
   \item $\emptyset \in  \mathcal{F}$
   \item $F \in  \mathcal{F} \implies F^{c} \in  \mathcal{F} $
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ countable, then 
     \begin{align*}
       A = \bigcup_{j=1}^{\infty} A_j \in \mathcal{F}
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Measure Space]
 A tuple $(\Omega ,\mathcal{F})$  is called a measurable space. The elements of $\mathcal{F}$ are 
 called measurable sets 
\end{definition}
\begin{definition}[Probability Measure]
 A probability measure $\P$ on $(\Omega ,\mathcal{F})$  is a function 
 \begin{align*}
   \P \ : \ \mathcal{F} \to [0,1]
 .\end{align*}
 s.t.
 \begin{enumerate}
   \item $\P(\emptyset) = 0$ , $\P(\Omega ) = 1$
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ s.t. $A_i \cap A_j = \emptyset \ \forall  i \neq j$  then
     \begin{align*}
       \P(\bigcup_{j=1}^{\infty} A_j ) = \sum_{j=1}^{\infty} \P(A_j) 
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Probability Space]
 The triple $(\Omega ,\mathcal{F},\P)$  is called a probability space. $F \in  \mathcal{F}$ is called
 event. We say the probability space $(\Omega ,\mathcal{F},\P)$ is complete, if $\mathcal{F}$ contains all zero-measure sets i.e.
 if 
 \begin{align*}
  \inf \{\P(F) \ : \ F \in  \mathcal{F},G \subset  F\}  = 0
 .\end{align*}
 then $G \in  \mathcal{F}$ and $\P(G) = 0$. Without loss of generality we use in this lecture $(\Omega ,\mathcal{F},\P)$
 as complete probability space
\end{definition}
\begin{definition}[Almost Surely]
  If for some $F \in  \mathcal{F}$ it holds $\P(F) = 1$ the we say that $F$ happens with 
  probability 1 or almost surely (a.s.)
\end{definition}
\begin{remark}
 Let $\mathcal{H}$  be a family of subsets of $\Omega$, then there exists a smallest $\sigma-$algebra of 
 $\Omega$ called $\mathcal{U}_{\mathcal{H}}$ with 
 \begin{align*}
   \mathcal{U}_{\mathcal{H}} = \bigcap_{\substack{\mathcal{H} \subset \mathcal{U} \\ \mathcal{H} \ \sigma-\text{alg.}}} \mathcal{H}  
 .\end{align*}
\end{remark}
\begin{example}
  The $\sigma-$algebra generated by a topology $\tau $ of $\Omega$ , $\mathcal{U}_{\tau } \triangleq \mathcal{B}$ is called 
  the Borel $\sigma-$algebra, the elements $B \in  \mathcal{B}$ are called Borel sets.
\end{example}
\begin{definition}[Measurable Functions]
 Let $(\Omega ,\mathcal{F},\P)$  be a probability space, a function 
 \begin{align*}
  Y \ : \ \Omega  \to \mathbb{R}^{d} 
 .\end{align*}
 is called measurable if and only if 
 \begin{align*}
  Y^{-1}(B) \in  \mathcal{F} 
 .\end{align*}
 holds for all $B \in  \mathcal{B}$ or equivalent for all $B \in  \tau $
\end{definition}
\begin{example}
 Let $X : \Omega  \to  \mathbb{R}^{d} $  be a given function, then the $\sigma-$algebra $\mathcal{U}(X)$ generated by X is 
 \begin{align*}
  \mathcal{U}(X) = \{X^{-1}(B) \ : \ B \in  \mathcal{B} \}  
 .\end{align*}
\end{example}
\begin{lemma}[Doob-Dynkin]
 If $X,Y \ : \ \Omega  \to \mathbb{R}^{d} $  are given then $Y$ is $\mathcal{U}(X)$ measurable if and only if 
 there exists a Boreal measurable function $g \ : \ \mathbb{R}^{d} \to  \mathbb{R}^{d}  $ such that 
 \begin{align*}
  Y = g(x)
 .\end{align*}
\end{lemma}
\begin{exercise}
  Proof the above lemma
\end{exercise}
From now on we denote $(\Omega ,\mathcal{F},\P)$ as a given probability space.
\begin{definition}[Random Variable]
 A random variable $X \ : \ \Omega  \to \mathbb{R}^{d} $  is a $\mathcal{F}-$measurable function.
 Every random variable induces a probability measure or $\mathbb{R}^{d} $ 
 \begin{align*}
  \mu_X(B) = \P(X^{-1}(B) ) \quad \forall B \in  \mathcal{B}
 .\end{align*}
This measure is called the distribution of X
\end{definition}
\begin{definition}[Expectation and Variance]
 Let $X$ be a random variable, if 
 \begin{align*}
   \int_{\Omega } \abs{X(\omega )}d\P(\omega ) < \infty
 .\end{align*}
 then 
 \begin{align*}
   \E[X] = \int_{\Omega } X(\omega ) d\P(\omega ) =  \int_{\mathbb{R}^{d} }x d\mu_X(x)
 .\end{align*}
 is called the expectation of $X$ (w.r.t. $\P$) \\[1ex]
 \begin{align*}
   \V[X] = \int_{\Omega } \abs{X - \E[X]}^2 d\P(\omega )
 .\end{align*}
 is called variance and there exists the simple relation 
 \begin{align*}
   \V[X] = \E[\abs{X-\E[X]}^2] = \E[\abs{X}^2] - \E[X]^2
 .\end{align*}
\end{definition}
\begin{remark}
 If $f : \mathbb{R}^{d} \to  \mathbb{R} $ measurable and 
 \begin{align*}
   \int_{\Omega } \abs{f(X(\omega ))} d\P(\Omega ) <\infty
 .\end{align*}
 then 
 \begin{align*}
   \E[f(x)] = \int_{\Omega }f(X(\omega ))d\P(\omega ) = \int_{\mathbb{R}^{d} }f(x) d\mu_X(x)
 .\end{align*}
\end{remark}
\begin{definition}[$L^p$ spaces]
  Let $X : \Omega  \to  \mathbb{R}^{d} $  be a random variable and $p \in [1,\infty)$.
  With the norm 
  \begin{align*}
    \|X\|_p = \|X\|_{L^{p}(\P ) } = \left( \int_{\Omega} \abs{X(\omega )}^{p} d\P(\omega )  \right)^{\frac{1}{p}} 
  .\end{align*}
  If $p=\infty$ 
  \begin{align*}
    \|X\|_{\infty} = \inf \{N \in  \mathbb{R} : \abs{X(\omega )} \le  N \text{ a.s.}\}  
  .\end{align*}
  the space $L^{p}(\P ) = L^{p}(\Omega ) = \{X \ : \ \Omega  \to  \mathbb{R}^{d}  \ | \ \|X\|_p \le \infty \}    $ is a Banach space.
\end{definition}
\begin{remark}
 If $p=2$ then $L^{2}(\P) $ is a Hilbert space with inner product 
 \begin{align*}
   \braket{X,Y} = \E[X(\omega )*Y(\Omega )] = \int_{\Omega }X(\omega )*Y(\omega )d\P(\omega )
 .\end{align*}
\end{remark}
\begin{definition}[Distribution Functions]
 Note for $x,y \in  \mathbb{R}^{d} $  we write $x\le y$ if $x_i \le  y_i$ for $\forall i$
 \begin{enumerate}
   \item $X: (\Omega ,\mathcal{F},\P) \to \mathbb{R}^{d} $ is a random variable the ints distribution function $F_x : \mathbb{R}^{d} \to [0,1] $
     is defined by 
     \begin{align*}
      F_X(x) = \P(X\le x) \quad x \in  \mathbb{R}^{d} 
     .\end{align*}
    \item If $X_{1},\ldots ,X_m : \Omega \to \mathbb{R}^{d} $ are random variables, their joint distribution function is
      \begin{align*}
        F_{X_{1},\ldots ,X_m} &: (\mathbb{R}^{d} )^m \to [0,1]\\
        F_{X_{1},\ldots ,X_M} &= \P(X_{1}\le x_{1},\ldots ,X_m\le x_m) \quad \forall x_i \in \mathbb{R}^{d} 
      .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Density Function Of X]
 If there exists a non-negative function $f(x) \in  L^{1}(\mathbb{R}^{d} ; \mathbb{R} ) $   such that 
 \begin{align*}
   F(x) = \int_{-\infty}^{x_{1}}  \ldots \int_{-\infty}^{x_n} f(y) dy \quad y = (y_{1},\ldots ,y_n)
 .\end{align*}
 then f is called density function of $X$ and 
 \begin{align*}
  \P(X^{-1}(B) ) = \int_B f(x) dx \quad \forall  B \in  \mathcal{B}
 .\end{align*}
\end{definition}
\begin{example}
 Let $X$ be random variable with density function  $x \in  \mathbb{R}$
 \begin{align*}
  f(x) = \frac{1}{\sqrt{2\pi \sigma ^2}e^{-\frac{\abs{x-m}^2}{2\sigma ^2}}  }
 .\end{align*}
 then we say that $X$ has a Gaussian (or Normal) distribution with mean m and variance $\sigma^2$ and write
 \begin{align*}
  X \sim \mathcal{N}(m,\sigma^2)
 .\end{align*}
 Obviously 
 \begin{align*}
   \int_{\mathbb{R}} xf(x) dx = m \qquad \int_{\mathbb{R}}\abs{x-m}^2f(x) dx = \sigma^2
  .\end{align*}
\end{example}
\begin{definition}[Independent Events]
  Events $A_{1},\ldots ,A_{n} \in  \mathcal{F}$ are called independent if $\forall 1 \le k_{1} < \ldots  < k_m \le  n$ it holds 
  \begin{align*}
    \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
  .\end{align*}
\end{definition}
\begin{definition}[Independent $\sigma-$Algebra]
 Let $\mathcal{F}_j \subset  \mathcal{F}$   be $\sigma-$algebras for $j=1,2,\ldots $. Then we say $\mathcal{F}_j$ are independent if 
 for $\forall 1 \le k_{1}<k_{2}<\ldots <k_m$ and $\forall A_{k_j} \in  \mathcal{F}_{k_j}$ it holds
 \begin{align*}
  \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
 .\end{align*}
\end{definition}
\begin{definition}[Independent Random Variables]
 We say random variables $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $  are independent if 
 for $\forall  B_{1},\ldots ,B_{m} \subset  \mathcal{B}$ in $\mathbb{R}^{d} $ it holds 
 \begin{align*}
   \P(X_{j_{1}}\in B_{j_{1}},\ldots, X_{j_k}\in B_{j_k} ) = \P(X_{j_{1}} \in  B_{j_{1}})\ldots \P(X_{j_k} \in  B_{j_k})
 .\end{align*}
 which is equivalent to proving that $\mathcal{U}(X_{1}),\ldots ,\mathcal{U}(X_k)$ are independent
\end{definition}
\begin{theorem}
 $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $ are independent if and only if 
 \begin{align*}
   F_{X_{1},\ldots ,X_m}(x_{1},\ldots ,x_m) =F_{X_{1}}(x_{1})\ldots F_{x_m}(x_m) \quad \forall  x_i \in \mathbb{R}^{d} 
 .\end{align*}
\end{theorem}
\newpage
\begin{theorem}
  If $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independent and $\E[\abs{X_i}] < \infty$ then 
  \begin{align*}
    \E[\abs{X_{1},\ldots ,X_m}]<\infty
  .\end{align*}
  and 
  \begin{align*}
    \E[X_{1}\ldots X_m] = \E[X_{1}]\ldots \E[X_m]
  .\end{align*}
\end{theorem}
\begin{theorem}
  $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independent and $\V[X_i] <\infty$ then 
  \begin{align*}
    \V[X_{1} + \ldots  + X_m] = \V[X_{1}] + \ldots  + \V[X_m]
  .\end{align*}
\end{theorem}
\begin{exercise}
 Proof the above theorems
\end{exercise}
\subsection{Borel Cantelli}
\begin{definition}
 Let $A_{1},\ldots ,A_m \in \mathcal{F}$   then the set 
 \begin{align*}
   \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty} A_m = \{\omega \in \Omega  \ : \ \omega  \text{ belongs to infinite many} A_{m}\text{'s}\}  
 .\end{align*}
 is called $A_m$ infinitely often or $A_m$ i.o.
\end{definition}
\begin{lemma}[Borel Cantelli]\label{borel_cantelli}
 If $\sum_{m=1}^{\infty} \P(A_m) < \infty  $  then $\P(A_ \text{ i.o. }) = 0$
\end{lemma}
\begin{proof}
 By definition we have 
 \begin{align*}
   \P(A_m \text{ i.o. }) \le  \P(\bigcup_{m=n}^{\infty} ) \le \sum_{m=n}^{\infty} \P(A_m)  \xrightarrow{m\to \infty} 0
 .\end{align*}
\end{proof}
\begin{definition}[Convergence In Probability]
  We say a sequence of random variables $(X_k)_{k=1}^{\infty} $  converges in probability to $X$ if 
  for $\forall  \epsilon > 0$
  \begin{align*}
    \lim_{k\to \infty} \P(\abs{X_k - X} > \epsilon ) = 0 
  .\end{align*}
\end{definition}
\begin{theorem}[Application Of Borel Cantelli]
 If $X_k \to  X$  in probability, then there exists 
 a subsequence $(X_{k_j})_{j=1}^{\infty} $ such that 
 \begin{align*}
   X_{k_j}(\omega ) \to X(\omega ) \text{ for almost every } \omega \in  \Omega 
 .\end{align*}
 This means that $\P(\abs{X_{k_j}-X}\to 0) = 1$
\end{theorem}
\begin{proof}
  For $\forall  j \ \exists k_j$  with $k_j < k_{j+1} \to  \infty$ s.t.
  \begin{align*}
    \P(\abs{X_{k_j} - X} > \frac{1}{j}) \le \frac{1}{j^2}
  .\end{align*}
  then 
  \begin{align*}
    \sum_{j=1}^{\infty} \P(\abs{X_{k_j}-X} > \frac{1}{j}) = \sum_{j=1}^{\infty} \frac{1}{j^2}   < \infty
  .\end{align*}
  Let $A_j = \{\omega  \ : \ \abs{X_{k_j}-X} > \frac{1}{j}\}  $ then by \nameref{borel_cantelli} we have $\P(A_j\text{ i.o.}) = 0$ s.t.
  \begin{align*}
    \forall  \omega  \in  \Omega  \ \exists J \text{ s.t. } \forall  j>J
  .\end{align*}
  it holds 
  \begin{align*}
    \abs{X_{k_j}(\omega ) - X(\omega )} \le  \frac{1}{j}
  .\end{align*}
\end{proof}
\subsection{Strong Law Of Large Numbers}
\begin{definition}
 A sequence of random variables $X_{1},\ldots ,X_n$  is called identically distributed if 
 \begin{align*}
   F_{X_{1}}(x)= F_{X_{2}}(x) = \ldots  = F_{X_n}(x) \quad \forall x \in  \mathbb{R}^{d}  
 .\end{align*}
 If additionally $X_{1},\ldots ,X_n$ are independent then we say they are identically-independent-distributed i.i.d
\end{definition}
\begin{theorem}[Strong Law Of Large Numbers]
 Let $X_{1},\ldots ,X_N$  be a sequence of i.i.d integrable random variables on the same probability
 space $(\Omega ,\mathcal{F},\P)$ then 
 \begin{align*}
   \P(\lim_{N \to  \infty} \frac{X_1 + \ldots  + X_N}{N} = \E[X_i]) = 1
 .\end{align*}
 where $\E[X_i] = \E[X_j]$ 
\end{theorem}
\begin{proof}
  Suppose for simplicity $\E[X^{4} ] < \infty$  for $\forall  i = 1,2,\ldots $.
  Then without loss of generality we may assume $\E[X_i] = 0$ otherwise we use $X_i - \E[X_i]$ as our new sequence.
  Consider 
  \begin{align*}
    \E[(\sum_{i=1}^{N} X_i )^{4} ] = \sum_{i,j,k,l}\E[X_iX_jX_kX_l]
  .\end{align*}
  If $i \neq j ,k,l$ then because of independence it follows that 
  \begin{align*}
    \E[X_iX_jX_kX_l] = \E[X_i]\E[X_jX_kX_l] = 0
  .\end{align*}
  Then 
  \begin{align*}
    \E[(\sum_{i=1}^{N}X_i )^{4} ] &= \sum_{i=1}^{N}\E[X_i^{4} ]  + 3 \sum_{i\neq j}\E[X_i^2X_j^2] \\
                                  &= N\E[X_{1}^{4} ] + 3(N^2-N)\E[X_{1}^2]^2 \\
                                  &\le  N^2C
  .\end{align*}
  Therefore for fixed $\epsilon > 0$
  \begin{align*}
    \P(\abs{\frac{1}{N} \sum_{i=1}^{N}X_i } \ge \epsilon) &= \P(\abs{\sum_{i=1}^{N}X_i }^{4}  \ge (\epsilon N)^{4} )\\
                                                          &\myS{Mrkv.}{\le} \frac{1}{(\epsilon N)^{4} } \E[\abs{\sum_{i=1}^{N} X_i }^{4} ]\\
                                                          &\le  \frac{C}{\epsilon^4}\frac{1}{N^2}
  .\end{align*}
  Then by \nameref{borel_cantelli} we get 
  \begin{align*}
    \P(\abs{\frac{1}{N} \sum_{i=1}^{N} X_i } \ge  \epsilon \text{ i.o.}) = 0
  .\end{align*}
  because 
  \begin{align*}
    \sum_{N=1}^{\infty} \P(A_N)  = \sum_{N=1}^{\infty} \frac{C}{\epsilon^4}\frac{1}{N^2}  < \infty
  .\end{align*}
  where  
  \begin{align*}
    A_N = \{\omega  \in  \Omega  \ : \ \abs{\frac{1}{N}\sum_{i=1}^{N}X_i } \ge  \epsilon\}  
  .\end{align*}
  Now we take $\epsilon = \frac{1}{k}$ then the above gives 
  \begin{align*}
    \lim_{N\to \infty} \sup \frac{1}{N} \sum_{i=1}^{N}  X_i(\omega ) \le \frac{1}{k}
  .\end{align*}
  holds except for $\omega  \in  B_k$ with $\P(B_k) = 0$. Let $B = \bigcup_{k=1}^{\infty} B_k $ then $\P(B) = 0$ and
  \begin{align*}
    \lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^{N} X_i(\omega ) = 0  \text{ a.e.}
  .\end{align*}
\end{proof}
\subsection{Conditional Expectation}
\begin{definition}
  Let $Y$ be random variable, then $\E[X|  Y] $ is defined as a $\mathcal{U}(Y)-$measurable random variable
  s.t for $\forall  A \in \mathcal{U}(Y)$ it holds 
  \begin{align*}
    \int_A X d\P = \int_A \E[X|Y] d\P
  .\end{align*}
\end{definition}
\begin{definition}
 Let $(\Omega ,\mathcal{F},\P)$  be a probability space and $\mathcal{ U} \subset  \mathcal{F}$ be a $\sigma-$algebra,
 if $X  : \Omega  \to  \mathbb{R}^{d} $ is an integrable random variable then $\E[X |\mathcal{U}]$  is
 defined as a random variable on $\Omega$ s.t. $\E[X | \mathcal{U}]$ is $\mathcal{U}-$measurable and for $\forall A \in  \mathcal{U}$
 \begin{align*}
   \int_A X d\P  = \int_A \E[X | \mathcal{U}] d \P
 .\end{align*}
\end{definition}
\begin{exercise}
 Proof the following equalities  
 \begin{enumerate}
   \item $\E[X|Y] = \E[X | \mathcal{U}]$
   \item $\E[\E[X|\mathcal{U}]] = \E[X]$
   \item $\E[X] = \E[X | \mathcal{W}]$, where $\mathcal{W} = \{\emptyset,\Omega \}  $
 \end{enumerate}
\end{exercise}
\begin{remark}
 One can define the conditional probability similarly. Let $\mathcal{V} \subset  \mathcal{U}$  be a $\sigma-$algebra 
 then for $A \in  \mathcal{U}$ the conditional probability is defined as follows
 \begin{align*}
   \P(A | \mathcal{V}) = \E[\cha_A | \mathcal{V}]
 .\end{align*}
Note the equivalent notation $\chi_A \equiv \cha_A$
\end{remark}
\begin{theorem}
 Let $X$ be an integrable random variable, then for all $\sigma-$algebras $\mathcal{U} \subset  \mathcal{F}$  the 
 conditional expectation $\E[X | \mathcal{U}]$ exists and is unique up to $\mathcal{U}-$measurable sets of probability
 zero
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
\begin{theorem}[Properties Of Conditional Expectation]
 \begin{enumerate}
   \item If $X$ is $\mathcal{U}-$measurable then $\E[X|\mathcal{U}] = X$ a.s.
   \item $\E[aX + bY|\mathcal{U}] = a\E[X|\mathcal{U}] + b \E[Y|\mathcal{Y}]$
   \item If $X$ is $\mathcal{U}-$measurable and $XY$ is integrable then 
     \begin{align*}
       \E[XY|\mathcal{U}] = X \E[Y|\mathcal{Y}]
     .\end{align*}
   \item If $X$ is independent of $\mathcal{U}$ then $\E[X|\mathcal{U}] = \E[X]$ a.s.
   \item If $\mathcal{W} \subset  \mathcal{U}$ are two $\sigma-$algebras then 
     \begin{align*}
       \E[X|\mathcal{W}] = \E[\E[X|\mathcal{U}]|\mathcal{W}] = \E[\E[X|\mathcal{W}]|\mathcal{U}] \text{ a.s.}
     .\end{align*}
   \item If $X\le Y$ a.s. then $\E[X|\mathcal{U}] \le \E[Y\mathcal{U}]$  a.s.
 \end{enumerate} 
\end{theorem}
\begin{exercise}
 Proof the above properties  
\end{exercise}
\begin{lemma}[Conditional Jensen's Inequality]
  Suppose $\phi  : \mathbb{R}\to \mathbb{R}$ is convex and $\E[\phi(x)] < \infty$ then
  \begin{align*}
    \phi(\E[X|\mathcal{U}]) \le \E[\phi(X)|\mathcal{U}]
  .\end{align*} 
\end{lemma}
\begin{exercise}
 Proof the above Lemma 
\end{exercise}
\subsection{Stochastic Processes And Brownian Motion}
\begin{definition}[Stochastic Process]
 A stochastic process is a parameterized collection of random variables 
 \begin{align*}
   (X(t))_{t \in [0,T]} \ : [0,T] \times \Omega  \ : \ \ (t,\omega ) \mapsto X(t,\omega )
 .\end{align*}
 For $\forall  \omega  \in  \Omega $ the map 
 \begin{align*}
   X(*,\omega ) \ : \ [0,T] \to \mathbb{R}^{d}  \ : \ t \mapsto X(t,\omega )
 .\end{align*}
 is called sample path
\end{definition}
\begin{definition}[History]
 Let $X(t)$ be a real valued process. The $\sigma-$algebra 
 \begin{align*}
  \mathcal{U}(t) \coloneqq  \mathcal{U}(X(s) \ | \ 0\le s\le t)
 .\end{align*}
 is called the history of $X$ until time $t\ge 0$
\end{definition}
\begin{definition}[Martingale]
  Let $X(t)$ be a real valued process and $\E[\abs{X(t)}] < \infty$  for $\forall t \ge 0$
  \begin{enumerate}
    \item If $X(s) = \E[X(t)|\mathcal{U}(s)]$ a.s. $\forall  t \ge  s \ge  0$  then $X(*)$ is called a martingale
    \item If $X(s) \lesseqqgtr  \E[X(t)|\mathcal{U}(s)]$ a.s. $\forall  t \ge  s \ge  0$  then $X(*)$ is called a (super) sub-martingale
  \end{enumerate}
\end{definition}
\begin{lemma}
  Suppose $X(*)$ is a real-valued martingale and $\phi  : \mathbb{R} \to  \mathbb{R}$ a convex function.
  If $\E[\abs{\phi(X(t))}] < \infty $ for $\forall  t\ge 0$ then $\phi(X(*))$ is a sub-martingale
\end{lemma}
\begin{theorem}[Martingale-Inequalities]
 Assume $X(*)$  is a process with continuous sample paths a.s. 
 \begin{enumerate}
   \item If $X(*)$ is a sub-martingale then $\forall  \lambda > 0$ , $t \ge 0$ it holds 
     \begin{align*}
       \P(\max_{0\le s\le t} X(s) \ge \lambda ) \le  \frac{1}{\lambda }\E[X(t)^{+} ]
     .\end{align*}
    \item If $X(*)$ is a martingale and $1 < p < \infty$ then
      \begin{align*}
        \E[\max_{0\le s\le t} \abs{X(s) }^{p} ] \le (\frac{p}{p-1})^{p} \E[\abs{X(t)}^{p} ]
      .\end{align*}
 \end{enumerate}
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
\subsection{Brownian Motion}
\begin{definition}[Brownian Motion]
 A real valued stochastic process $W(*)$ is called a Brownian motion 
 or Wiener process if 
 \begin{enumerate}
   \item $W(0) = 0$ a.s.
   \item $W(t)$ is continuous a.s.
   \item $W(t) - W(s) \sim \mathcal{N}(0,t-s)$ for $\forall t\ge s\ge 0$
   \item $\forall \ 0 < t_{1}<t_{2}<\ldots <t_n$ , $W(t_{1}),W(t_{2})-W(t_{1}),\ldots ,W(t_n)-W(t_{n-1})$ are independent 
 \end{enumerate}
\end{definition}
\begin{remark}
 One can derive directly that 
 \begin{align*}
   \E[W(t)] = 0 \quad \E[W^2(t)] = t \qquad \forall t \ge 0
 .\end{align*}
\end{remark}
Furthermore based on the above remark for $t\ge s$ 
\begin{align*}
  \E[W(t)W(s)] &= \E[(W(t)-W(s))(W(s))]+\E[(W(s)w(s))]\\
               &= \E[W(t)-W(s)]\E[W(s)] + \E[W(s)W(s)] \\
               &= s
.\end{align*}
which means generally 
\begin{align*}
  \E[W(t)W(s)] = t \land s
.\end{align*}
\begin{definition}
 An $\mathbb{R}^{d} $  valued process $W(*) = (W^{1}(*),\ldots ,W^{d}(*)  )$ is a $d-$dimensional Wiener process (or Brownian motion) if
 \begin{enumerate}
   \item $W^{k}(*) $ is a 1-$D$ Wiener process for $\forall  k =1 ,\ldots ,d$
   \item $\mathcal{U}(W^{k}(t) \ , \ t\ge 0 )$ $\sigma-$algebras are independent $k=1,\ldots ,d$
 \end{enumerate}
\end{definition}
\begin{remark}
 If $W(*)$  is  a $d-$Dimensional Brownian motion, then $W(t) \sim \mathcal{N}(0,t)$ and for any Borel set $A \subset  \mathbb{R}^{2} $
 \begin{align*}
  \P(W(t) \in  A) = \frac{1}{(2\pi t)^{\frac{n}{2}} } \int_A e^{-\frac{\abs{x}^2}{2t}} dx
 .\end{align*}
\end{remark}
\begin{theorem}
 If $X(*)$  is a given stochastic process with a.s. continuous sample paths and 
 \begin{align*}
   \E[\abs{X(t)-X(s)}^{\beta } ] \le  C \abs{t-s}^{1+\alpha } 
 .\end{align*}
 Then for $\forall 0 < \gamma  < \frac{\alpha }{\beta }$ and $T > 0$ a.s. $\omega$, there $\exists  K = K(\omega ,\gamma ,T)$ s.t.
 \begin{align*}
  \abs{X(t,\omega )-X(s,\omega )}\le K \abs{t-s}^{\gamma } \quad \forall  0 \le s, t \le T 
 .\end{align*}
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
An application of this result on Brownian motion is interesting since 
\begin{align*}
  \E[\abs{W(t)-W(s)}^{2m} ] \le  C \abs{t-s}^{m}  
.\end{align*}
we get immediately 
\begin{align*}
  W(*,\omega ) \in  \mathcal{C}^{\gamma }([0,T])  \quad 0<\gamma <\frac{m-1}{2m} < \frac{1}{2} \ \forall  m \gg 1
.\end{align*}
This means that Brownian motions is a.s. path Hölder continuous up to exponent $\frac{1}{2}$
\begin{remark}
  One can also further prove that the path wise smoothness  of Brownian motion can not be better than Hölder  continuous. Namely 
  \begin{enumerate}
    \item $\forall  \gamma  \in  (\frac{1}{2},1]$  and a.s. $\omega , t \mapsto W(t,\omega )$ is nowhere Hölder  continuous with exponent $\gamma $
    \item $\forall $ a.s. $\omega  \in  \Omega $ the map $t \mapsto W(t,\omega )$ is nowhere differentiable and is of infinite variation on each subinterval.
  \end{enumerate}
\end{remark}
\begin{definition}[Markov Property]
 An $\mathbb{R}^{d}-$valued process $X(*)$ is said to have the Markov property, if $\forall  0\le s\le t$ and 
 $\forall B \subset  \mathbb{R}^{d} $ Borel. , it holds 
 \begin{align*}
   \P(X(t) \in  B | \mathcal{U}(s)) = \P(X(t) \in  B | X(s)) \text{ a.s.}
 .\end{align*}
\end{definition}
\begin{remark}
 The $d-$Dimensional Wiener Process $W(*)$  has Markov property and 
 \begin{align*}
  \P(W(t)\in B | W(s)) = \frac{1}{(2\pi(t-s))^{\frac{n}{2}} } \int_B e^{-\frac{\abs{x - W(s)}^2}{2(t-s)}}  dx \text{ a.s.}
 .\end{align*}
\end{remark}
\section{It\^o Integral}
From now on we denote by $W(*)$ the $1-D$ Brownian motion on $(\Omega ,\mathcal{F},\P)$
\begin{definition}
  \hspace{0mm}\\
  \begin{enumerate}
    \item $\mathcal{W}(t) = \mathcal{U}(W(s) | 0 \le s \le t)$ is called the history up to t
    \item The $\sigma-$algebra 
      \begin{align*}
        \mathcal{W}^{+}(t) \coloneqq \mathcal{U}(W(s)-W(t) | s\ge t) 
      .\end{align*}
      is called the future of the Brownian motion beyond time $t$
  \end{enumerate}
\end{definition}
\begin{definition}[Non-Anticipating Filtration]
 A family $\mathcal{F}(*)$  of $\sigma-$algebras is called non-anticipating (w.r.t $W(*)$) if 
 \begin{enumerate}
   \item $\mathcal{F}(t) \supseteq \mathcal{F}(s)$ for $\forall t \ge  s \ge 0$ 
   \item $\mathcal{F}(t) \supseteq \mathcal{W}(t)$ for $\forall t \ge 0$ 
   \item $\mathcal{F}(t)$ is independent of $\mathcal{W}^{+}(t) $ for $\forall t \ge 0$ 
 \end{enumerate}
\end{definition}
A primary example of this is 
\begin{align*}
  \mathcal{F}(t) \coloneqq \mathcal{U}(W(s) , 0\le s\le t, X_{0})
.\end{align*}
where $X_{0}$ is a random variable independent of $\mathcal{W}^{+}(0) $
\begin{definition}[Non-Anticipating Process]
 A real-valued stochastic process $G(*)$  is called non-anticipating (w.r.t. $\mathcal{F}(*)$) if 
 for $\forall t \ge  0$ , $G(t)$ is $\mathcal{F}(t)-$measurable
\end{definition}
From now on we use $(\omega,\mathcal{F},\mathcal{F}(t),\P)$ as a filtered probability space with right continuous filtration 
$\mathcal{F}(t) = \bigcap_{s \ge t} \mathcal{F}(s)$. Note we also use the convention that $\mathcal{F}(t)$ is complete 
\begin{definition}
  \hspace{0mm}\\
  \begin{enumerate}
    \item A stochastic process is adapted to $(\mathcal{F}(t))_{t\ge 0}$  if $X_t$ is $\mathcal{F}(t)$ measurable for $\forall  t \ge 0$
    \item A stochastic process is progressively measurable w.r.t. $\mathcal{F}(t)$ if
      \begin{align*}
        X_t(s,\omega ) \ : \ [0,t] \times  \Omega  \to  \mathbb{R}
      .\end{align*}
      is $\mathcal{B}([0,t]) \times  \mathcal{F}(t)$ measurable for $\forall  t > 0$
  \end{enumerate}
\end{definition}
\begin{definition}
  We denote $\mathbb{L}^2([0,T])$  the space of all real-valued progressively measurable stochastic processes $G(*)$ s.t.
  \begin{align*}
    \E[\int_0^{T} G^2 dt ] < \infty
  .\end{align*}
  We denote $\mathbb{L}^{1}([0,T]) $ the space of all real-valued progressively measurable stochastic processes $F(*)$ s.t.
  \begin{align*}
    \E[\int_0^{T} \abs{F} dt ] < \infty
  .\end{align*}
\end{definition}
\begin{definition}[Step-Process]
  $G \in  \mathbb{L}^2([0,T])$  is called a step process if there exists a partition of the interval $[0,T]$ i.e.
  $P = \{0 = t_{0} < t_{1} < \ldots <t_m =T\}$ s.t. 
  \begin{align*}
    G(t) = G_k \quad \forall  t_k \le t < t_{k+1} \quad k=0,\ldots ,m-1
  .\end{align*}
  where $G_k$ is an $\mathcal{F}(t_k)$ measurable random variable
\end{definition}
\begin{remark}
Note that the above definition directly yields the following representation for any step process $G \in  \mathbb{L}^2([0,T])$ 
\begin{align*}
  G(t,\omega ) = \sum_{k=0}^{m-1} G_k(\omega )*\cha_{[t_k,t_{k+1})}(t)
.\end{align*}
\end{remark}
\begin{definition}[(Simple) It\^o Integral]
  Let $G \in  \mathbb{L}^2([0,T])$  be a step process. Then we define 
  \begin{align*}
    \int_0^{T} G(t,\omega ) dW_t \coloneqq \sum_{k=0}^{m-1} G_k(\omega )*(W(t_{k+1},\omega )-W(t_k,\omega ))
  .\end{align*}
\end{definition}
\begin{prop}
  Let $G,H \in  \mathbb{L}^2([0,T])$  be two step processes, then for $\forall  a,b \in  \mathbb{R}$ it holds 
  \begin{enumerate}
    \item $\int_0^{T}(aG + bH)dW_t  = a \int_0^{T} G dW_t + b \int_0^{T} HdW_t  $
    \item $\E{\int_0^{T}GdW_t } = 0$
  \end{enumerate}
\end{prop}
\begin{proof}
  (1). This case is easy. Set 
  \begin{align*}
    G(t) &= G_k \quad t_k \le t <t_{k+1} \quad k=0,\ldots,m_1 -1
    H(t) &= H_l \quad t_l \le t <t_{l+1} \quad l=0,\ldots,m_2 -1
  .\end{align*}
  Let $0 \le  t_{0}<t_{1}<\ldots \le t_n=T$ be the collection of $t_k$'s and $t_k$'s which together form a new partition
  of $[0,T]$ then obviously $G,H \in  \mathbb{L}^2([0,T])$ are again step processes on this new partition. We have 
  directly the linearity by definition on the It\^o integral for step processes
  \begin{align*}
    \int_0^{T} (G+H)d W_t = \sum_{j=0}^{n-1} (G_j+H_j)*(W(t_{j+1})-W(t_j))
  .\end{align*}
  (2). By definition we have 
  \begin{align*}
    \E[\int_0^{T} GdW_t ] = \E[\sum_{k=0}^{m-1}G_k(W(t_{k+1})-W(t_k)) ] = \sum_{k=0}^{m-1} \E[G_k(W(t_{k+1})- W(t_k))] 
  .\end{align*}
  Notice that $G_k$ by definition is $\mathcal{F}_{t_k}$ measurable and $W(t_{k+1}) - W(t_k)$ is measurable in $\mathcal{W}^{+}(t_k) $. Since
  $\mathcal{F}_{t_k}$ is independent of $\mathcal{W}^{+}(t_k) $, we can deduce that $G_k$ is independent of $W(t_{k+1}) - W(t_k)$ which implies 
  \begin{align*}
    \sum_{k=0}^{m-1} \E[G_k(W(t_{k+1})- W(t_k))]  = \sum_{k=0}^{m-1} \E[G_k]*\E[W(t_{k+1}) - W(t_k)]  =0
  .\end{align*}
\end{proof}
\begin{lemma}[(Simple) It\^o isometry]
  For step processes $G \in  \mathbb{L}^2([0,T])$  we have 
  \begin{align*}
    \E[(\int_0^{T} G dW_t )^2] = \E[\int_0^{T} G^2 dt ]
  .\end{align*}
\end{lemma}
\begin{proof}
  By definition we can write 
  \begin{align*}
    \E[\left( \int_0^{T} G dW_t  \right)^2 ] = \sum_{k,j=0}^{m-1} \E[G_kG_j(W(t_{k+1})-W(t_k))(W(t_{j+1})-W(t_j))] 
  .\end{align*}
  If $j < k$, then $W(t_{k+1}) -W(t_k)$ is independent of $G_kG_j(W(t_{j+1})-W(t_j))$. Therefore 
  \begin{align*}
    \sum_{j<k}\E[\ldots ] = 0 \quad \text{ and }  \quad \sum_{j>k}\E[\ldots ] = 0
  .\end{align*}
  Then we have 
  \begin{align*}
    \E[\left( \int_0^{T} GdW_t  \right)^2 ] &= \sum_{k=0}^{m-1} \E[G_k^2(W(t_{k+1})-W(t_k))^2]  \\
                                            &= \sum_{k=0}^{m-1} \E[G_k^2]\E[(W(t_{k+1})-W(t_k))^2] \\
                                            &= \sum_{k=0}^{m-1} \E[G_k^2](t_{k+1}-t_k) \\
                                            &= \E[\int_0^{T} G^2dt ] 
  .\end{align*}
\end{proof}
For general $\mathbb{L}^2([0,T])$ processes we use approximation by step processes to define the It\^o integral 
\begin{lemma}
  If $G \in  \mathbb{L}^2([0,T])$  then there exists a sequence of bounded step processes  $G^{n} \in  \mathbb{L}^2([0,T])$  s.t. 
  \begin{align*}
   \E[\int_0^{T} \abs{G - G^{n} }^2 dt ] \xrightarrow{n\to \infty} 0
  .\end{align*}
\end{lemma}
\begin{proof}
  We roughly sketch the Idea here \\[1ex]
  If $G(*,\omega )$  is a.e. continuous then we can take  
 \begin{align*}
   G^{n}(t) \coloneqq  G(\frac{k}{n})  \quad \frac{k}{n} \le t < \frac{k+1}{n} \quad k=0,\ldots ,\floor{nT}
 .\end{align*}
 For general $G \in  \mathbb{L}^2([0,T])$ let 
 \begin{align*}
  G^{m}(t) \coloneqq  \int_0^{t} m e^{m(s-t)}G(s) ds   
 .\end{align*}
 Then $G^{m} \in  \mathbb{L}^2([0,T]) $ , $t \mapsto G^{m}(t,\omega ) $ is continuous for a.s. $\omega $ and 
 \begin{align*}
  \int_0^{T} \abs{G - G^{m} }^2 dt \to 0 \text{ a.s.}
 .\end{align*}
\end{proof}
\begin{definition}[It\^p Integral]\label{ito_integral}
  If $G \in  \mathbb{L}^2([0,T])$. Let step processes $G^{n} $ be an approximation of $G$. Then we define
  the It\^o  integral by using the limit 
  \begin{align*}
    I(G) = \int_0^{T} GdW_t \coloneqq  \lim_{n\to \infty} \int_0^{T} G^{n} dW_t
  .\end{align*}
  where the limit exists in $L^2(\Omega)$
\end{definition}
In order to derive the validity of this definition, one has to check 
\begin{enumerate}
  \item Existence of the limit. This can be obtained by showing that it is a Cauchy sequence, namely by It\o isometry we have
    \begin{align*}
      \E[\left( \int_0^{T} (G^{m} - G^{n}  ) dW_t  \right)^2 ] = \E[\int_0^{T} \abs{G^{m} - G^{n}  }^2 dt] \xrightarrow{n,m\to \infty} \to 0
    .\end{align*}
    This implies $\int_0^{T} G^{n} dW_t  $ has a limit in $L^2(\Omega )$ as $n\to \infty$
  \item The limit is independent of the choice of approximation sequences.
    Let $\tilde{G}^{n}  $ be another step process which converges to $G$. Then we have 
    \begin{align*}
      \E[\int_0^{T} \abs{\tilde{G}^{n} - G^{n}   }^2 dt ] \le  \E[\int_0^{T} \abs{G^{n} - G }^2 dt ] + \E[\int_0^{T} \abs{\tilde{G}^{n} - G  }^2 dt ]
    .\end{align*}
    it follows that 
    \begin{align*}
      \E[\left( \int_0^{T} \tilde{G}^{n} dW_t - \int_0^{T} G^{n} dW_t      \right)^2 ] = \E[\int_0^{T} \abs{\tilde{G}^{n} - G^{n}   }^2 dt ] \to 0
    .\end{align*}
\end{enumerate}
By using this approximation, all the properties for step  processes can be obtained for general $\mathbb{L}^2([0,T])$ processes
\begin{theorem}[Properties Of The It\^o Integral]
  For $\forall  a,b \in  \mathbb{R}$  and $\forall  G,H \in \mathbb{L}^2([0,T])$ it holds 
  \begin{enumerate}
    \item $\int_0^{T} (aG+bH) dW_t = a\int_0^{T} GdW_t + b\int_0^{T} H dW_t   $ 
    \item $\E[\int_0^{T} GdW_t ] = 0$
    \item $\E[\int_0^{T} GdW_t * \int_0^{T} HdW_t  ] = \E[\int_0^{T} GH dt ]$
  \end{enumerate}
\end{theorem}
\begin{lemma}[It\^o Isometry]
  For general $G \in  \mathbb{L}^2([0,T])$  we have 
  \begin{align*}
    \E[\left( \int_0^{T} G dW_t  \right)^2 ] = \E[\int_0^{T} G^2  dt ]
  .\end{align*}
\end{lemma}
\begin{proof}
  Choose step processes $G_{n} \in \mathbb{L}^2([0,T]) $  such that $G_{n} \to G $ (in the sense previously defined) then by \autoref{ito_integral} we get 
  \begin{align*}
    \|I(G) - I(G_n)\|_{L^2} \xrightarrow{n\to \infty} 0
  .\end{align*}
  Then using the simple version of It\^o isometry one obtains 
  \begin{align*}
    \E[\left(\int_0^{T} G dW_t\right)^2] = \lim_{n\to \infty} \E[\left( \int_0^{T} G_{n} dW_t   \right)^2 ]  = \lim_{n\to \infty} \E[\int_0^{T} (G_n)^2dt ] = \E[\int_0^{T} (G)^2dt ]
  .\end{align*}
\end{proof}
\begin{remark}
  The It\^o integral is a map from $\mathbb{L}^2([0,T]) $  to $L^2(\Omega )$
\end{remark}
\begin{remark}
  For $G \in \mathbb{L}^2([0,T])$  the It\^o integral $\int_0^{\tau } G dW_t $ with $0 \le \tau  \le T$ is a martingale
\end{remark}
\subsection{It\^o's Formula}
\begin{definition}[It\^o Process]
 Let $X(*)$ be a real-valued process given by 
 \begin{align*}
  X(r) = X(s) + \int_s^{r} F dt + \int_s^{r} GdW_t  
 .\end{align*}
 for some $F \in  \mathbb{L}^1([0,T])$ and $G \in  \mathbb{L}^2([0,T])$ for $0\le s\le r\le T$, then $X(*)$ is called It\^o process.\\[1ex]
 Furthermore we way $X(*)$ has a stochastic differential 
 \begin{align*}
  dX =  Fdt + gdW_t \quad \forall  0\le t\le T
 .\end{align*}
\end{definition}
\begin{theorem}[It\^o's Formula]
  Let $X(*)$  be an It\^o process given by $dX = F dt + GdW_t$ for some $F \in  \mathbb{L}^{1}([0,T]) $ and $G \in  \mathbb{L}^2([0,T])$. Assume 
  $u : \mathbb{R} \times  [0,T] \to \mathbb{R}$ is continuous and $\frac{\partial u}{\partial t} ,\frac{\partial u}{\partial x} ,\frac{\partial ^2 u}{\partial x^2} $ exists and 
  are continuous. Then $Y(t) \coloneqq  u(X(t),t)$ satisfies 
  \begin{align*}
    dY &= \frac{\partial u}{\partial t} dt + \frac{\partial u}{\partial x} dX + \frac{1}{2 }\frac{\partial ^2 u}{\partial x^2} G^2 dt\\
       &=(\frac{\partial u}{\partial t} +\frac{\partial u}{\partial x} F + \frac{1}{2} \frac{\partial ^2 u}{\partial x^2} G^2 )dt + \frac{\partial u}{\partial x} G dW_t
  .\end{align*}
  Note that the differential form of the It\^o formula is understood as an abbreviation of the following integral form, for all $0\le s < r \le T$
  \begin{align*}
    &u(X(r),r) - u(X(s),s) \\
    &= \int_s^{r}(\frac{\partial u}{\partial t}(X(t),t)+\frac{\partial u}{\partial x}(X(t),t)F(t) + \frac{1}{2}\frac{\partial ^2 u}{\partial x^2}(X(t),t)G^2(t) )dt + \int_s^{r} \frac{\partial u}{\partial x}(X(t),t) G(t)dW_t
  .\end{align*}
  \end{theorem}
\begin{proof}
  The proof is split into five steps \\[1ex]
  \textbf{Step 1.} First we prove two simple cases. If $X(t)=W_t$ then 
  \begin{enumerate}
    \item $d(W_t)^2 = 2W_t dW_t + dt $
    \item $d(tW_t) = W_t dt + t dW_t$
  \end{enumerate}
  For (1) it is sufficient to prove $W_t^2 - W_0^2 = \int_0^{t} 2W_s dW_s + t$ a.s. By definition of It\^o integral, for a.s. $\omega  \in \Omega $ we have 
  \begin{align*}
    \int_0^{t}2W_sdW_s &= 2 \lim_{n\to \infty} \sum_{k=0}^{n-1} W(t_k^{n} )\left(W(t_{k+1}^{n})-W(t_k^{n} )\right)\\
                       &= \lim_{n\to \infty} \Bigg[\sum_{k=0}^{n-1} W(t_k^{n})\left(W(t_{k+1}^{n}) - W(t_k^{n} )\right) - \sum_{k=0}^{n-1}\left(W(t_{k+1}^{n})-W(t_k^{n}) \right)  \\
                       & \hspace{1.1cm}+ \sum_{k=0}^{n-1}W(t_{k+1}^{n} )  \left(W(t_{k+1}^{n})-W(t_k^{n} ) \right) \Bigg]\\
                       &= - \lim_{n\to \infty} \Bigg[\sum_{k=0}^{n-1}\left( W(t_{k+1}^{n}) - W(t_k^{n} ) \right)^2 -  \sum_{k=0}^{n-1}\left(W(t_k^{n} )\right)^2 + \sum_{k=0}^{n-1}\left( W(t_{k+1}^n) \right)^2      \Bigg]\\
                       &= - \lim_{n\to \infty} \sum_{k=0}^{n-1}\left( W(t_{k+1}^{n}) - W(t_k^{n} ) \right)^2 +  \left(W(t)\right)^2 - \left( W(0) \right)^2 
  .\end{align*}
  where for any fixed $n$, the partition of $[0,T]$ is given by $0\le t_{0}^{n} < t_{1}^{n} < \ldots <t_n^{n} = T   $ and 
  $t_{k}^{n} - t_{k+1}^{n}   = \frac{1}{n}$ . It remains to prove that the limit 
  \begin{align*}
    \lim_{n\to \infty}\sum_{k=0}^{n-1} \left( W(t_{k+1}^{n} ) - W(t_k^{n} ) \right)^2 - t = 0 
  .\end{align*}
  holds true. Actually 
  \begin{align*}
    &\E \left[ \sum_{k=0}^{n-1}\left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)^2   \right] \\
    &=\E[\sum_{k=0}^{n-1}\sum_{l=0}^{n-1} \left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)*\left( \left( W(t_{l+1}^{n} ) - W(t_l^{n})\right)^2 - \left( t_{l+1}^{n} - t_l^{n}   \right)   \right) ]
  .\end{align*}
  The terms with $k\neq l$ vanish because of the independence. Therefore
  \begin{align*}
    &\E \left[ \sum_{k=0}^{n-1}\left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)^2   \right] \\ 
    &= \sum_{k=0}^{n-1}(t_{k+1}^{n} - t_k^{n}  )^2 \E \left[ \left( \frac{\left( W(t_{k+1}^{n} ) - W(t_k^{n} ) \right)^2 }{t_{k+1}^{n} - t_{k}^{n}  } - 1 \right)^2  \right]  \\
    &= \sum_{k=0}^{n-1}(t_{k+1}^{n} - t_k^{n}  )^2 \E \left[ \left( \left(\frac{ W(t_{k+1}^{n} ) - W(t_k^{n} ) }{\sqrt{t_{k+1}^{n} - t_{k}^{n}}  } \right)^2 - 1 \right)^2  \right]  \\
    &\le C*\frac{t^2}{n}\\
    &\to  0
  .\end{align*}
  where we have used the fact that $Y = \frac{ W(t_{k+1}^{n} ) - W(t_k^{n} ) }{\sqrt{t_{k+1}^{n} - t_{k}^{n}}  } \sim \mathcal{N}(0,1)$. Hence $\E[(Y^2 - 1)^2]$ is 
  bounded by a constant $C$\\[1ex]
  For (2) : It is sufficient to prove $tW_t - 0 W_0 =  \int_0^{t} W_s ds + \int_0^{t} s dW_s  $. Actually we have 
  \begin{align*}
    \int_0^{t} s dW_s = \lim_{n\to \infty}  \sum_{k=0}^{n-1} t_k^{n}\left( W(t_{k+1}^{n} - W(t_k^{n} )  ) \right)  \text{ a.s.}
  .\end{align*}
  and for a.s. $\omega $ the standard Riemann sum
  \begin{align*}
    \int_0^{t} W_s ds = \lim_{n\to \infty}  \sum_{k=0}^{n-1} W(t_{k+1}^{n} )(t_{k+1}^{n} - t_k^{n}  )
  .\end{align*}
  The summation of the above integrals yields 
  \begin{align*}
    \int_0^{t} s dW_s  + \int_0^{t} W_s ds &= \lim_{n\to \infty}   \sum_{k=0}^{n-1} t_k^{n}\left( W(t_{k+1}^{n}) -W(t_k^{n} ) \right) + \lim_{n\to \infty}\sum_{k=0}^{n-1} W(t_{k+1}^{n} ) (t_{k+1}^{n} - t_k^{n}  )\\
                                           &= W(t)*t - 0*W(0)
  .\end{align*}
  \textbf{Step 2.} Now let us prove the It\^o product rule. If 
  \begin{align*}
    dX_{1} = F_{1}dt + G_{1}dW_t \quad \text{ and } \quad dX_{2} = F_{2}dt+G_{2}dW_t
  .\end{align*}
  for some $G_i \in  \mathbb{L}^2([0,T])$ and $F_i \in  \mathbb{L}^1([0,T])$ $i=1,2$ , then 
  \begin{align*}
    d(X_{1}X_{2}) &= X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2} dt
                  &=(X_{2}F_{1}+X_{1}F_{2}+G_{1}G_{2})dt + (X_{2}G_{1}+X_{1}G_{2})dW_t
  .\end{align*}
  where the above should be understood as the integral equation.\\
  (1) We prove the case $F_i,G_i$ are time independent. Assume for simplicity $X_{1}(0) = X_{2}(0)$ then it follows that 
  \begin{align*}
    X_i(t) = F_it G_iW(t)
  .\end{align*}
  Then it holds a.s. that 
  \begin{align*}
    &\int_0^{t} (X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2} ds) \\
    &= \int_0^{t} (X_{2}F_{1}+X_{1}F_{2}) ds + \int_0^{t} (X_{2}G_{1}+X_{1}G_{2}) dW_s + \int_0^{t} G_{1}G_{2}ds   \\
    &= \int_0^{t} \left( F_{1}(F_{2}s + G_{2}W(s))  + F_{2}(F_{1}s+G_{1}W(s))\right) ds + G_{1}G_{2}t\\
    &= \int_0^{t} \left( G_{1}(F_{2}s + G_{2}W(s)) + G_{2}(F_{1}s+G_{1}W(s)) \right) dW_s \\
    &= G_{1}G_{2}t F_{1}F_{2}t^2+(F_{1}G_{2}+F_{2}G_{1})\left( \int_0^{t}W(s) ds + \int_0^{t} s dW_s   \right) \\
    & \quad + 2G_{1}G_{2} \int_0^{t} W(s) dW_s 
  .\end{align*}
  using (1) and (2) from Step 1. It continues to hold that 
  \begin{align*}
    G_{1}G_{2}(W(t))^2  + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1})tW(t) = X_{1}(t)+X_{2}(t)
  .\end{align*}
  Therefore It\^o formula is true when $F_i,G_i$ are time independent random variables.\\
  (2) If $F_i,G_i$ are step processes, then we apply the above formula in each sub-interval\\
  (3) For $F_i \in  \mathbb{L}^1([0,T])$ and $G_i \in  \mathbb{L}^2([0,T])$, we take the step process approximation of them, namely
  \begin{align*}
    \E[\int_0^{T} \abs{F_i^{n} - F_i } dt ] \to 0 \quad \E[\int_0^{T} \abs{G_i^{n} - G_i }^2 dt ] \to 0 \qquad (n\to \infty), i=1,2
  .\end{align*}
  Notice that for each It\^o process given by step processes 
  \begin{align*}
    X_i^{n}(t) = X_i(0) + \int_0^{t} F_i^{n} ds + \int_0^{t} G_i^{n} dW_s     
  .\end{align*}
  the product rule holds, i.e. 
  \begin{align*}
    X_1^{n}(t)X_2^{n}(t) - X_1(0)X_2(0) = \int_0^{t} \left( X_1^{n}(s)dX_2^{n}(s) + X_2^{n}(s) dX_1^{n}(s) + G_{1}G_{2}ds     \right)    
  .\end{align*}
  \textbf{Step 3.} If $u(X) = X^{m} $ for $m \in  \mathbb{N}$ then we claim 
  \begin{align*}
    d(X^{m} ) = mX^{m-1}dX + \frac{1}{2}m(m-1) X^{m-2} G^2dt
  .\end{align*}
  We prove this by induction.\\
  \textbf{IA} Note that $m=2$ is given by the product rule. \\
  \textbf{IV} Suppose the formula holds for $m-1 \in  \mathbb{N}$ \\
  \textbf{IS} $m-1 \to m$ then 
  \begin{align*}
    d(X^{m} )= d(XX^{m-1} ) &= Xd(X^{m-1} ) + X^{m-1}dX + (m-1)X^{m-2} G^2dt \\
                            &\myS{IV}{=} X \left( (m-1)X^{m-2} dX + \frac{1}{2}(m-1)(m-2)X^{m-3}G^2 dt   \right) \\
                            & \quad + X^{m-1}dX + (m-1)X^{m-2}G^2 dt \\
                            &= mX^{m-1} dX + (m-1)(\frac{m}{2} -1 +1) X^{m-2} G^2 dt 
  .\end{align*}
  Thus the statement holds for all $m \in  \mathbb{N}$\\[1ex]
  \textbf{Step 4.} If $u(X,t) = f(X)g(t)$ where $f$ and $g$ are polynomials $f(X) = X^{m} $ , $g(t) =t^n$.
  Then by the product rule we have 
  \begin{align*}
    d(u(X,t)) = d(f(X)g(t)) = f(X)dg + g df(X) + (G_{1}*0) dt
  .\end{align*}
  by step 3  this is equal to
  \begin{align*}
    f(X)g'(t)dt + gf'(X)dX + \frac{1}{2}f^{''}(X)G^2 dt =\frac{\partial u}{\partial t} dt + \frac{\partial u}{\partial X} dX + \frac{1}{2} \frac{\partial ^2 u}{\partial X^2} G^2 dt
  .\end{align*}
  Note the It\^o formula is also true if $u(X,t) = \sum_{i=1}^{m} g_m(t)f_m(X) $ where $f_m$ and $g_m$ are polynomials\\[1ex]
  \textbf{Step 5.} For $u$ continuous such that $\frac{\partial u}{\partial t} , \frac{\partial u}{\partial x} ,\frac{\partial ^2 u}{\partial x^2}$ exists and are also continuous, then
  there exists polynomial sequences $u^{n} $ s.t.
  \begin{align*}
    u^{n} \to  u \quad \frac{\partial u^{n } }{\partial t} \to \frac{\partial u}{\partial t} , \quad \frac{\partial u^{n } }{\partial x} \to \frac{\partial u}{\partial x} , \quad \frac{\partial ^2 u}{\partial x^2} \to \frac{\partial ^2 u}{\partial x^2} 
  .\end{align*}
  uniformly on compact $K \subset  \mathbb{R}\times [0,T]$. Since 
  \begin{align*}
    u^{n}(X(t),t) -u^{n}(X(0),0)  = \int_0^{t} \left( \frac{\partial u^{n } }{\partial t} +\frac{\partial u^{n } }{\partial x} F + \frac{1}{2} \frac{\partial ^2 u^{n} }{\partial x^2} G^2  \right)  dr + \int_0^{t} \frac{\partial u^{n } }{\partial x}  GdW_r \quad \text{a.s.} 
  .\end{align*}
  then by taking the limit $n\to \infty$ It\^o's formula is proven 
\end{proof}


