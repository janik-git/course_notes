\chapter{MEAN FIELD LIMIT FOR SDE SYSTEM}
\section{Basics On Probability Theory}
This section is dedicated to a small review of basic concepts 
in probability theory in preparations of SDE's
\subsection{Probability Spaces and Random Variables}
\begin{definition}[$\sigma$-Algebra]
 Let $\Omega $  be a given set, then a $\sigma-$algebra $\mathcal{F}$ on $\Omega $ is a
 family of subsets of $\Omega $ s.t.
 \begin{enumerate}
   \item $\emptyset \in  \mathcal{F}$
   \item $F \in  \mathcal{F} \implies F^{c} \in  \mathcal{F} $
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ countable, then 
     \begin{align*}
       A = \bigcup_{j=1}^{\infty} A_j \in \mathcal{F}
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Measure Space]
 A tuple $(\Omega ,\mathcal{F})$  is called a measurable space. The elements of $\mathcal{F}$ are 
 called measurable sets 
\end{definition}
\begin{definition}[Probability Measure]
 A probability measure $\P$ on $(\Omega ,\mathcal{F})$  is a function 
 \begin{align*}
   \P \ : \ \mathcal{F} \to [0,1]
 .\end{align*}
 s.t.
 \begin{enumerate}
   \item $\P(\emptyset) = 0$ , $\P(\Omega ) = 1$
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ s.t. $A_i \cap A_j = \emptyset \ \forall  i \neq j$  then
     \begin{align*}
       \P(\bigcup_{j=1}^{\infty} A_j ) = \sum_{j=1}^{\infty} \P(A_j) 
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Probability Space]
 The triple $(\Omega ,\mathcal{F},\P)$  is called a probability space. $F \in  \mathcal{F}$ is called
 event. We say the probability space $(\Omega ,\mathcal{F},\P)$ is complete, if $\mathcal{F}$ contains all zero-measure sets i.e.
 if 
 \begin{align*}
  \inf \{\P(F) \ : \ F \in  \mathcal{F},G \subset  F\}  = 0
 .\end{align*}
 then $G \in  \mathcal{F}$ and $\P(G) = 0$. Without loss of generality we use in this lecture $(\Omega ,\mathcal{F},\P)$
 as complete probability space
\end{definition}
\begin{definition}[Almost Surely]
  If for some $F \in  \mathcal{F}$ it holds $\P(F) = 1$ the we say that $F$ happens with 
  probability 1 or almost surely (a.s.)
\end{definition}
\begin{remark}
 Let $\mathcal{H}$  be a family of subsets of $\Omega$, then there exists a smallest $\sigma-$algebra of 
 $\Omega$ called $\mathcal{U}_{\mathcal{H}}$ with 
 \begin{align*}
   \mathcal{U}_{\mathcal{H}} = \bigcap_{\substack{\mathcal{H} \subset \mathcal{U} \\ \mathcal{H} \ \sigma-\text{alg.}}} \mathcal{H}  
 .\end{align*}
\end{remark}
\begin{example}
  The $\sigma-$algebra generated by a topology $\tau $ of $\Omega$ , $\mathcal{U}_{\tau } \triangleq \mathcal{B}$ is called 
  the Borel $\sigma-$algebra, the elements $B \in  \mathcal{B}$ are called Borel sets.
\end{example}
\begin{definition}[Measurable Functions]
 Let $(\Omega ,\mathcal{F},\P)$  be a probability space, a function 
 \begin{align*}
  Y \ : \ \Omega  \to \mathbb{R}^{d} 
 .\end{align*}
 is called measurable if and only if 
 \begin{align*}
  Y^{-1}(B) \in  \mathcal{F} 
 .\end{align*}
 holds for all $B \in  \mathcal{B}$ or equivalent for all $B \in  \tau $
\end{definition}
\begin{example}
 Let $X : \Omega  \to  \mathbb{R}^{d} $  be a given function, then the $\sigma-$algebra $\mathcal{U}(X)$ generated by X is 
 \begin{align*}
  \mathcal{U}(X) = \{X^{-1}(B) \ : \ B \in  \mathcal{B} \}  
 .\end{align*}
\end{example}
\begin{lemma}[Doob-Dynkin]
 If $X,Y \ : \ \Omega  \to \mathbb{R}^{d} $  are given then $Y$ is $\mathcal{U}(X)$ measurable if and only if 
 there exists a Boreal measurable function $g \ : \ \mathbb{R}^{d} \to  \mathbb{R}^{d}  $ such that 
 \begin{align*}
  Y = g(x)
 .\end{align*}
\end{lemma}
\begin{exercise}
  Proof the above lemma
\end{exercise}
From now on we denote $(\Omega ,\mathcal{F},\P)$ as a given probability space.
\begin{definition}[Random Variable]
 A random variable $X \ : \ \Omega  \to \mathbb{R}^{d} $  is a $\mathcal{F}-$measurable function.
 Every random variable induces a probability measure or $\mathbb{R}^{d} $ 
 \begin{align*}
  \mu_X(B) = \P(X^{-1}(B) ) \quad \forall B \in  \mathcal{B}
 .\end{align*}
This measure is called the distribution of X
\end{definition}
\begin{definition}[Expectation and Variance]
 Let $X$ be a random variable, if 
 \begin{align*}
   \int_{\Omega } \abs{X(\omega )}d\P(\omega ) < \infty
 .\end{align*}
 then 
 \begin{align*}
   \E[X] = \int_{\Omega } X(\omega ) d\P(\omega ) =  \int_{\mathbb{R}^{d} }x d\mu_X(x)
 .\end{align*}
 is called the expectation of $X$ (w.r.t. $\P$) \\[1ex]
 \begin{align*}
   \V[X] = \int_{\Omega } \abs{X - \E[X]}^2 d\P(\omega )
 .\end{align*}
 is called variance and there exists the simple relation 
 \begin{align*}
   \V[X] = \E[\abs{X-\E[X]}^2] = \E[\abs{X}^2] - \E[X]^2
 .\end{align*}
\end{definition}
\begin{remark}
 If $f : \mathbb{R}^{d} \to  \mathbb{R} $ measurable and 
 \begin{align*}
   \int_{\Omega } \abs{f(X(\omega ))} d\P(\Omega ) <\infty
 .\end{align*}
 then 
 \begin{align*}
   \E[f(x)] = \int_{\Omega }f(X(\omega ))d\P(\omega ) = \int_{\mathbb{R}^{d} }f(x) d\mu_X(x)
 .\end{align*}
\end{remark}
\begin{definition}[$L^p$ spaces]
  Let $X : \Omega  \to  \mathbb{R}^{d} $  be a random variable and $p \in [1,\infty)$.
  With the norm 
  \begin{align*}
    \|X\|_p = \|X\|_{L^{p}(\P ) } = \left( \int_{\Omega} \abs{X(\omega )}^{p} d\P(\omega )  \right)^{\frac{1}{p}} 
  .\end{align*}
  If $p=\infty$ 
  \begin{align*}
    \|X\|_{\infty} = \inf \{N \in  \mathbb{R} : \abs{X(\omega )} \le  N \text{ a.s.}\}  
  .\end{align*}
  the space $L^{p}(\P ) = L^{p}(\Omega ) = \{X \ : \ \Omega  \to  \mathbb{R}^{d}  \ | \ \|X\|_p \le \infty \}    $ is a Banach space.
\end{definition}
\begin{remark}
 If $p=2$ then $L^{2}(\P) $ is a Hilbert space with inner product 
 \begin{align*}
   \braket{X,Y} = \E[X(\omega )*Y(\Omega )] = \int_{\Omega }X(\omega )*Y(\omega )d\P(\omega )
 .\end{align*}
\end{remark}
\begin{definition}[Distribution Functions]
 Note for $x,y \in  \mathbb{R}^{d} $  we write $x\le y$ if $x_i \le  y_i$ for $\forall i$
 \begin{enumerate}
   \item $X: (\Omega ,\mathcal{F},\P) \to \mathbb{R}^{d} $ is a random variable the ints distribution function $F_x : \mathbb{R}^{d} \to [0,1] $
     is defined by 
     \begin{align*}
      F_X(x) = \P(X\le x) \quad x \in  \mathbb{R}^{d} 
     .\end{align*}
    \item If $X_{1},\ldots ,X_m : \Omega \to \mathbb{R}^{d} $ are random variables, their joint distribution function is
      \begin{align*}
        F_{X_{1},\ldots ,X_m} &: (\mathbb{R}^{d} )^m \to [0,1]\\
        F_{X_{1},\ldots ,X_M} &= \P(X_{1}\le x_{1},\ldots ,X_m\le x_m) \quad \forall x_i \in \mathbb{R}^{d} 
      .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Density Function Of X]
 If there exists a non-negative function $f(x) \in  L^{1}(\mathbb{R}^{d} ; \mathbb{R} ) $   such that 
 \begin{align*}
   F(x) = \int_{-\infty}^{x_{1}}  \ldots \int_{-\infty}^{x_n} f(y) dy \quad y = (y_{1},\ldots ,y_n)
 .\end{align*}
 then f is called density function of $X$ and 
 \begin{align*}
  \P(X^{-1}(B) ) = \int_B f(x) dx \quad \forall  B \in  \mathcal{B}
 .\end{align*}
\end{definition}
\begin{example}
 Let $X$ be random variable with density function  $x \in  \mathbb{R}$
 \begin{align*}
  f(x) = \frac{1}{\sqrt{2\pi \sigma ^2}e^{-\frac{\abs{x-m}^2}{2\sigma ^2}}  }
 .\end{align*}
 then we say that $X$ has a Gaussian (or Normal) distribution with mean m and variance $\sigma^2$ and write
 \begin{align*}
  X \sim \mathcal{N}(m,\sigma^2)
 .\end{align*}
 Obviously 
 \begin{align*}
   \int_{\mathbb{R}} xf(x) dx = m \qquad \int_{\mathbb{R}}\abs{x-m}^2f(x) dx = \sigma^2
  .\end{align*}
\end{example}
\begin{definition}[Independent Events]
  Events $A_{1},\ldots ,A_{n} \in  \mathcal{F}$ are called independent if $\forall 1 \le k_{1} < \ldots  < k_m \le  n$ it holds 
  \begin{align*}
    \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
  .\end{align*}
\end{definition}
\begin{definition}[Independent $\sigma-$Algebra]
 Let $\mathcal{F}_j \subset  \mathcal{F}$   be $\sigma-$algebras for $j=1,2,\ldots $. Then we say $\mathcal{F}_j$ are independent if 
 for $\forall 1 \le k_{1}<k_{2}<\ldots <k_m$ and $\forall A_{k_j} \in  \mathcal{F}_{k_j}$ it holds
 \begin{align*}
  \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
 .\end{align*}
\end{definition}
\begin{definition}[Independent Random Variables]
 We say random variables $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $  are independent if 
 for $\forall  B_{1},\ldots ,B_{m} \subset  \mathcal{B}$ in $\mathbb{R}^{d} $ it holds 
 \begin{align*}
   \P(X_{j_{1}}\in B_{j_{1}},\ldots, X_{j_k}\in B_{j_k} ) = \P(X_{j_{1}} \in  B_{j_{1}})\ldots \P(X_{j_k} \in  B_{j_k})
 .\end{align*}
 which is equivalent to proving that $\mathcal{U}(X_{1}),\ldots ,\mathcal{U}(X_k)$ are independent
\end{definition}
\begin{theorem}
 $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $ are independent if and only if 
 \begin{align*}
   F_{X_{1},\ldots ,X_m}(x_{1},\ldots ,x_m) =F_{X_{1}}(x_{1})\ldots F_{x_m}(x_m) \quad \forall  x_i \in \mathbb{R}^{d} 
 .\end{align*}
\end{theorem}
\newpage
\begin{theorem}
  If $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independent and $\E[\abs{X_i}] < \infty$ then 
  \begin{align*}
    \E[\abs{X_{1},\ldots ,X_m}]<\infty
  .\end{align*}
  and 
  \begin{align*}
    \E[X_{1}\ldots X_m] = \E[X_{1}]\ldots \E[X_m]
  .\end{align*}
\end{theorem}
\begin{theorem}
  $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independent and $\V[X_i] <\infty$ then 
  \begin{align*}
    \V[X_{1} + \ldots  + X_m] = \V[X_{1}] + \ldots  + \V[X_m]
  .\end{align*}
\end{theorem}
\begin{exercise}
 Proof the above theorems
\end{exercise}
\subsection{Borel Cantelli}
\begin{definition}
 Let $A_{1},\ldots ,A_m \in \mathcal{F}$   then the set 
 \begin{align*}
   \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty} A_m = \{\omega \in \Omega  \ : \ \omega  \text{ belongs to infinite many} A_{m}\text{'s}\}  
 .\end{align*}
 is called $A_m$ infinitely often or $A_m$ i.o.
\end{definition}
\begin{lemma}[Borel Cantelli]\label{borel_cantelli}
 If $\sum_{m=1}^{\infty} \P(A_m) < \infty  $  then $\P(A_ \text{ i.o. }) = 0$
\end{lemma}
\begin{proof}
 By definition we have 
 \begin{align*}
   \P(A_m \text{ i.o. }) \le  \P(\bigcup_{m=n}^{\infty} ) \le \sum_{m=n}^{\infty} \P(A_m)  \xrightarrow{m\to \infty} 0
 .\end{align*}
\end{proof}
\begin{definition}[Convergence In Probability]
  We say a sequence of random variables $(X_k)_{k=1}^{\infty} $  converges in probability to $X$ if 
  for $\forall  \epsilon > 0$
  \begin{align*}
    \lim_{k\to \infty} \P(\abs{X_k - X} > \epsilon ) = 0 
  .\end{align*}
\end{definition}
\begin{theorem}[Application Of Borel Cantelli]
 If $X_k \to  X$  in probability, then there exists 
 a subsequence $(X_{k_j})_{j=1}^{\infty} $ such that 
 \begin{align*}
   X_{k_j}(\omega ) \to X(\omega ) \text{ for almost every } \omega \in  \Omega 
 .\end{align*}
 This means that $\P(\abs{X_{k_j}-X}\to 0) = 1$
\end{theorem}
\begin{proof}
  For $\forall  j \ \exists k_j$  with $k_j < k_{j+1} \to  \infty$ s.t.
  \begin{align*}
    \P(\abs{X_{k_j} - X} > \frac{1}{j}) \le \frac{1}{j^2}
  .\end{align*}
  then 
  \begin{align*}
    \sum_{j=1}^{\infty} \P(\abs{X_{k_j}-X} > \frac{1}{j}) = \sum_{j=1}^{\infty} \frac{1}{j^2}   < \infty
  .\end{align*}
  Let $A_j = \{\omega  \ : \ \abs{X_{k_j}-X} > \frac{1}{j}\}  $ then by \nameref{borel_cantelli} we have $\P(A_j\text{ i.o.}) = 0$ s.t.
  \begin{align*}
    \forall  \omega  \in  \Omega  \ \exists J \text{ s.t. } \forall  j>J
  .\end{align*}
  it holds 
  \begin{align*}
    \abs{X_{k_j}(\omega ) - X(\omega )} \le  \frac{1}{j}
  .\end{align*}
\end{proof}
\subsection{Strong Law Of Large Numbers}
\begin{definition}
 A sequence of random variables $X_{1},\ldots ,X_n$  is called identically distributed if 
 \begin{align*}
   F_{X_{1}}(x)= F_{X_{2}}(x) = \ldots  = F_{X_n}(x) \quad \forall x \in  \mathbb{R}^{d}  
 .\end{align*}
 If additionally $X_{1},\ldots ,X_n$ are independent then we say they are identically-independent-distributed i.i.d
\end{definition}
\begin{theorem}[Strong Law Of Large Numbers]
 Let $X_{1},\ldots ,X_N$  be a sequence of i.i.d integrable random variables on the same probability
 space $(\Omega ,\mathcal{F},\P)$ then 
 \begin{align*}
   \P(\lim_{N \to  \infty} \frac{X_1 + \ldots  + X_N}{N} = \E[X_i]) = 1
 .\end{align*}
 where $\E[X_i] = \E[X_j]$ 
\end{theorem}
\begin{proof}
  Suppose for simplicity $\E[X^{4} ] < \infty$  for $\forall  i = 1,2,\ldots $.
  Then without loss of generality we may assume $\E[X_i] = 0$ otherwise we use $X_i - \E[X_i]$ as our new sequence.
  Consider 
  \begin{align*}
    \E[(\sum_{i=1}^{N} X_i )^{4} ] = \sum_{i,j,k,l}\E[X_iX_jX_kX_l]
  .\end{align*}
  If $i \neq j ,k,l$ then because of independence it follows that 
  \begin{align*}
    \E[X_iX_jX_kX_l] = \E[X_i]\E[X_jX_kX_l] = 0
  .\end{align*}
  Then 
  \begin{align*}
    \E[(\sum_{i=1}^{N}X_i )^{4} ] &= \sum_{i=1}^{N}\E[X_i^{4} ]  + 3 \sum_{i\neq j}\E[X_i^2X_j^2] \\
                                  &= N\E[X_{1}^{4} ] + 3(N^2-N)\E[X_{1}^2]^2 \\
                                  &\le  N^2C
  .\end{align*}
  Therefore for fixed $\epsilon > 0$
  \begin{align*}
    \P(\abs{\frac{1}{N} \sum_{i=1}^{N}X_i } \ge \epsilon) &= \P(\abs{\sum_{i=1}^{N}X_i }^{4}  \ge (\epsilon N)^{4} )\\
                                                          &\myS{Mrkv.}{\le} \frac{1}{(\epsilon N)^{4} } \E[\abs{\sum_{i=1}^{N} X_i }^{4} ]\\
                                                          &\le  \frac{C}{\epsilon^4}\frac{1}{N^2}
  .\end{align*}
  Then by \nameref{borel_cantelli} we get 
  \begin{align*}
    \P(\abs{\frac{1}{N} \sum_{i=1}^{N} X_i } \ge  \epsilon \text{ i.o.}) = 0
  .\end{align*}
  because 
  \begin{align*}
    \sum_{N=1}^{\infty} \P(A_N)  = \sum_{N=1}^{\infty} \frac{C}{\epsilon^4}\frac{1}{N^2}  < \infty
  .\end{align*}
  where  
  \begin{align*}
    A_N = \{\omega  \in  \Omega  \ : \ \abs{\frac{1}{N}\sum_{i=1}^{N}X_i } \ge  \epsilon\}  
  .\end{align*}
  Now we take $\epsilon = \frac{1}{k}$ then the above gives 
  \begin{align*}
    \lim_{N\to \infty} \sup \frac{1}{N} \sum_{i=1}^{N}  X_i(\omega ) \le \frac{1}{k}
  .\end{align*}
  holds except for $\omega  \in  B_k$ with $\P(B_k) = 0$. Let $B = \bigcup_{k=1}^{\infty} B_k $ then $\P(B) = 0$ and
  \begin{align*}
    \lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^{N} X_i(\omega ) = 0  \text{ a.e.}
  .\end{align*}
\end{proof}
\subsection{Conditional Expectation}
\begin{definition}
  Let $Y$ be random variable, then $\E[X|  Y] $ is defined as a $\mathcal{U}(Y)-$measurable random variable
  s.t for $\forall  A \in \mathcal{U}(Y)$ it holds 
  \begin{align*}
    \int_A X d\P = \int_A \E[X|Y] d\P
  .\end{align*}
\end{definition}
\begin{definition}
 Let $(\Omega ,\mathcal{F},\P)$  be a probability space and $\mathcal{ U} \subset  \mathcal{F}$ be a $\sigma-$algebra,
 if $X  : \Omega  \to  \mathbb{R}^{d} $ is an integrable random variable then $\E[X |\mathcal{U}]$  is
 defined as a random variable on $\Omega$ s.t. $\E[X | \mathcal{U}]$ is $\mathcal{U}-$measurable and for $\forall A \in  \mathcal{U}$
 \begin{align*}
   \int_A X d\P  = \int_A \E[X | \mathcal{U}] d \P
 .\end{align*}
\end{definition}
\begin{exercise}
 Proof the following equalities  
 \begin{enumerate}
   \item $\E[X|Y] = \E[X | \mathcal{U}]$
   \item $\E[\E[X|\mathcal{U}]] = \E[X]$
   \item $\E[X] = \E[X | \mathcal{W}]$, where $\mathcal{W} = \{\emptyset,\Omega \}  $
 \end{enumerate}
\end{exercise}
\begin{remark}
 One can define the conditional probability similarly. Let $\mathcal{V} \subset  \mathcal{U}$  be a $\sigma-$algebra 
 then for $A \in  \mathcal{U}$ the conditional probability is defined as follows
 \begin{align*}
   \P(A | \mathcal{V}) = \E[\cha_A | \mathcal{V}]
 .\end{align*}
Note the equivalent notation $\chi_A \equiv \cha_A$
\end{remark}
\begin{theorem}
 Let $X$ be an integrable random variable, then for all $\sigma-$algebras $\mathcal{U} \subset  \mathcal{F}$  the 
 conditional expectation $\E[X | \mathcal{U}]$ exists and is unique up to $\mathcal{U}-$measurable sets of probability
 zero
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
\begin{theorem}[Properties Of Conditional Expectation]
 \begin{enumerate}
   \item If $X$ is $\mathcal{U}-$measurable then $\E[X|\mathcal{U}] = X$ a.s.
   \item $\E[aX + bY|\mathcal{U}] = a\E[X|\mathcal{U}] + b \E[Y|\mathcal{Y}]$
   \item If $X$ is $\mathcal{U}-$measurable and $XY$ is integrable then 
     \begin{align*}
       \E[XY|\mathcal{U}] = X \E[Y|\mathcal{Y}]
     .\end{align*}
   \item If $X$ is independent of $\mathcal{U}$ then $\E[X|\mathcal{U}] = \E[X]$ a.s.
   \item If $\mathcal{W} \subset  \mathcal{U}$ are two $\sigma-$algebras then 
     \begin{align*}
       \E[X|\mathcal{W}] = \E[\E[X|\mathcal{U}]|\mathcal{W}] = \E[\E[X|\mathcal{W}]|\mathcal{U}] \text{ a.s.}
     .\end{align*}
   \item If $X\le Y$ a.s. then $\E[X|\mathcal{U}] \le \E[Y\mathcal{U}]$  a.s.
 \end{enumerate} 
\end{theorem}
\begin{exercise}
 Proof the above properties  
\end{exercise}
\begin{lemma}[Conditional Jensen's Inequality]
  Suppose $\phi  : \mathbb{R}\to \mathbb{R}$ is convex and $\E[\phi(x)] < \infty$ then
  \begin{align*}
    \phi(\E[X|\mathcal{U}]) \le \E[\phi(X)|\mathcal{U}]
  .\end{align*} 
\end{lemma}
\begin{exercise}
 Proof the above Lemma 
\end{exercise}
\subsection{Stochastic Processes And Brownian Motion}
\begin{definition}[Stochastic Process]
 A stochastic process is a parameterized collection of random variables 
 \begin{align*}
   (X(t))_{t \in [0,T]} \ : [0,T] \times \Omega  \ : \ \ (t,\omega ) \mapsto X(t,\omega )
 .\end{align*}
 For $\forall  \omega  \in  \Omega $ the map 
 \begin{align*}
   X(*,\omega ) \ : \ [0,T] \to \mathbb{R}^{d}  \ : \ t \mapsto X(t,\omega )
 .\end{align*}
 is called sample path
\end{definition}
\begin{definition}[Modification and Indistinguishable]
 Let $X(*)$  and $Y(*)$ be two stochastic processes, then we say they are modifications of each other if 
 \begin{align*}
   \P(X(t) = Y(t))  = 1 \qquad \forall t \in [0,T] 
 .\end{align*}
 We say they are indistinguishable if 
 \begin{align*}
   \P(X(t) = Y(t) \ \forall  t \in  [0,T])  = 1 
 .\end{align*}
\end{definition}
\begin{remark}
 Note that if two stochastic processes are indistinguishable then they are also always a modification of each other,
 the reverse is not always true.
\end{remark}
\begin{definition}[History]
 Let $X(t)$ be a real valued process. The $\sigma-$algebra 
 \begin{align*}
  \mathcal{U}(t) \coloneqq  \mathcal{U}(X(s) \ | \ 0\le s\le t)
 .\end{align*}
 is called the history of $X$ until time $t\ge 0$
\end{definition}
\begin{definition}[Martingale]
  Let $X(t)$ be a real valued process and $\E[\abs{X(t)}] < \infty$  for $\forall t \ge 0$
  \begin{enumerate}
    \item If $X(s) = \E[X(t)|\mathcal{U}(s)]$ a.s. $\forall  t \ge  s \ge  0$  then $X(*)$ is called a martingale
    \item If $X(s) \lesseqqgtr  \E[X(t)|\mathcal{U}(s)]$ a.s. $\forall  t \ge  s \ge  0$  then $X(*)$ is called a (super) sub-martingale
  \end{enumerate}
\end{definition}
\begin{lemma}
  Suppose $X(*)$ is a real-valued martingale and $\phi  : \mathbb{R} \to  \mathbb{R}$ a convex function.
  If $\E[\abs{\phi(X(t))}] < \infty $ for $\forall  t\ge 0$ then $\phi(X(*))$ is a sub-martingale
\end{lemma}
\begin{theorem}[Martingale-Inequalities]
 Assume $X(*)$  is a process with continuous sample paths a.s. 
 \begin{enumerate}
   \item If $X(*)$ is a sub-martingale then $\forall  \lambda > 0$ , $t \ge 0$ it holds 
     \begin{align*}
       \P(\max_{0\le s\le t} X(s) \ge \lambda ) \le  \frac{1}{\lambda }\E[X(t)^{+} ]
     .\end{align*}
    \item If $X(*)$ is a martingale and $1 < p < \infty$ then
      \begin{align*}
        \E[\max_{0\le s\le t} \abs{X(s) }^{p} ] \le (\frac{p}{p-1})^{p} \E[\abs{X(t)}^{p} ]
      .\end{align*}
 \end{enumerate}
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
\subsection{Brownian Motion}
\begin{definition}[Brownian Motion]
 A real valued stochastic process $W(*)$ is called a Brownian motion 
 or Wiener process if 
 \begin{enumerate}
   \item $W(0) = 0$ a.s.
   \item $W(t)$ is continuous a.s.
   \item $W(t) - W(s) \sim \mathcal{N}(0,t-s)$ for $\forall t\ge s\ge 0$
   \item $\forall \ 0 < t_{1}<t_{2}<\ldots <t_n$ , $W(t_{1}),W(t_{2})-W(t_{1}),\ldots ,W(t_n)-W(t_{n-1})$ are independent 
 \end{enumerate}
\end{definition}
\begin{remark}
 One can derive directly that 
 \begin{align*}
   \E[W(t)] = 0 \quad \E[W^2(t)] = t \qquad \forall t \ge 0
 .\end{align*}
\end{remark}
Furthermore based on the above remark for $t\ge s$ 
\begin{align*}
  \E[W(t)W(s)] &= \E[(W(t)-W(s))(W(s))]+\E[(W(s)w(s))]\\
               &= \E[W(t)-W(s)]\E[W(s)] + \E[W(s)W(s)] \\
               &= s
.\end{align*}
which means generally 
\begin{align*}
  \E[W(t)W(s)] = t \land s
.\end{align*}
\begin{definition}
 An $\mathbb{R}^{d} $  valued process $W(*) = (W^{1}(*),\ldots ,W^{d}(*)  )$ is a $d-$dimensional Wiener process (or Brownian motion) if
 \begin{enumerate}
   \item $W^{k}(*) $ is a 1-$D$ Wiener process for $\forall  k =1 ,\ldots ,d$
   \item $\mathcal{U}(W^{k}(t) \ , \ t\ge 0 )$ $\sigma-$algebras are independent $k=1,\ldots ,d$
 \end{enumerate}
\end{definition}
\begin{remark}
 If $W(*)$  is  a $d-$Dimensional Brownian motion, then $W(t) \sim \mathcal{N}(0,t)$ and for any Borel set $A \subset  \mathbb{R}^{2} $
 \begin{align*}
  \P(W(t) \in  A) = \frac{1}{(2\pi t)^{\frac{n}{2}} } \int_A e^{-\frac{\abs{x}^2}{2t}} dx
 .\end{align*}
\end{remark}
\begin{theorem}
 If $X(*)$  is a given stochastic process with a.s. continuous sample paths and 
 \begin{align*}
   \E[\abs{X(t)-X(s)}^{\beta } ] \le  C \abs{t-s}^{1+\alpha } 
 .\end{align*}
 Then for $\forall 0 < \gamma  < \frac{\alpha }{\beta }$ and $T > 0$ a.s. $\omega$, there $\exists  K = K(\omega ,\gamma ,T)$ s.t.
 \begin{align*}
  \abs{X(t,\omega )-X(s,\omega )}\le K \abs{t-s}^{\gamma } \quad \forall  0 \le s, t \le T 
 .\end{align*}
\end{theorem}
\begin{proof}
 Omit 
\end{proof}
An application of this result on Brownian motion is interesting since 
\begin{align*}
  \E[\abs{W(t)-W(s)}^{2m} ] \le  C \abs{t-s}^{m}  
.\end{align*}
we get immediately 
\begin{align*}
  W(*,\omega ) \in  \mathcal{C}^{\gamma }([0,T])  \quad 0<\gamma <\frac{m-1}{2m} < \frac{1}{2} \ \forall  m \gg 1
.\end{align*}
This means that Brownian motions is a.s. path Hölder continuous up to exponent $\frac{1}{2}$
\begin{remark}
  One can also further prove that the path wise smoothness  of Brownian motion can not be better than Hölder  continuous. Namely 
  \begin{enumerate}
    \item $\forall  \gamma  \in  (\frac{1}{2},1]$  and a.s. $\omega , t \mapsto W(t,\omega )$ is nowhere Hölder  continuous with exponent $\gamma $
    \item $\forall $ a.s. $\omega  \in  \Omega $ the map $t \mapsto W(t,\omega )$ is nowhere differentiable and is of infinite variation on each subinterval.
  \end{enumerate}
\end{remark}
\begin{definition}[Markov Property]
 An $\mathbb{R}^{d}-$valued process $X(*)$ is said to have the Markov property, if $\forall  0\le s\le t$ and 
 $\forall B \subset  \mathbb{R}^{d} $ Borel. , it holds 
 \begin{align*}
   \P(X(t) \in  B | \mathcal{U}(s)) = \P(X(t) \in  B | X(s)) \text{ a.s.}
 .\end{align*}
\end{definition}
\begin{remark}
 The $d-$Dimensional Wiener Process $W(*)$  has Markov property and 
 \begin{align*}
  \P(W(t)\in B | W(s)) = \frac{1}{(2\pi(t-s))^{\frac{n}{2}} } \int_B e^{-\frac{\abs{x - W(s)}^2}{2(t-s)}}  dx \text{ a.s.}
 .\end{align*}
\end{remark}
\section{It\^o Integral}
From now on we denote by $W(*)$ the $1-D$ Brownian motion on $(\Omega ,\mathcal{F},\P)$
\begin{definition}
  \hspace{0mm}\\
  \begin{enumerate}
    \item $\mathcal{W}(t) = \mathcal{U}(W(s) | 0 \le s \le t)$ is called the history up to t
    \item The $\sigma-$algebra 
      \begin{align*}
        \mathcal{W}^{+}(t) \coloneqq \mathcal{U}(W(s)-W(t) | s\ge t) 
      .\end{align*}
      is called the future of the Brownian motion beyond time $t$
  \end{enumerate}
\end{definition}
\begin{definition}[Non-Anticipating Filtration]
 A family $\mathcal{F}(*)$  of $\sigma-$algebras is called non-anticipating (w.r.t $W(*)$) if 
 \begin{enumerate}
   \item $\mathcal{F}(t) \supseteq \mathcal{F}(s)$ for $\forall t \ge  s \ge 0$ 
   \item $\mathcal{F}(t) \supseteq \mathcal{W}(t)$ for $\forall t \ge 0$ 
   \item $\mathcal{F}(t)$ is independent of $\mathcal{W}^{+}(t) $ for $\forall t \ge 0$ 
 \end{enumerate}
\end{definition}
A primary example of this is 
\begin{align*}
  \mathcal{F}(t) \coloneqq \mathcal{U}(W(s) , 0\le s\le t, X_{0})
.\end{align*}
where $X_{0}$ is a random variable independent of $\mathcal{W}^{+}(0) $
\begin{definition}[Non-Anticipating Process]
 A real-valued stochastic process $G(*)$  is called non-anticipating (w.r.t. $\mathcal{F}(*)$) if 
 for $\forall t \ge  0$ , $G(t)$ is $\mathcal{F}(t)-$measurable
\end{definition}
From now on we use $(\omega,\mathcal{F},\mathcal{F}(t),\P)$ as a filtered probability space with right continuous filtration 
$\mathcal{F}(t) = \bigcap_{s \ge t} \mathcal{F}(s)$. Note we also use the convention that $\mathcal{F}(t)$ is complete 
\begin{definition}
  \hspace{0mm}\\
  \begin{enumerate}
    \item A stochastic process is adapted to $(\mathcal{F}(t))_{t\ge 0}$  if $X_t$ is $\mathcal{F}(t)$ measurable for $\forall  t \ge 0$
    \item A stochastic process is progressively measurable w.r.t. $\mathcal{F}(t)$ if
      \begin{align*}
        X_t(s,\omega ) \ : \ [0,t] \times  \Omega  \to  \mathbb{R}
      .\end{align*}
      is $\mathcal{B}([0,t]) \times  \mathcal{F}(t)$ measurable for $\forall  t > 0$
  \end{enumerate}
\end{definition}
\begin{definition}
  We denote $\mathbb{L}^2([0,T])$  the space of all real-valued progressively measurable stochastic processes $G(*)$ s.t.
  \begin{align*}
    \E[\int_0^{T} G^2 dt ] < \infty
  .\end{align*}
  We denote $\mathbb{L}^{1}([0,T]) $ the space of all real-valued progressively measurable stochastic processes $F(*)$ s.t.
  \begin{align*}
    \E[\int_0^{T} \abs{F} dt ] < \infty
  .\end{align*}
\end{definition}
\begin{definition}[Step-Process]
  $G \in  \mathbb{L}^2([0,T])$  is called a step process if there exists a partition of the interval $[0,T]$ i.e.
  $P = \{0 = t_{0} < t_{1} < \ldots <t_m =T\}$ s.t. 
  \begin{align*}
    G(t) = G_k \quad \forall  t_k \le t < t_{k+1} \quad k=0,\ldots ,m-1
  .\end{align*}
  where $G_k$ is an $\mathcal{F}(t_k)$ measurable random variable
\end{definition}
\begin{remark}
Note that the above definition directly yields the following representation for any step process $G \in  \mathbb{L}^2([0,T])$ 
\begin{align*}
  G(t,\omega ) = \sum_{k=0}^{m-1} G_k(\omega )*\cha_{[t_k,t_{k+1})}(t)
.\end{align*}
\end{remark}
\begin{definition}[(Simple) It\^o Integral]
  Let $G \in  \mathbb{L}^2([0,T])$  be a step process. Then we define 
  \begin{align*}
    \int_0^{T} G(t,\omega ) dW_t \coloneqq \sum_{k=0}^{m-1} G_k(\omega )*(W(t_{k+1},\omega )-W(t_k,\omega ))
  .\end{align*}
\end{definition}
\begin{prop}
  Let $G,H \in  \mathbb{L}^2([0,T])$  be two step processes, then for $\forall  a,b \in  \mathbb{R}$ it holds 
  \begin{enumerate}
    \item $\int_0^{T}(aG + bH)dW_t  = a \int_0^{T} G dW_t + b \int_0^{T} HdW_t  $
    \item $\E{\int_0^{T}GdW_t } = 0$
  \end{enumerate}
\end{prop}
\begin{proof}
  (1). This case is easy. Set 
  \begin{align*}
    G(t) &= G_k \quad t_k \le t <t_{k+1} \quad k=0,\ldots,m_1 -1
    H(t) &= H_l \quad t_l \le t <t_{l+1} \quad l=0,\ldots,m_2 -1
  .\end{align*}
  Let $0 \le  t_{0}<t_{1}<\ldots \le t_n=T$ be the collection of $t_k$'s and $t_k$'s which together form a new partition
  of $[0,T]$ then obviously $G,H \in  \mathbb{L}^2([0,T])$ are again step processes on this new partition. We have 
  directly the linearity by definition on the It\^o integral for step processes
  \begin{align*}
    \int_0^{T} (G+H)d W_t = \sum_{j=0}^{n-1} (G_j+H_j)*(W(t_{j+1})-W(t_j))
  .\end{align*}
  (2). By definition we have 
  \begin{align*}
    \E[\int_0^{T} GdW_t ] = \E[\sum_{k=0}^{m-1}G_k(W(t_{k+1})-W(t_k)) ] = \sum_{k=0}^{m-1} \E[G_k(W(t_{k+1})- W(t_k))] 
  .\end{align*}
  Notice that $G_k$ by definition is $\mathcal{F}_{t_k}$ measurable and $W(t_{k+1}) - W(t_k)$ is measurable in $\mathcal{W}^{+}(t_k) $. Since
  $\mathcal{F}_{t_k}$ is independent of $\mathcal{W}^{+}(t_k) $, we can deduce that $G_k$ is independent of $W(t_{k+1}) - W(t_k)$ which implies 
  \begin{align*}
    \sum_{k=0}^{m-1} \E[G_k(W(t_{k+1})- W(t_k))]  = \sum_{k=0}^{m-1} \E[G_k]*\E[W(t_{k+1}) - W(t_k)]  =0
  .\end{align*}
\end{proof}
\begin{lemma}[(Simple) It\^o isometry]
  For step processes $G \in  \mathbb{L}^2([0,T])$  we have 
  \begin{align*}
    \E[(\int_0^{T} G dW_t )^2] = \E[\int_0^{T} G^2 dt ]
  .\end{align*}
\end{lemma}
\begin{proof}
  By definition we can write 
  \begin{align*}
    \E[\left( \int_0^{T} G dW_t  \right)^2 ] = \sum_{k,j=0}^{m-1} \E[G_kG_j(W(t_{k+1})-W(t_k))(W(t_{j+1})-W(t_j))] 
  .\end{align*}
  If $j < k$, then $W(t_{k+1}) -W(t_k)$ is independent of $G_kG_j(W(t_{j+1})-W(t_j))$. Therefore 
  \begin{align*}
    \sum_{j<k}\E[\ldots ] = 0 \quad \text{ and }  \quad \sum_{j>k}\E[\ldots ] = 0
  .\end{align*}
  Then we have 
  \begin{align*}
    \E[\left( \int_0^{T} GdW_t  \right)^2 ] &= \sum_{k=0}^{m-1} \E[G_k^2(W(t_{k+1})-W(t_k))^2]  \\
                                            &= \sum_{k=0}^{m-1} \E[G_k^2]\E[(W(t_{k+1})-W(t_k))^2] \\
                                            &= \sum_{k=0}^{m-1} \E[G_k^2](t_{k+1}-t_k) \\
                                            &= \E[\int_0^{T} G^2dt ] 
  .\end{align*}
\end{proof}
For general $\mathbb{L}^2([0,T])$ processes we use approximation by step processes to define the It\^o integral 
\begin{lemma}
  If $G \in  \mathbb{L}^2([0,T])$  then there exists a sequence of bounded step processes  $G^{n} \in  \mathbb{L}^2([0,T])$  s.t. 
  \begin{align*}
   \E[\int_0^{T} \abs{G - G^{n} }^2 dt ] \xrightarrow{n\to \infty} 0
  .\end{align*}
\end{lemma}
\begin{proof}
  We roughly sketch the Idea here \\[1ex]
  If $G(*,\omega )$  is a.e. continuous then we can take  
 \begin{align*}
   G^{n}(t) \coloneqq  G(\frac{k}{n})  \quad \frac{k}{n} \le t < \frac{k+1}{n} \quad k=0,\ldots ,\floor{nT}
 .\end{align*}
 For general $G \in  \mathbb{L}^2([0,T])$ let 
 \begin{align*}
  G^{m}(t) \coloneqq  \int_0^{t} m e^{m(s-t)}G(s) ds   
 .\end{align*}
 Then $G^{m} \in  \mathbb{L}^2([0,T]) $ , $t \mapsto G^{m}(t,\omega ) $ is continuous for a.s. $\omega $ and 
 \begin{align*}
  \int_0^{T} \abs{G - G^{m} }^2 dt \to 0 \text{ a.s.}
 .\end{align*}
\end{proof}
\begin{definition}[It\^p Integral]\label{ito_integral}
  If $G \in  \mathbb{L}^2([0,T])$. Let step processes $G^{n} $ be an approximation of $G$. Then we define
  the It\^o  integral by using the limit 
  \begin{align*}
    I(G) = \int_0^{T} GdW_t \coloneqq  \lim_{n\to \infty} \int_0^{T} G^{n} dW_t
  .\end{align*}
  where the limit exists in $L^2(\Omega)$
\end{definition}
In order to derive the validity of this definition, one has to check 
\begin{enumerate}
  \item Existence of the limit. This can be obtained by showing that it is a Cauchy sequence, namely by It\o isometry we have
    \begin{align*}
      \E[\left( \int_0^{T} (G^{m} - G^{n}  ) dW_t  \right)^2 ] = \E[\int_0^{T} \abs{G^{m} - G^{n}  }^2 dt] \xrightarrow{n,m\to \infty} \to 0
    .\end{align*}
    This implies $\int_0^{T} G^{n} dW_t  $ has a limit in $L^2(\Omega )$ as $n\to \infty$
  \item The limit is independent of the choice of approximation sequences.
    Let $\tilde{G}^{n}  $ be another step process which converges to $G$. Then we have 
    \begin{align*}
      \E[\int_0^{T} \abs{\tilde{G}^{n} - G^{n}   }^2 dt ] \le  \E[\int_0^{T} \abs{G^{n} - G }^2 dt ] + \E[\int_0^{T} \abs{\tilde{G}^{n} - G  }^2 dt ]
    .\end{align*}
    it follows that 
    \begin{align*}
      \E[\left( \int_0^{T} \tilde{G}^{n} dW_t - \int_0^{T} G^{n} dW_t      \right)^2 ] = \E[\int_0^{T} \abs{\tilde{G}^{n} - G^{n}   }^2 dt ] \to 0
    .\end{align*}
\end{enumerate}
By using this approximation, all the properties for step  processes can be obtained for general $\mathbb{L}^2([0,T])$ processes
\begin{theorem}[Properties Of The It\^o Integral]
  For $\forall  a,b \in  \mathbb{R}$  and $\forall  G,H \in \mathbb{L}^2([0,T])$ it holds 
  \begin{enumerate}
    \item $\int_0^{T} (aG+bH) dW_t = a\int_0^{T} GdW_t + b\int_0^{T} H dW_t   $ 
    \item $\E[\int_0^{T} GdW_t ] = 0$
    \item $\E[\int_0^{T} GdW_t * \int_0^{T} HdW_t  ] = \E[\int_0^{T} GH dt ]$
  \end{enumerate}
\end{theorem}
\begin{lemma}[It\^o Isometry]
  For general $G \in  \mathbb{L}^2([0,T])$  we have 
  \begin{align*}
    \E[\left( \int_0^{T} G dW_t  \right)^2 ] = \E[\int_0^{T} G^2  dt ]
  .\end{align*}
\end{lemma}
\begin{proof}
  Choose step processes $G_{n} \in \mathbb{L}^2([0,T]) $  such that $G_{n} \to G $ (in the sense previously defined) then by \autoref{ito_integral} we get 
  \begin{align*}
    \|I(G) - I(G_n)\|_{L^2} \xrightarrow{n\to \infty} 0
  .\end{align*}
  Then using the simple version of It\^o isometry one obtains 
  \begin{align*}
    \E[\left(\int_0^{T} G dW_t\right)^2] = \lim_{n\to \infty} \E[\left( \int_0^{T} G_{n} dW_t   \right)^2 ]  = \lim_{n\to \infty} \E[\int_0^{T} (G_n)^2dt ] = \E[\int_0^{T} (G)^2dt ]
  .\end{align*}
\end{proof}
\begin{remark}
  The It\^o integral is a map from $\mathbb{L}^2([0,T]) $  to $L^2(\Omega )$
\end{remark}
\begin{remark}
  For $G \in \mathbb{L}^2([0,T])$  the It\^o integral $\int_0^{\tau } G dW_t $ with $0 \le \tau  \le T$ is a martingale
\end{remark}
\subsection{It\^o's Formula}
\begin{definition}[It\^o Process]
 Let $X(*)$ be a real-valued process given by 
 \begin{align*}
  X(r) = X(s) + \int_s^{r} F dt + \int_s^{r} GdW_t  
 .\end{align*}
 for some $F \in  \mathbb{L}^1([0,T])$ and $G \in  \mathbb{L}^2([0,T])$ for $0\le s\le r\le T$, then $X(*)$ is called It\^o process.\\[1ex]
 Furthermore we way $X(*)$ has a stochastic differential 
 \begin{align*}
  dX =  Fdt + gdW_t \quad \forall  0\le t\le T
 .\end{align*}
\end{definition}
\begin{theorem}[It\^o's Formula]
  Let $X(*)$  be an It\^o process given by $dX = F dt + GdW_t$ for some $F \in  \mathbb{L}^{1}([0,T]) $ and $G \in  \mathbb{L}^2([0,T])$. Assume 
  $u : \mathbb{R} \times  [0,T] \to \mathbb{R}$ is continuous and $\frac{\partial u}{\partial t} ,\frac{\partial u}{\partial x} ,\frac{\partial ^2 u}{\partial x^2} $ exists and 
  are continuous. Then $Y(t) \coloneqq  u(X(t),t)$ satisfies 
  \begin{align*}
    dY &= \frac{\partial u}{\partial t} dt + \frac{\partial u}{\partial x} dX + \frac{1}{2 }\frac{\partial ^2 u}{\partial x^2} G^2 dt\\
       &=(\frac{\partial u}{\partial t} +\frac{\partial u}{\partial x} F + \frac{1}{2} \frac{\partial ^2 u}{\partial x^2} G^2 )dt + \frac{\partial u}{\partial x} G dW_t
  .\end{align*}
  Note that the differential form of the It\^o formula is understood as an abbreviation of the following integral form, for all $0\le s < r \le T$
  \begin{align*}
    &u(X(r),r) - u(X(s),s) \\
    &= \int_s^{r}(\frac{\partial u}{\partial t}(X(t),t)+\frac{\partial u}{\partial x}(X(t),t)F(t) + \frac{1}{2}\frac{\partial ^2 u}{\partial x^2}(X(t),t)G^2(t) )dt + \int_s^{r} \frac{\partial u}{\partial x}(X(t),t) G(t)dW_t
  .\end{align*}
  \end{theorem}
\begin{proof}
  The proof is split into five steps \\[1ex]
  \textbf{Step 1.} First we prove two simple cases. If $X(t)=W_t$ then 
  \begin{enumerate}
    \item $d(W_t)^2 = 2W_t dW_t + dt $
    \item $d(tW_t) = W_t dt + t dW_t$
  \end{enumerate}
  For (1) it is sufficient to prove $W_t^2 - W_0^2 = \int_0^{t} 2W_s dW_s + t$ a.s. By definition of It\^o integral, for a.s. $\omega  \in \Omega $ we have 
  \begin{align*}
    \int_0^{t}2W_sdW_s &= 2 \lim_{n\to \infty} \sum_{k=0}^{n-1} W(t_k^{n} )\left(W(t_{k+1}^{n})-W(t_k^{n} )\right)\\
                       &= \lim_{n\to \infty} \Bigg[\sum_{k=0}^{n-1} W(t_k^{n})\left(W(t_{k+1}^{n}) - W(t_k^{n} )\right) - \sum_{k=0}^{n-1}\left(W(t_{k+1}^{n})-W(t_k^{n}) \right)  \\
                       & \hspace{1.1cm}+ \sum_{k=0}^{n-1}W(t_{k+1}^{n} )  \left(W(t_{k+1}^{n})-W(t_k^{n} ) \right) \Bigg]\\
                       &= - \lim_{n\to \infty} \Bigg[\sum_{k=0}^{n-1}\left( W(t_{k+1}^{n}) - W(t_k^{n} ) \right)^2 -  \sum_{k=0}^{n-1}\left(W(t_k^{n} )\right)^2 + \sum_{k=0}^{n-1}\left( W(t_{k+1}^n) \right)^2      \Bigg]\\
                       &= - \lim_{n\to \infty} \sum_{k=0}^{n-1}\left( W(t_{k+1}^{n}) - W(t_k^{n} ) \right)^2 +  \left(W(t)\right)^2 - \left( W(0) \right)^2 
  .\end{align*}
  where for any fixed $n$, the partition of $[0,T]$ is given by $0\le t_{0}^{n} < t_{1}^{n} < \ldots <t_n^{n} = T   $ and 
  $t_{k}^{n} - t_{k+1}^{n}   = \frac{1}{n}$ . It remains to prove that the limit 
  \begin{align*}
    \lim_{n\to \infty}\sum_{k=0}^{n-1} \left( W(t_{k+1}^{n} ) - W(t_k^{n} ) \right)^2 - t = 0 
  .\end{align*}
  holds true. Actually 
  \begin{align*}
    &\E \left[ \sum_{k=0}^{n-1}\left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)^2   \right] \\
    &=\E[\sum_{k=0}^{n-1}\sum_{l=0}^{n-1} \left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)*\left( \left( W(t_{l+1}^{n} ) - W(t_l^{n})\right)^2 - \left( t_{l+1}^{n} - t_l^{n}   \right)   \right) ]
  .\end{align*}
  The terms with $k\neq l$ vanish because of the independence. Therefore
  \begin{align*}
    &\E \left[ \sum_{k=0}^{n-1}\left( \left( W(t_{k+1}^{n} ) - W(t_k^{n})\right)^2 - \left( t_{k+1}^{n} - t_k^{n}   \right)   \right)^2   \right] \\ 
    &= \sum_{k=0}^{n-1}(t_{k+1}^{n} - t_k^{n}  )^2 \E \left[ \left( \frac{\left( W(t_{k+1}^{n} ) - W(t_k^{n} ) \right)^2 }{t_{k+1}^{n} - t_{k}^{n}  } - 1 \right)^2  \right]  \\
    &= \sum_{k=0}^{n-1}(t_{k+1}^{n} - t_k^{n}  )^2 \E \left[ \left( \left(\frac{ W(t_{k+1}^{n} ) - W(t_k^{n} ) }{\sqrt{t_{k+1}^{n} - t_{k}^{n}}  } \right)^2 - 1 \right)^2  \right]  \\
    &\le C*\frac{t^2}{n}\\
    &\to  0
  .\end{align*}
  where we have used the fact that $Y = \frac{ W(t_{k+1}^{n} ) - W(t_k^{n} ) }{\sqrt{t_{k+1}^{n} - t_{k}^{n}}  } \sim \mathcal{N}(0,1)$. Hence $\E[(Y^2 - 1)^2]$ is 
  bounded by a constant $C$\\[1ex]
  For (2) : It is sufficient to prove $tW_t - 0 W_0 =  \int_0^{t} W_s ds + \int_0^{t} s dW_s  $. Actually we have 
  \begin{align*}
    \int_0^{t} s dW_s = \lim_{n\to \infty}  \sum_{k=0}^{n-1} t_k^{n}\left( W(t_{k+1}^{n} - W(t_k^{n} )  ) \right)  \text{ a.s.}
  .\end{align*}
  and for a.s. $\omega $ the standard Riemann sum
  \begin{align*}
    \int_0^{t} W_s ds = \lim_{n\to \infty}  \sum_{k=0}^{n-1} W(t_{k+1}^{n} )(t_{k+1}^{n} - t_k^{n}  )
  .\end{align*}
  The summation of the above integrals yields 
  \begin{align*}
    \int_0^{t} s dW_s  + \int_0^{t} W_s ds &= \lim_{n\to \infty}   \sum_{k=0}^{n-1} t_k^{n}\left( W(t_{k+1}^{n}) -W(t_k^{n} ) \right) + \lim_{n\to \infty}\sum_{k=0}^{n-1} W(t_{k+1}^{n} ) (t_{k+1}^{n} - t_k^{n}  )\\
                                           &= W(t)*t - 0*W(0)
  .\end{align*}
  \textbf{Step 2.} Now let us prove the It\^o product rule. If 
  \begin{align*}
    dX_{1} = F_{1}dt + G_{1}dW_t \quad \text{ and } \quad dX_{2} = F_{2}dt+G_{2}dW_t
  .\end{align*}
  for some $G_i \in  \mathbb{L}^2([0,T])$ and $F_i \in  \mathbb{L}^1([0,T])$ $i=1,2$ , then 
  \begin{align*}
    d(X_{1}X_{2}) &= X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2} dt
                  &=(X_{2}F_{1}+X_{1}F_{2}+G_{1}G_{2})dt + (X_{2}G_{1}+X_{1}G_{2})dW_t
  .\end{align*}
  where the above should be understood as the integral equation.\\
  (1) We prove the case $F_i,G_i$ are time independent. Assume for simplicity $X_{1}(0) = X_{2}(0)$ then it follows that 
  \begin{align*}
    X_i(t) = F_it G_iW(t)
  .\end{align*}
  Then it holds a.s. that 
  \begin{align*}
    &\int_0^{t} (X_{2}dX_{1} + X_{1}dX_{2} + G_{1}G_{2} ds) \\
    &= \int_0^{t} (X_{2}F_{1}+X_{1}F_{2}) ds + \int_0^{t} (X_{2}G_{1}+X_{1}G_{2}) dW_s + \int_0^{t} G_{1}G_{2}ds   \\
    &= \int_0^{t} \left( F_{1}(F_{2}s + G_{2}W(s))  + F_{2}(F_{1}s+G_{1}W(s))\right) ds + G_{1}G_{2}t\\
    &= \int_0^{t} \left( G_{1}(F_{2}s + G_{2}W(s)) + G_{2}(F_{1}s+G_{1}W(s)) \right) dW_s \\
    &= G_{1}G_{2}t F_{1}F_{2}t^2+(F_{1}G_{2}+F_{2}G_{1})\left( \int_0^{t}W(s) ds + \int_0^{t} s dW_s   \right) \\
    & \quad + 2G_{1}G_{2} \int_0^{t} W(s) dW_s 
  .\end{align*}
  using (1) and (2) from Step 1. It continues to hold that 
  \begin{align*}
    G_{1}G_{2}(W(t))^2  + F_{1}F_{2}t^2 + (F_{1}G_{2}+F_{2}G_{1})tW(t) = X_{1}(t)+X_{2}(t)
  .\end{align*}
  Therefore It\^o formula is true when $F_i,G_i$ are time independent random variables.\\
  (2) If $F_i,G_i$ are step processes, then we apply the above formula in each sub-interval\\
  (3) For $F_i \in  \mathbb{L}^1([0,T])$ and $G_i \in  \mathbb{L}^2([0,T])$, we take the step process approximation of them, namely
  \begin{align*}
    \E[\int_0^{T} \abs{F_i^{n} - F_i } dt ] \to 0 \quad \E[\int_0^{T} \abs{G_i^{n} - G_i }^2 dt ] \to 0 \qquad (n\to \infty), i=1,2
  .\end{align*}
  Notice that for each It\^o process given by step processes 
  \begin{align*}
    X_i^{n}(t) = X_i(0) + \int_0^{t} F_i^{n} ds + \int_0^{t} G_i^{n} dW_s     
  .\end{align*}
  the product rule holds, i.e. 
  \begin{align*}
    X_1^{n}(t)X_2^{n}(t) - X_1(0)X_2(0) = \int_0^{t} \left( X_1^{n}(s)dX_2^{n}(s) + X_2^{n}(s) dX_1^{n}(s) + G_{1}G_{2}ds     \right)    
  .\end{align*}
  \textbf{Step 3.} If $u(X) = X^{m} $ for $m \in  \mathbb{N}$ then we claim 
  \begin{align*}
    d(X^{m} ) = mX^{m-1}dX + \frac{1}{2}m(m-1) X^{m-2} G^2dt
  .\end{align*}
  We prove this by induction.\\
  \textbf{IA} Note that $m=2$ is given by the product rule. \\
  \textbf{IV} Suppose the formula holds for $m-1 \in  \mathbb{N}$ \\
  \textbf{IS} $m-1 \to m$ then 
  \begin{align*}
    d(X^{m} )= d(XX^{m-1} ) &= Xd(X^{m-1} ) + X^{m-1}dX + (m-1)X^{m-2} G^2dt \\
                            &\myS{IV}{=} X \left( (m-1)X^{m-2} dX + \frac{1}{2}(m-1)(m-2)X^{m-3}G^2 dt   \right) \\
                            & \quad + X^{m-1}dX + (m-1)X^{m-2}G^2 dt \\
                            &= mX^{m-1} dX + (m-1)(\frac{m}{2} -1 +1) X^{m-2} G^2 dt 
  .\end{align*}
  Thus the statement holds for all $m \in  \mathbb{N}$\\[1ex]
  \textbf{Step 4.} If $u(X,t) = f(X)g(t)$ where $f$ and $g$ are polynomials $f(X) = X^{m} $ , $g(t) =t^n$.
  Then by the product rule we have 
  \begin{align*}
    d(u(X,t)) = d(f(X)g(t)) = f(X)dg + g df(X) + (G_{1}*0) dt
  .\end{align*}
  by step 3  this is equal to
  \begin{align*}
    f(X)g'(t)dt + gf'(X)dX + \frac{1}{2}f^{''}(X)G^2 dt =\frac{\partial u}{\partial t} dt + \frac{\partial u}{\partial X} dX + \frac{1}{2} \frac{\partial ^2 u}{\partial X^2} G^2 dt
  .\end{align*}
  Note the It\^o formula is also true if $u(X,t) = \sum_{i=1}^{m} g_m(t)f_m(X) $ where $f_m$ and $g_m$ are polynomials\\[1ex]
  \textbf{Step 5.} For $u$ continuous such that $\frac{\partial u}{\partial t} , \frac{\partial u}{\partial x} ,\frac{\partial ^2 u}{\partial x^2}$ exists and are also continuous, then
  there exists polynomial sequences $u^{n} $ s.t.
  \begin{align*}
    u^{n} \to  u \quad \frac{\partial u^{n } }{\partial t} \to \frac{\partial u}{\partial t} , \quad \frac{\partial u^{n } }{\partial x} \to \frac{\partial u}{\partial x} , \quad \frac{\partial ^2 u}{\partial x^2} \to \frac{\partial ^2 u}{\partial x^2} 
  .\end{align*}
  uniformly on compact $K \subset  \mathbb{R}\times [0,T]$. Since 
  \begin{align*}
    u^{n}(X(t),t) -u^{n}(X(0),0)  = \int_0^{t} \left( \frac{\partial u^{n } }{\partial t} +\frac{\partial u^{n } }{\partial x} F + \frac{1}{2} \frac{\partial ^2 u^{n} }{\partial x^2} G^2  \right)  dr + \int_0^{t} \frac{\partial u^{n } }{\partial x}  GdW_r \quad \text{a.s.} 
  .\end{align*}
  then by taking the limit $n\to \infty$ It\^o's formula is proven 
\end{proof}
\begin{remark}
 One can get the existence of the polynomial sequence in Step 5, by using Hermetian polynomials 
 \begin{align*}
  H_n(x) = (-1)^{n}  e^{\frac{x^2}{2}} \frac{d^{n}}{d x^{n} } e^{-\frac{x^2}{2}} 
 .\end{align*}
\end{remark}
\begin{exercise}
  If $u \in  \mathcal{C}^{\infty} $  , $\frac{\partial u}{\partial x} \in  \mathcal{C}_b$ then prove Step 4 $\implies$ Step 5 \\[1ex]
  \textit{Use Taylor expansion and use the uniform convergence of the Taylor series on compact support }
\end{exercise}
\newpage
\subsection{Multi-Dimensional It\^o processes and Formula}
We shortly extend the definition of It\^o processes and the It\^o Formula to the multi-dimensional case, we
include the dimensionality as a subscript for clearness.
\begin{definition}[Multi-Dimensional It\^o's Integral]
  We the define the $n-$dimensional It\^o integral for $G \in  \mathbb{L}^{2}_{n*m}([0,T]) $ , $G_{ij} \in  \mathbb{L}^{2}([0,T])$ $1\le i\le n \ , \ 1 \le j \le m$
  \begin{align*}
    \int_0^{T} G d W_t = \begin{pmatrix} \vdots \\ \int_0^{T} G_{ij} d W^{j}_t \\ \vdots    \end{pmatrix}_{n \times 1}
  .\end{align*}
  With the Properties 
  \begin{align*}
    \E[\int_0^{T} G dW_t ] &= 0  \\
    \E[(\int_0^{T} G dW_t )^2] &= \E[\int_0^{T} \abs{G}^2 dt ]
  .\end{align*}
  Where $\displaystyle\abs{G}^2 = \sum_{i,j}^{n,m} \abs{G_{ij}}^2 $ 
\end{definition}
\begin{definition}[Multi-Dimensional It\^o process]
 We define the $n-$dimensional It\^o process as  
 \begin{align*}
   X(t) &= X(s) + \int_s^{t} F_{n \times  1}(r) dr   + \int_0^{t} G_{n \times  m}(r) dW_{m \times  1}(r)  \\
   dX^{i} &= F^{i} dt + \sum_{j=1}^{m} G^{ij} dW_t^i      \qquad 1\le i \le n
 .\end{align*}
\end{definition}
\begin{theorem}[Multi Dimensional Ito's formula]
  We define the $n-$dimensional It\^o's formula for $u \in  \mathcal{C}^{2,1}(\mathbb{R}^{n} \times [0,T],\mathbb{R} ) $ by 
  \begin{align*}
    du(X(t),t) &= \frac{\partial u}{\partial t}(X(t),t) dt + \nabla u(X(t),t) * dX(t) \\
               &+ \frac{1}{2} \sum \frac{\partial ^2 u}{\partial X_i \partial X_j}(X(t),t) \sum_{l=1}^{m}  G^{il} G^{il}dt 
  .\end{align*}
\end{theorem}
\begin{prop}
  For real valued processes $X_{1},X_{2}$
 \begin{align*}
  \begin{cases}
    dX_{1} &= F_{1} dt + G_1 dW_1 \\
    d X_2 &= F_{2} dt + G_{2} dW_2
  \end{cases} \implies d(X_{1},X_{2}) = XdX_{2} + X_{2}dX_{1} + \sum_{k=1}^{m} G_1^{k} G_2^{k} dt   
 .\end{align*} 
\end{prop}
\begin{definition}[Multiplication Rules]
 Formal multiplication rules for SDEs
 \begin{align*}
   (dt)^2 = 0 \ , \ dt dW^{k} = 0 \ , \ dW^{k}dW^{l} = \delta_{kl} dt \\
 .\end{align*}
\end{definition}
\newpage
\begin{remark}
  Using the above we can simplify It\^o's formula as follows 
\begin{align*}
  du(X,t) &= \frac{\partial u}{\partial t} dt + \nabla_X u*dX + \frac{1}{2}\sum_{i,j=1}^{n} \frac{\partial ^2  u}{\partial X_i \partial X_j}   dX^{i}dX^{j}   \\ 
          &= \frac{\partial u}{\partial t} dt + \sum_{i=1}^{n} \frac{\partial u}{\partial X^{i} } F^{i} dt + \sum_{i=1}^{n} \frac{\partial u }{\partial X_i}      \sum_{i=1}^{m} G^{ik} d W_k   \\
          &+  \frac{1}{2} \sum_{i,j=1}^{n}  \frac{\partial ^2  u}{\partial X_i \partial X_j} \left(F^{i} dt + \sum_{k=1}^{m} G^{ik} dW_k   \right)\left( F^{j} dt + \sum_{l=1}^{m} G^{i;} dW_l    \right)   \\
          &= (\frac{\partial u}{\partial t} + F*\nabla u + \frac{1}{2} H*D^2 u) dt + \sum_{i=1}^{n} \frac{\partial u}{\partial X_i} \sum_{k=1}^{m} G^{ik} dW_{k}
.\end{align*}
Where 
\begin{align*}
  dX^{i} &= F^{i} dt + \sum_{k=1}^{m}  G^{ik} dW_k   \\
  H_{ij} &= \sum_{k=1}^{m} G^{ik}G^{jk}  \ , \ A *B = \sum_{i,j=1}^{m} A_{ij} B_{ij} 
.\end{align*}
\end{remark}
\begin{example}
 A typical example for $G$ is  
  \begin{align*}
      G^{T}G = \sigma  I_{n \times  n} 
  .\end{align*}
\end{example}
\begin{remark} 
 If $F$ and $G$ are deterministic 
 \begin{align*}
   dX =  F(t) dt + GdW_t
 .\end{align*}
 Then for arbitrary test function $u \in  \mathcal{C}_0^{\infty}(\mathbb{R}^{n} ) $ we have  by It\^o's formula 
 \begin{align*}
   u(x(t)) - u(x(0)) &= \int_0^{t} \nabla u (x(s)) * F(s) ds + \int_0^{t}  \frac{1}{2}(G^{T}G ) : D^2u(x(s)) ds \\
                     &+ \int_0^{t} \nabla u(x(s)) * G(s) dW_{s} 
 .\end{align*}
 Let $\mu(s,*)$  be the law of $X(s)$ then by taking the expectation of the above integral 
 \begin{align*}
   \int_{\mathbb{R}^{n} } u(x) d\mu(s,x) - \int_{\mathbb{R}^{n} } u(x) d\mu_0(x) &=  \int_{0}^{t} \int_{\mathbb{R}^{n} }  \nabla u(x) * F(s) d\mu(s,x)\\
                                                                                 &+ \int_0^{t} \int_{\mathbb{R}^{n} }  \frac{1}{2}(G^{T}(s)G(s)) : D^2 u(x) * d\mu (s,x) \\
                                                                                 &+ 0
 .\end{align*}
\end{remark}
\newpage
\begin{definition}[Parabolic Operator]
 \begin{align*}
   \partial_t u  - \frac{1}{2} \sum_{i,j=1}^{n}  D_{ij} (\sum_{k=1}^{m}  G^{ik}G^{kj}  )\mu  + \nabla * (F \mu )  = 0 
 .\end{align*} 
\end{definition}
\begin{example}
  If $F=0$  $m=n$ and $G=\sqrt{2}I_{n \times  n} $ then 
  \begin{align*}
    dX = \sqrt{2} dW_t 
  .\end{align*}
  And the law $\mu $ of $X$ fulfills the heat equation i.e
  \begin{align*}
    \dot{\mu} t- \Delta \mu  = 0
  .\end{align*}
\end{example}
\section{Relation To The Mean Field Limit}
To find out how all this translates to our Mean field Limit we consider the particle system given by 
\begin{align*}
  \begin{cases}  
  d X_i &=  \frac{1}{N} \sum K(x_i,x_j) dt + \sqrt{2} dW_t^1  \qquad 1\le i\le N \ N\to \infty\\
  X_i(0)    &= x_{0,i} \\
  \mu_N(t) &= \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i(t)} 
  \end{cases}
.\end{align*}
And denote 
\begin{align*}
  \mathbb{X}_N = F(\mathbb{X}_N) dt + \sqrt{2}dW_{t} 
.\end{align*}
At time $t = 0$ the $X_i$ are independent random variables, at any time $t>0$ they are dependent and the particles have joint law 
\begin{align*}
  (X_{1}(t),\ldots ,X_N(t)) \sim  u(X_{1},\ldots ,X_n)
.\end{align*}
Where $u \in  \mathcal{M}(\mathbb{R}^{dN})$, then by It\^o's formula we get for arbitrary test function $\forall  \phi  \in  \mathcal{C}_0^{\infty}(\mathbb{R}^{dN} ) $ 
\begin{align*}
  \phi(\mathbb{X}_N(t)) &=  \phi(\mathbb{X}_N(0)) + \int_0^{t} \nabla\phi  *\begin{pmatrix} \vdots \\ \frac{1}{n} \sum_{j=1}^{N} K(X_i,X_j) \\ \vdots  \end{pmatrix} \\
                        &+ \int_0^{t}  \Delta{\mathbb{X}_N} \phi  dt + \int_0^{t} \sqrt{2} \nabla \phi  dW_t^{i} 
.\end{align*}
Taking the expectation on both sides, then the last term disappears by definition of Ito processes 
\begin{align*}
  \partial_t - \sum_{i=1}^{N} \Delta_i u  + \sum_{i=1}^{N} \nabla_{X_i} \left( \frac{1}{N} \sum_{j=1}^{N} K(X_i,X_j) u \right)  = 0
.\end{align*}
Now consider the Mean-Field-Limit, if the joint particle law can be rewritten as the tensor product of a single $\overline{u}$ 
\begin{align*}
 u(X_{1},\ldots ,X_N)  = \overline{u}^{\otimes N}  
.\end{align*}
the equation simplifies
\begin{align*}
  \partial_t - \sum_{i=1}^{N} \Delta_i u  + \sum_{i=1}^{N} \nabla_{X_i} \left( \overline{u}^{\otimes N}k \star \overline{u}(X_i)   \right)  = 0
.\end{align*}
\section{Solving Stochastic Differential Equations}
The setup of the following section will be the following 
\begin{definition}[Basic Setup]
 We consider the probability space $(\Omega ,\mathcal{F},\mathbb{P})$, With a $m$-$D$ dimensional Brownian motion $W(*)$.
 Let $X_0$ be an $n$-$D$ dimensional random variable independent of $W(0)$, then our Filtration is given by
 \begin{align*}
  \mathcal{F}_t = \sigma(X_{0}) \cup \sigma(W(s) , 0\le s\le t)
 .\end{align*}
\end{definition}
\begin{definition}[SDE]\label{sde}
 Given the above basic setup we are trying to solve equations of the type 
 \begin{align*}
  \begin{cases}
    d\underbrace{X_t}_{n \times 1} &= \underbrace{b}_{n \times 1}(X_t,t) dt + \underbrace{B}_{n \times m}(X_t,t) d\underbrace{W_t}_{m \times 1} \quad 0\le t\le T \\
    X_{t}\rvert_{t=0} &= X_{0} \quad X \ : \ (t,\omega ) \to  \mathbb{R}^{n} 
  \end{cases}
 .\end{align*}
 Where 
 \begin{align*}
   b &: \mathbb{R}^{n} \times [0,T] \to \mathbb{R}^{n}   \\
   B &: \mathbb{R}^{n} \times [0,T] \to  M^{n\times m}  
 .\end{align*}
\end{definition}
\begin{remark}
 The differential equation should always be understood as the Integral equation 
 \begin{align*}
  X_t - X_{0} = \int_0^{t}  b(X_s,s) ds + \int_0^{t} B(X_s,s) dW_s 
 .\end{align*}
\end{remark}
\begin{definition}[Solution]
 We say an $\mathbb{R}^{n}$-valued stochastic process $X(*)$ is a solution of the SDE if 
 \begin{enumerate}
  \item $X_t$ is progressively measurable w.r.t $\mathcal{F}_t$
  \item (drift) $F\coloneqq b(X_t,t) \in  L_{n}^{1}([0,T]) \ \Leftrightarrow \  \int_0^{t} \E[F_s] ds < \infty $ 
  \item (diffusion) $G\coloneqq B(X_t,t) \in  L_{n \times m}^{2}([0,T]) \Leftrightarrow \  \int_0^{t} \E[\abs{G_s}^2] ds < \infty $ 
 \end{enumerate}
\end{definition}
\begin{remark}
  (1) implies that for any given $t \in  [0,T]$ $X_t$ is random variable measurable with respect to $\mathcal{F}_t$.
\end{remark}
\hspace{0mm}\\
The goal from now on is to prove the existence and uniqueness of such solutions, for that we first define what it means 
for a solution to be unique
\begin{definition}
 For two solution $X,\tilde{X} $ we say they are unique if
 \begin{align*}
   \P(X(t) = \tilde{X}(t), \ \forall  t \in  [0,T] ) = 1 \Leftrightarrow \max_{0\le t \le T} \abs{x(t)-\tilde{x}(t) }  = 0 \text{ a.s.}
 .\end{align*}
 i.e they are indistinguishable.
\end{definition}
\begin{assumption}\label{assumption_sde_sol}\hspace{0mm}\\
  Let $b : \mathbb{R}^{n} \times  [0,T] \to  \mathbb{R}^{n}  $ and 
  $B : \mathbb{R}^{n} \times  [0,T] \to  M^{n \times m}  $,
  be continuous (in $(t,x)$) and Lipschitz continuous with respect to $x$ for some $L > 0$.
  Furthermore assume they fulfill the linear growth condition
  \begin{align*}
    \abs{b(x,t)} + \abs{B(x,t)} \le  L(1+\abs{x}) 
  .\end{align*}
\end{assumption}
\begin{remark}
  Note the Lipschitz continuity from \autoref{assumption_sde_sol} implies that there $\exists L >0$ such that
  \begin{align*}
    \abs{b(x,t) - b(\tilde{x},t )} +  \abs{B(x ,t) - B(\tilde{x},t )} \le  L \abs{x - \tilde{x} }
  .\end{align*}
\end{remark}
\begin{theorem}[Existence and Uniqueness of Solution]
  Let \autoref{assumption_sde_sol} hold for an \nameref{(SDE)} and assume the initial data $X_0$ is  
  square integrable and independent of $W^{t}(0)$.
  Then there exists a unique solution $X \in  \mathbb{L}^2_n([0,T])$ of the \nameref{sde}.
\end{theorem}
\begin{proof}
  We begin with the uniqueness prove.\\[1ex] 
  Suppose we have two solutions $X$ and $\tilde{X} $ of the SDE then the goal is to show that they are indistinguishable,
  then by using the definition of a solution 
  \begin{align*}
    X_t - \tilde{X}_t = \int_0^{t} (b(X_s,s) - b(\tilde{X}_s,s )) ds + \int_0^{t} B(X_s,s) - B(\tilde{X}(s),s )  dW_s
  .\end{align*}
  If the diffusion term were to be 0 we could use a Grönwall type inequality and get the uniqueness. 
  To work with the diffusion term we consider the square of the above and apply Itos isometry. Note that
  generally $\abs{a+b}^2 \nleqslant  (a^2+b^2)$  which is why we need the extra 2.
  \begin{align*}
    \abs{X_t - \tilde{X}_t}^2 \le  2\abs{\int_0^{t} (b(X_s,s) - b(\tilde{X}_s,s )) ds}^2 + \abs{\int_0^{t} B(X_s,s) - B(\tilde{X}(s),s )  dW_s}^2
  .\end{align*}
  Now consider the following 
  \begin{align*}
    \E[\abs{X_t-\tilde{X}_t}^2] &\le 2\E[\abs{\int_0^{t} \abs{b(X_s,s) - b(\tilde{X}_s,x )} ds}^2 ]  \\
                                & \qquad + 2 \E[\abs{\int_0^{t} B(X_s,s) - B(\tilde{X}_s,s ) dW_s}^2]\\
                                &\myS{Hold.}{\le } 2t \E[\int_0^{t} \abs{b(X_s,s) - b(\tilde{X})s,s )}^2 ds ] + 2\E[\int_0^{t} \abs{B(X_s,s)-B(\tilde{X}_s,s )}^2 ds ] \\
                                &\myS{Lip.}{\le } 2(t+1)L^2 E[\int_0^{t} \abs{X_s-\tilde{X}_s }^2 ds ]\\
                                &= 2(t+1)L^2 \int_0^{t} E[\abs{X_s-\tilde{X}_s }^2  ]ds\\
  .\end{align*}
 Where the following Hoelders inequality was used 
 \begin{align*}
   \left( \int_0^{t} 1 \abs{f} ds  \right)^2 &\le  \left( \int_0^{t} 1^2 ds  \right)^{\frac{1}{2}*2}*\left( \int_0^{t} \abs{f}^2 ds  \right)^{\frac{1}{2}*2}  \\
                                             &\le t \int_0^{t} \abs{f}^2 ds 
 .\end{align*}
 Now by Gronwalls inequality we have 
 \begin{align*}
   \E[\abs{X_t-\tilde{X}_t }^2] = 0
 .\end{align*}
 i.e $X_t$ and $\tilde{X}_t $ are modifications of each other and it remains to show that they are actually
 indistinguishable.\\[1ex]
 Define 
 \begin{align*}
  A_t = \{ \omega  \in  \Omega  \ | \ \abs{X_t - \tilde{X}_t  } > 0\}   \qquad \P(A_t) = 0
 .\end{align*}
 \begin{align*}
   \P(\max_{t \in  \mathbb{Q} \cap [0,T]} \abs{X_t-\tilde{X}_t } > 0 ) = \P(\bigcup_{k=1}^{\infty} A_{t_k} ) = 0
 .\end{align*}
 Now since $X_t(\omega )$ is continuous in $t$ we can extend the maximum over the entire interval $[0,T]$ 
 \begin{align*}
   \max_{t \in  \mathbb{Q} \cap [0,T]} \abs{X_t - \tilde{X}_t} = \max_{t \in  [0,T]} \abs{X_t - \tilde{X}_t}
 .\end{align*}
 Then the probability over the entire interval must also be 0 
 \begin{align*}
   \P(\max_{t \in  [0,T]} \abs{X_t - \tilde{X}_t} >0)  = 0 \quad \text{ i.e. } X_t = \tilde{X}_t \ \forall  t \text{ a.s.} 
 .\end{align*}
 This concludes the uniqueness proof, for existence similar to the deterministic case we use Picard iteration.\\[1ex]
 First define the Picard iteration by  
 \begin{align*}
   X_t^{0} &= X_0  \\
           &\vdots\\
   X_t^{n+1} &= X_0 + \int_0^{t} b(X_s^{n},s ) ds + \int_0^{t} B(X_s^{n},s ) dW_s   
 .\end{align*}
 Let $d(t)^{n} = \E[\abs{X_t^{n+1}-X_t^{n}}^2] $, then we claim by induction that $d^{n}(t) \le  \frac{(Mt)^{n+1} }{(n+1)!} $ for some $M>0$.\\
  \textbf{IA:} For $n=0$ we have
  \begin{align*}
    d(t)^{0} = \E[\abs{X_t^1-X_t^0}^2] &\le  \E[2 (\int_0^{t} b(X_0,s) ds )^2 + 2 (\int_0^{t} B(X_0,s )dW_s )^2]  \\
                                       &\le  2t \E[\int_0^{t} L^2(1+X_{0}^2) ds ] + 2\E[\int_0^{t} L^2(1+X_{0}) ds ] \\
                                       &\le  tM \qquad \text{ where } M\ge 2L^2(1+\E[X_{0}^2]) +2L^2(1+T)
  .\end{align*}
  \textbf{IV:} suppose the assumption holds for $n-1 \in  \mathbb{N}$\\
  \textbf{IS:} Take $n-1 \to n$ then 
  \begin{align*}
    d^{n}(t) &= \E[\abs{X_t^{n+1} - X^{n}_t }^2] \le  2 L^2 T \E[\int_0^{t} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ]  + 2L^2\E[\int_0^{t} \abs{X_s^{n} - X_s^{n-1}  }^2  ds] \\
             &\myS{IV}{\le } 2L^2(1+T) \int_0^{t} \frac{(Ms)^n}{n!} ds \\
             &= 2L^2(1+t) \frac{M^n}{(n+1)!} t^{n+1} \le \frac{M^{n+1}t^{n+1}}{(n+1)!} 
  .\end{align*}
  Because of $\Omega $ we cannot use completeness to argue the convergence and instead are forced to use a similar argument as in the uniqueness proof. 
  \begin{align*}
    &\E[\max_{0\le t \le T} \abs{X^{n+1}_t - X^{n}_t  }^2] \\
    &\le \E[\max_{0\le t\le T} 2\abs*{\int_0^{t} b(X_s^{n},s )-b(X_s^{n-1},s ) ds}^2 + 2\abs*{\int_0^{t}B(X_s^{n},s )-B(X_s^{n-1},s ) dW_s}^2] \\
    &\le 2TL^2 \E[\int_0^{T} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ] + 2\E[\max_{0\le t\le T} \abs*{\int_0^{t} B(X_s^{n},s )- B(X_s^{n-1},s ) ds W_s}]\\
    &\le  2TL^2 \E[\int_0^{T} \abs{X_s^{n} - X_s^{n-1}  }^2 ds ] + 8\E[\int_0^{T} \abs{B(X_s^{n},s  )-B(X_s^{n-1},s )}^2 ds ]\\
    &\le C*\E[\int_0^{T} \abs{X_s^{n}-X_s^{n-1}  }^2 ds ]
  .\end{align*}
  Where we used the following Doobs martingales $L^{p}$ inequality 
  \begin{align*}
    \E[\max_{0\le s\le t} \abs{X(s)}^{p} ] \le  (\frac{p}{p-1})^{p} \E[\abs{X(t)}^{p}  ] 
  .\end{align*}
  By Picard iteration we know the distance $d^{n}(t) = \E[\abs{X_s^{n}-X_s^{n-1}  }^2] $ is bounded by 
  \begin{align*}
    C*\E[\int_0^{T} \abs{X_s^{n}-X_s^{n-1}  }^2 ds ] &= C* \int_0^{T} \E[\abs{X_s^{n} - X_s^{n-1}  }^2] ds \\
                                                     &\le \int_0^{T} \frac{(Mt)^{n} }{(n)!} \\
                                                     &= C \frac{M^{n} T^{n+1} }{(n+1)!}
  .\end{align*}
  Further more we get with a Markovs inequality
  \begin{align*}
    \P(\underbrace{\max_{0\le t\le T} \abs{X_t^{n+1}-X_t^n}^2 > \frac{1}{2^{n} }}_{A_n}) &\le 2^{2n} \E[\max_{0\le t\le T} \abs{X_t^{n+1}-X_t^n}^2]\\
                                                                      &\le 2^{2n} \frac{CM^{n}T^{n+1}  }{(n+1)!} 
  .\end{align*}
  Then by Borel-Cantelli
  \begin{align*}
    \sum_{n=0}^{\infty} \P(A_n) \le  C \sum_{n=0}^{\infty}2^{2n} \frac{(MT)^{n} }{(n+1)!}    <\infty \implies \P(\bigcap_{n=0}^{\infty} \bigcup_{m=n}^{\infty} A_m ) = 0
   .\end{align*}
   i.e $\exists  B \subset  \Omega  $ with $\P(B) = 1$ s.t $\forall \ \omega  \in  B$ , $\exists \ N(\omega ) > 0 $ s.t
   \begin{align*}
     \max_{0\le t\le T} \abs{X_t^{n+1}(\omega ) - X_t^{n}(\omega )} \le  2^{-n} 
   .\end{align*}
   In fact we can give $B$ directly by 
   \begin{align*}
     \left( \bigcap_{n=0}^{\infty} \bigcup_{m=n}^{\infty} A_m   \right)^{C} = \bigcup_{n=0}^{\infty} \bigcap_{m=n}^{\infty} A_m^{C} = B     
   .\end{align*}
   then for each $\omega  \in  B$  we can make a Cauchy sequence argument by
   \begin{align*}
     \max_{0\le t\le T} \abs{X_t^{n+k} - X^{n}_t } &\le  \sum_{j=1}^{k} \max \abs{X_t^{n+j} - X^{n+(j-1)}_t } \\
                                                   &\le \sum_{j=1}^{k} \frac{1}{2^{n+j-1} }\\
                                                   &< \frac{1}{2^{n-1} } 
   .\end{align*}
   By the above we get 
   \begin{align*}
     X_t^{n}(\omega ) \to X_t(\omega ) \qquad \text{ uniform in } t\in [0,T]
   .\end{align*}
   Therefore for a.s. $\omega $ , take the limit in the iteration  and obtain 
   \begin{align*}
    X_t = X_{0} + \int_0^{t} b(X_s,s) ds + \int_0^{t} B(X_s,s) dW_s  
   .\end{align*}
   It remains to show that $X_t \in  \mathbb{L}^2([0,T])$ note that $X_{0} \in  \mathbb{L}^2([0,T])$ already and 
   \begin{align*}
     \E[\abs{X_t^{n+1}}^2] &\le  C(1+\E[\abs{X_0}^2]) + C \int_0^{t} \E[\abs{X_s^{n} }^2] ds \\
                           &\le  C \sum_{j=0}^{n} C^{j+1} \frac{t^{j+1}}{(j+1)!} (1+\E[\abs{X_0}^2]) \\
                           &\le  C* e^{Ct} 
   .\end{align*}
   Where we used $\E[X_{0}] = 0$ ,the  linear growth condition for the first integral and It\^o isometry for the second and then again the linear growth condition\\[1ex]
    Using the above we conclude
    \begin{align*}
      \E[\abs{X_t}^2] = \E[\abs{X_t + X_t^n -X_t^n}^2] \le  2\E[\abs{X_t - X_t^n}^2] + 2\E[\abs{X_t^n}^2] < \infty
    .\end{align*}
\end{proof}
\begin{remark}
  One should remember that if the diffusion term $B(X_t,t)$ is 0 then we get a unique solution iff $b(X_t,t)$ is Lipschitz
\end{remark}
\begin{theorem}[Higher Moments Estimate]
  Assumptions for $b$ , $B$ and $X_{0}$ are the same as before, if in addition 
  \begin{align*}
    \E[\abs{X_0}^{2p} ]< \infty
  .\end{align*}
  for some $p \ge 1$ then $\forall  t \in  [0,T]$ 
  \begin{align*}
    \E[\abs{X_t}^{2p}]\le C(1+\E[\abs{X}_0^{2p} ])e^{Ct} 
  .\end{align*}
  and $\E[\abs{X_t - X_0}^{2p} ] \le  C(1+\E[\abs{X_0}^{2p} ])e^{Ct}t^p $
\end{theorem}
\begin{proof}
 Left as an exercise \\
\end{proof}
\newpage
\section{General Convergence Results}
\begin{definition}[Weak convergence of measures]
 The following statements are equivalent   
 \begin{enumerate}
   \item $\mu_n \rightharpoonup \mu $
   \item For $\forall  f \in  \mathcal{C}_b(\mathbb{R}^{d} )$ it holds 
 \begin{align*}
  \int f d\mu_n \to \int  f d\mu 
 .\end{align*}
\item For $\forall  B \in \mathcal{B} $  
  \begin{align*}
    \mu_n(B) \to  \mu(B)
  .\end{align*}
\item For $\forall f \in  \mathcal{C}_b(\mathbb{R}^{d} )$ uniform continuous it holds 
  \begin{align*}
    \int  f d\mu_n \to  \int f d\mu 
  .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Weak convergence of Random variable]
 The following statements are equivalent  
 \begin{enumerate}
   \item $X_n$ converges weakly in Law to $X$  
     \begin{align*}
      X_n \rightharpoonup X
     .\end{align*}
   \item For $\forall f \in  \mathcal{C}_b(\mathbb{R}^{d} )$ it holds 
     \begin{align*}
       \E[f(X_n)] \to \E[f(x)]
     .\end{align*}
 \end{enumerate}
 \begin{enumerate}
   \item $X_n$ converges to $X$ in probability
   \item For $\forall \epsilon  >0$ 
     \begin{align*}
       \P(\abs{X_n - X} >\epsilon ) \xrightarrow{n\to \infty} 0
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{exercise}
 Prove that 
 \begin{align*}
   X_n \to  X \ \text{a.s.} \implies \P(\abs{X_n - X} > \epsilon ) \xrightarrow{n\to \infty} 0 \implies X_n \xRightarrow{(D)} X
 .\end{align*}
\end{exercise}
\begin{definition}[Tightness]
 A set of probability measures $S \subset  \mathcal{P}(\mathbb{R}^{d} )$  is called tight, if 
 for $\forall \ \epsilon  > 0$ there exists $\exists \  K \subset  \mathbb{R}^{d} $ compact such that 
 \begin{align*}
   \sup_{\mu  \in  S} \mu(K^{c} )  \le  \epsilon 
 .\end{align*}
\end{definition}
\begin{theorem}[Prokhorov's theorem]
  A sequence of measures $(\mu_n)_{n \in  \mathbb{N}}$  is tight in $\mathcal{P}(\mathbb{R}^{d} )$ iff 
  any subsequence has a weakly convergences subsequence.
\end{theorem}
\begin{proof}
 Refer to literature 
\end{proof}
We can now define the Stochastic Empirical measure  
\begin{definition}[Empirical Measure (Stochastic version)]
  For a set of  random variables $(X_i)_{i\le N}$  we define the (random) empirical measure 
  \begin{align*}
    \mu_n = \frac{1}{N}\sum_{i=1}^{N} \delta_{X_i} 
  .\end{align*}
\end{definition}
Point wise this definition coincides with the deterministic case.
\begin{corollary}
  If  $X_i$ are i.i.d random variables with law $\mu_{X}$ then  $\forall  \ f \in  \mathcal{C}_b(\mathbb{R}^{d} ) $ it holds that
  \begin{align*}
    \P(\lim_{N \to \infty} \int  f d \mu_N = \int f d\mu ) = 1
  .\end{align*}
\end{corollary}
Actually one can prove that the choice of $f \in  \mathcal{C}_b$ does not matter for the convergence 
We get the stronger corollary
\begin{corollary}
  If  $X_i$ are i.i.d random variables with law $\mu_{X}$ then  it holds that
\begin{align*}
  \P(\mu_N \rightharpoonup \mu ) = 1
.\end{align*}
i.e 
\begin{align*}
 \P( \forall  f \in  \mathcal{C}_b(\mathbb{R}^{d} ) \ : \int f_n d\mu_n = \int  f d\mu )  = 1
.\end{align*}
\end{corollary}
\begin{proof}
 The proof relies mainly on proving that $\mathcal{C}_b(\mathbb{R}^{d} )$  is separable for compact support 
 we can use the density of the polynomials. Then we can go from arbitrary $f$ to the union over a countable sequence of $f$ 
 and then argue through separability that this is equal to the entire space.
\end{proof}
Remembering the definition of the Wasserstein distance 
\begin{definition}[Wasserstein Distance]
  For all $\mu , \nu  \in  \mathcal{P}_p(\mathbb{R}^{d} )$  , $(p\ge 1) $ the Wasserstein Distance of $\mu $ and $\nu $ is given by 
  \begin{align*}
    W^{p}(\mu ,\nu ) = \dist_{MK,p}(\mu ,\nu ) = \inf_{\pi \in  \Pi(\mu ,\nu )} \left( \int \int_{\mathbb{R}^{2d} } \abs{x-y}^{p} \pi(dxdy) \right)^{\frac{1}{p}}  
  .\end{align*}
  Where  
  \begin{align*}
    \Pi(\mu ,\nu ) = \{\pi \in \mathcal{P}(\mathbb{R}^{d} \times  \mathbb{R}^{d}  ) : &\int_{\mathbb{R}^{d}\times E } \pi(dx,dy) = \nu(E) \\
                                                                                      &\int_{E \times  \mathbb{R}^{d} } \pi(dx,dy) = \mu(E)\}  
  .\end{align*}
  then the following convergences are equivalent
  \begin{enumerate}
   \item $W_p(\mu_n,\mu ) \to 0$
    \item  For $\forall f \in \mathcal{C}(\mathbb{R}^{d} )$ such that $\abs{f(x)} \le  C(1+\abs{x}^{p} )$
     \begin{align*}
      \int  f d\mu_n \to \int  f d\mu 
     .\end{align*} 
    \item $\mu_n \rightharpoonup \mu $
  \end{enumerate}
\end{definition}
\begin{corollary}
 If $X_i$  are i.i.d random variables with law $\mu_X$ and $\int \abs{x}^{p} \mu  < \infty $ then
 \begin{align*} 
 W_p(\mu_N,\mu ) \to  0 \qquad \text{ a.s.}
 .\end{align*}
 and 
 \begin{align*}
   \E[W_p^{p}(\mu_N,\mu ) ] \to 0
 .\end{align*}
 Where 
 \begin{align*}
   \mu_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i} 
 .\end{align*}
\end{corollary}
