\chapter{MEAN FIELD LIMIT FOR SDE SYSTEM}
\input{lectures_new/stochastic_primer.tex}
\input{lectures_new/ito_integral.tex}
\input{lectures_new/solving_sdes.tex}
\section{Stochastic Mean Field Limit}
First recall the metric we use to talk about distance between two measures i.e the Wasserstein Distance
\begin{definition}[Wasserstein Distance]
  For all $\mu , \nu  \in  \mathcal{P}_p(\mathbb{R}^{d} )$  , $(p\ge 1) $ the Wasserstein Distance of $\mu $ and $\nu $ is given by 
  \begin{align*}
    W^{p}(\mu ,\nu ) = \dist_{MK,p}(\mu ,\nu ) = \inf_{\pi \in  \Pi(\mu ,\nu )} \left( \int \int_{\mathbb{R}^{2d} } \abs{x-y}^{p} \pi(dxdy) \right)^{\frac{1}{p}}  
  .\end{align*}
  Where  
  \begin{align*}
    \Pi(\mu ,\nu ) = \Bigg\{\pi \in \mathcal{P}(\mathbb{R}^{d} \times  \mathbb{R}^{d}  ) : &\int_{\mathbb{R}^{d}\times E } \pi(dx,dy) = \nu(E) \\
                                                                                      &\int_{E \times  \mathbb{R}^{d} } \pi(dx,dy) = \mu(E)\Bigg\}  
  .\end{align*}
\end{definition}
\begin{remark}
 Note that 
 \begin{align*}
   W_1(\mu ,\tilde{\mu } ) \le  W_{2}(\mu ,\tilde{\mu } )
 .\end{align*}
 follows naturally by HÃ¶lders inequality, in fact this holds for all $p>q$
  \begin{align*}
   W_q(\mu ,\tilde{\mu } ) \le  W_{p}(\mu ,\tilde{\mu } )
 .\end{align*}
\end{remark}
\begin{remark}
  Let $(\mu_n)_{n \in  \mathbb{N}} \subset  \mathcal{P}_p(\mathbb{R}^{d} )$ be a sequence of measures,then following are equivalent
  \begin{enumerate}
   \item $W_p(\mu_n,\mu ) \to 0$
    \item  For $\forall f \in \mathcal{C}(\mathbb{R}^{d} )$ such that $\abs{f(x)} \le  C(1+\abs{x}^{p} )$
     \begin{align*}
      \int  f d\mu_n \to \int  f d\mu 
     .\end{align*} 
    \item $\mu_n \rightharpoonup \mu $
  \end{enumerate} 
\end{remark}
\newpage
\subsection{Stochastic Particle System}
Let us begin by shortly defining the stochastic particle systems we study. 
\begin{definition}[Empirical Measure (Stochastic version)]\label{empirical_stochastic}
  For random variables $(X_i)_{i\le N}$  we define the (stochastic) empirical measure by
  \begin{align*}
    \mu_N(\omega ) = \frac{1}{N}\sum_{i=1}^{N} \delta_{X_i(\omega)} 
  .\end{align*}
\end{definition}
Then our stochastic particle system is given by,
\begin{definition}[Stochastic Particle System ]\label{sden}
  For $N$ interacting particles $(X^{1} ,\ldots ,X^{N} )$ with i.i.d initial data $(X_i^{N}(0))_{i \in  \{1,\ldots ,N\}  } \subset  L^2(\Omega) $ and law $\mu_0$
\begin{align*}
  \text{(SDEN)}\begin{cases}
    d X_i^{N}(t) &=   b(X_i^{N}(t) , \mu_N(t) )dt + \sigma(X_i(t)^{N},\mu_N(t))dW^{i}_t \\
    X_i^{N}(0) &= X_{i,0}^{N}   
  \end{cases}
.\end{align*}
Where $\mu_N$ is the stochastic empirical measure and note $\mathcal{L}(X_{0}) = \mu_0$
\end{definition}
\begin{remark}
  The dimensions for our Stochastic-Particle-System are the same as in \autoref{sde}
\end{remark}
\begin{remark}
  For our initial measure we already have
\begin{align*}
  \E[W_2^{2}(\mu_N(0),\mu_0) ] \to  0
.\end{align*}
\end{remark}
\subsection{I.I.D Case}
Let us shortly consider the convergence of the empirical measure in the case where our random variables are i.i.d, note in the mean field limit 
this is only the case for our initial data all particles at $t>0$ are not i.i.d.
\begin{corollary}
  If  $(X_i)_{i \in  \{1,\ldots ,N\}  }$ are i.i.d random variables with law $\mu_{X}$ then  $\forall  \ f \in  \mathcal{C}_b(\mathbb{R}^{d} ) $ it holds that
  \begin{align*}
    \P(\lim_{N \to \infty} \int  f d \mu_N = \int f d\mu ) = 1
  .\end{align*}
\end{corollary}
We can actually prove the stronger statement that the choice of $f \in  \mathcal{C}_b$ does not matter for the convergence i.e.
we can pull the function selection into the probability similarly to the difference between modification and indistinguishable. 
\begin{corollary}
  If  $(X_i)_{i \in  \{1,\ldots ,N\}  }$ are i.i.d random variables with law $\mu_{X}$ then  it holds that
\begin{align*}
  \P(\mu_N \rightharpoonup \mu ) = 1
.\end{align*}
i.e 
\begin{align*}
 \P( \forall  f \in  \mathcal{C}_b(\mathbb{R}^{d} ) \ : \int f d\mu_N \to  \int  f d\mu )  = 1
.\end{align*}
\end{corollary}
\begin{proof}
  \textcolor{Red}{Needs revision, this should only work for $\mathcal{C}_b(K)$ $K$ compact in my opinion}\\
 The proof relies mainly on showing that $\mathcal{C}_b(\mathbb{R}^{d} )$  is separable for compact support 
 we can use the density of the polynomials. Then we can go from arbitrary $f$ to the union over a countable sequence of $f$ 
 and then argue through separability that this is equal to the entire space.
\end{proof}
\newpage
\begin{lemma}[General Dominated Convergence]\label{general_dct}
  Let $(X_n)_{n \in  \mathbb{N}} \subset   L^{p} $ be a sequence of random variables then the following are equivalent 
  \begin{enumerate}
    \item $(X_n)_{n \in  \mathbb{N}}$ are uniformly integrable  and $X_n \to X$ $\P$-a.s.
    \item $\|X_n - X\| \to  0$ for some $X \in  L^p$
  \end{enumerate}
\end{lemma}
\begin{proof}
  First note that  (1) implies 
  \begin{align*}
    \|X_n\|_p \to  \|X\|_p
  .\end{align*}
  and $X_n \to  X$ $\P$-a.s. and $\|X_n\|_p \to \|X\|_p$ together imply
  \begin{align*}
    \|X_n -X\|_p \to 0
  .\end{align*}
\end{proof}
\begin{remark}
  In general a sequence $(X_i)_{i \in  \mathbb{N}}$ is called uniform integrability means that 
 \begin{align*}
   \lim_{r \to  \infty} \sup_{i \in  \mathbb{N}} \E[\abs{X_i}*\cha_{\abs{X_i}\ge r}] = 0
 .\end{align*}
\end{remark}
\begin{lemma}[De la Vall\`ee Poussin Criterion]\label{de_la_valle}
 A sequence of random variables $(X_i)$  is uniformly integrable iff there 
 $\exists \phi $ convex with
 \begin{align*}
   \lim_{x \to \infty} \frac{\phi(x)}{x} = \infty
 .\end{align*}
s.t.
\begin{align*}
 \sup_i \E[\phi(\abs{X_i})] < \infty
.\end{align*}
\end{lemma}
\begin{proof}
  As the construction of $\phi $ is heavily technical we refer to xyz
\end{proof}
\begin{corollary}\label{wasserstein_convergence_arb}
  If $(X_i)_{i \in \{1,\ldots ,N\}  }$  are i.i.d random variables with law $\mu_X$ and $\int \abs{x}^{p} \mu  < \infty $ i.e $\mu  \in  \mathcal{P}^p(\mathbb{R}^{d} ) $
 \begin{align*} 
 W_p(\mu_N,\mu ) \to  0 \qquad \text{ a.s.}
 .\end{align*}
 and 
 \begin{align*}
   \E[W_p^{p}(\mu_N,\mu ) ] \to 0
 .\end{align*}
 Where 
 \begin{align*}
   \mu_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i} 
 .\end{align*}
\end{corollary}
\begin{proof}
 Remember that the following convergences are equivalent 
 \begin{enumerate}
   \item $W_p(\mu_N,\mu ) \to 0$
   \item $\mu_N \rightharpoonup \mu $ and $\int \abs{x}^{p} d\mu_N \to \int \abs{x}^{p} d\mu   $
   \item $\mu_n \rightharpoonup \mu $ and $\lim_{n\to \infty} \sup_r \int_{\abs{x} \ge r} \abs{x}^p d\mu_N = 0$
 \end{enumerate}
 Note that if we fix a.s. $\omega $ then we can treat this as the deterministic case. \\[1ex]
 We already know that 
 \begin{align*}
  \mu_N \rightharpoonup \mu \text{ a.s.}
 .\end{align*}
since $(X_i)$ are i.i.d then $\abs{X_i}^{p} $ is also i.i.d  and we use the Law of large numbers
 \begin{align*}
   \int \abs{x}^{p} d\mu_N   &= \frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}  \xrightarrow{L.L.N.} \E[\abs{X_i}^{p} ] < \infty
 .\end{align*} 
 And we get a.s. that $W_p(\mu_N,\mu ) \to  0$\\[1ex]
 For the stronger statement 
 \begin{align*}
   \E[W^{p}(\mu_n,\mu ) ] \to  0
 .\end{align*}
  we first note that  
 \begin{align*}
   W_p^{p}(\mu_N,\mu )  &\le  2^{p-1} (W^{p}_p(\mu_N,\delta_0)  + W_p^{p}(\delta_0,\mu )) \\
                        &= 2^{p-1} (\frac{1}{N} \sum_{i=1}^{N}   \abs{X_i}^{p}  + W_p^{p}(\delta_0,\mu ))
 .\end{align*}
 then it is sufficient to show the uniform integrability of the first part 
 \begin{align*}
  \frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}  
 .\end{align*}
 Since $\abs{X_i}^{p} $ is integrable then there exists a convex function  $\phi $ with $\lim_{x \to \infty} \frac{\phi(x)}{x} = \infty$ and 
 \begin{align*}
   \E[\phi(\abs{X_i}^{p} )] < \infty
 .\end{align*}
 Since $\phi $ is convex we apply Jensen's inequality to get 
 \begin{align*}
   \sup_N \E[\phi \left(\frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}\right)  ]  \myS{Jen.}{\le } \sup_N \sum_{i=1}^{N}\E[\phi (\abs{X_i}^{p} )] =  \E[\phi(\abs{X_i}^{p} )] < \infty
 .\end{align*}
 Finally \autoref{de_la_valle}  implies the uniform integrability and we conclude by \autoref{general_dct}
 \begin{align*}
   \E[W_p^{p}(\mu_N,\mu ) ]\to 0
 .\end{align*}
\end{proof}
\hspace{0mm}\\[1ex]
All the above statement only apply to arbitrary i.i.d sequences of random variables, but in our Mean-Field-Limit we only get 
the i.i.d property at $t=0$ such that we seek to prove that even as $N \to \infty$ we nonetheless get a convergence.
\begin{remark}
 Formally our goal is to prove the convergence 
 \begin{align*}
   \E[\sup_{t} W_2^{2}(\mu_N(t),\mu(t))  ] \to 0
 .\end{align*}
\end{remark}
\newpage
\subsection{Toy Example}
Let us first consider a simple stochastic particle system given by 
\begin{assumption}\label{sde_solution_assumption_strong}
Assume drift  $b : \mathbb{R}^{d} \times  \mathcal{P}^2(\mathbb{R}^{d} ) \to  \mathbb{R}^{d} $ and diffusion $\sigma : \mathbb{R}^{d} \times  \mathcal{P}^2(\mathbb{R}^{d} ) \to \mathbb{R}^{d \times  m}  $   are Lipschitz continuous i.e. $\exists  L >0$ s.t.
 \begin{align*}
  \abs{b(X,\mu ) - b(\tilde{X},\tilde{\mu }  )} + \abs{\sigma(X,\mu ) - \sigma(\tilde{X},\tilde{\mu }  )} \le  L \left( \abs{X - \tilde{X} } + W_2(\mu ,\tilde{\mu } ) \right) 
 .\end{align*}
\end{assumption}
\begin{example}[Stochastic Toy Model]
  Let our particle system be given as in \autoref{sden} with drift and diffusion for $\nabla V \in  \text{Lip}$
 \begin{align*}
   b(X,\mu )&= \nabla V \star  \mu(X)\\
   \sigma(X,\mu ) &= \sigma_0 >0
 .\end{align*}
\end{example}
\begin{exercise}
 Think about what happens if the initial data is i.i.d but the diffusion term is 0, can you prove a convergence ?
\end{exercise}
\begin{theorem}[Convergence Of Toy Model For Fixed $N$]
  Let our \hyperref[sden]{(SDEN)} be given with drift and diffusion as above and assume they fulfill \autoref{sde_solution_assumption_strong}, then  
  for fixed $N$ we get a unique strong solution in $\mathbb{L}^{2}_{dN}([0,T]) $
\end{theorem}
\begin{proof} 
  First we note that by \autoref{sde_solution_assumption_strong} we get
  \begin{align*}
    \abs{b(X,\mu ) - b(\tilde{X},\tilde{\mu }  )} &= \abs*{\int \nabla V(X-y) d\mu(y) - \int  \nabla V(\tilde{X}-y ) d \tilde{\mu }(y) }\\
                                                  &\ge \int \abs{\nabla V(X-y) - \nabla V(\tilde{X}-y )}d\mu (y) + \abs*{\int \nabla V(\tilde{X - y}(d\mu(y) - d \tilde{\mu }(y) ) )}\\
                                                  &\myS{Lip.}{\le }L*\abs{X - \tilde{X} } + LW_1(\mu ,\tilde{\mu } ) \\
                                                  &\le L*(\abs{X-\tilde{X} } + W_2(\mu,\tilde{\mu } ))
  .\end{align*}
  Let use the notation $\mathbb{X} = (X_1^{N},\ldots ,X_N^{N}  ) \in  \mathbb{R}^{dn} $ and $\mathbb{W} = (W^{1},\ldots ,W^{N}  )$ then
  \begin{align*}
    &B(\mathbb{X}) = \begin{pmatrix} \vdots \\ b(X_i^{N},\frac{1}{N}\sum_{k=1}^{N}  \delta_{X_k} ) \end{pmatrix}_{dN} \\ 
    &\Sigma(\mathbb{X})_{dN \times mN} \ : \ \text{diag}(\Sigma(\mathbb{X})) = \begin{pmatrix} \delta(X_1,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k} ), \ldots \delta(X_N,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k} ) \end{pmatrix}
  .\end{align*}
  Then our SDE is given by 
  \begin{align*}
    d \mathbb{X}(t) = B(\mathbb{X}(t)) dt + \Sigma(\mathbb{X}(t)) d \mathbb{W}_t
  .\end{align*}
  Now if $B$ and $\Sigma $ satisfy \autoref{assumption_sde_sol} we get a solution by \autoref{sde_solution_theorem}
  \begin{align*}
    \abs{B(\mathbb{X})-B(\mathbb{Y})}_{\mathbb{R}^{dn} }^2  &= \sum_{j=1}^{N} \abs{X_j,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k}  - b(Y_j,\frac{1}{N}\sum_{k=1}^{N} \delta_{Y_k} )}  \\
                                                            &\le \sum_{j=1}^{N} 2L^2 \left( \abs{X_j-Y_j}^2  + W_2^2(\mu_N(X),\mu_N(Y))\right)   \\
                                                            &\le  4L^2 \|\mathbb{X} - \mathbb{Y}\|^2 
  .\end{align*}
  For $\Sigma $ the argument is analog where for the Wasserstein distance we used 
  Then by \autoref{sde_solution_theorem} we get a solution $X \in  L^2([0,T])$ for fixed $N$\\[1ex]
\end{proof}
\begin{remark}
 To get a bound on the Wasserstein Distance we used the following  
  \begin{align*}
    \pi  = \frac{1}{N} \sum_{k=1}^{N} \delta_{(X_k,Y_k)}  \in  \Pi
  .\end{align*}
  then  the Wasserstein distance is given by 
  \begin{align*}
   \frac{1}{N}\sum_{k=1}^{N} \abs{X_k-Y_k}^2  
  .\end{align*}
  and one can further simplify to get the bound used.
\end{remark}
\begin{remark}
  As $N \to \infty$ we expect to get the following 
  \begin{align*}
    \begin{cases}
      dY^{i}(t) &= b(Y^{i}(t) ,\mu(t) ) dt + \sigma(Y^{i}(t),\mu(t) )dW_t^i \\
      Y^{i}(0)  &=  X_{i,0}^{N} \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}
  In fact since the above system beyond the initial data is independent of $N$, we may consider the simplified equation
  \begin{align*}
    \begin{cases}
      dY(t) &= b(Y(t) ,\mu(t) ) dt + \sigma(Y(t),\mu(t) )dW_t^i \\
      Y(0)  &=  \xi \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}
this equation is called Makean-Vlasov equation which is a non-linear non-local SDE
\end{remark}
\newpage
\subsection{Makean-Vlasov}
\begin{definition}[Makean-Vlasov Equation]
  The following non-linear and non-local SDE is called Makean-Vlasov Equation 
\begin{align*}
    \text{(MVE)} \begin{cases}
      dY(t) &= b(Y(t) ,\mu(t) ) dt + \sigma(Y(t),\mu(t) )dW_t^i \\
      Y(0)  &=  \xi \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}  
\textcolor{Red}{Add Space of $Y$ and dimensions} 
\end{definition}

\begin{definition}[Space Of Continuous Sample Paths]
 The Space  $\mathcal{C}^{d} = \mathcal{C}([0,T];\mathbb{R}^{d} ) $ is called the continuous sample path space with norm 
 \begin{align*}
   \|X\|_t = \sup_{0\le t \le T} \abs{X(t)}
 .\end{align*}
 this norm $\|*\|_T$ induces a $\sigma$-algebra on $\mathcal{C}^{d} $ 
\end{definition}
\begin{definition}[Random Variable]
  A random Variable on $\mathcal{C}^{d} $ is a map 
  \begin{align*}
    X : \Omega_{\text{a.s.}} \to \mathcal{C}^{d} 
  .\end{align*}
\end{definition}
\begin{definition}[Measure]
 Since the norm $\|*\|_T$  induces a $\sigma$-algebra on $\mathcal{C}^{d} $ we can define measures $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ by 
 \begin{align*}
   \mu  \coloneqq  (\mu(t))_{t \in  [0,T]} \qquad \mu(t)
 .\end{align*}
 and by using the function 
 \begin{align*}
  l_t : \mathcal{C}^{d} \to  \mathbb{R}^{d} \ X \mapsto X(t)  
 .\end{align*}
 then we get a measure on $\mathbb{R}^{d} $ by using the pushforward
 \begin{align*}
  \mu_t \coloneqq \mathcal{B} \to \mathbb{R}^{d}   \ A \mapsto \mu(l^{-1}_t(A) )
 .\end{align*}
\end{definition}

\begin{definition}[Wasserstein Distance]\label{c_d_wasserstein}
And we can define for arbitrary measures $\mu ,\tilde{\mu } \in  \mathcal{P}^2(\mathcal{C}^{d} ) $ the Wasserstein distance by
\begin{align*}
  \sup_{t \in  [0,T]} W_{\mathbb{R}^{d},2 }(\mu(t),\tilde{\mu }(t) ) \le  W_{\mathcal{C}^{d},2 } (\mu ,\tilde{\mu } )
.\end{align*}
Where 
\begin{align*}
  W_{\mathcal{C}^{d},2 }(\mu ,\tilde{\mu } ) = \inf_{\pi  \in  \Pi(\mu ,\tilde{\mu } )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \|x-y \|^2 d\pi(x,y)
.\end{align*}
\end{definition}
\begin{remark}
 Note that
 \begin{align*}
   \int_{\mathcal{C}^{d} } f(x) d\mu(x) = \int_{\mathbb{R}^{d} }  f(x(t)) d\mu_t
 .\end{align*}
\end{remark}
\begin{theorem}[Unique and Existence of Solution for Makean-Vlasov]\label{solution_vlasov}
  If $b$ and $\sigma $ satisfy \autoref{sde_solution_assumption_strong} then MVE has a unique and strong solution
  $Y \in  \mathbb{L}^2([0,T])$ and $\mu  \in  \mathcal{L}(Y)$
\end{theorem}
\begin{proof}
 We use the notation 
 \begin{align*}
   d_t^2 =  \inf_{\pi  \in  \Pi(\mu,\tilde{\mu } )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  }\|x-y\|^2_t d\pi(x,y)
 .\end{align*}
 For any given $\mu  \in \mathcal{P}^2(\mathcal{C}^{d} )$ we consider the following SDE 
 \begin{align*}
   \begin{cases} 
   dY^{\mu } (t) &= b(Y^{\mu} (t),\mu(t))dt + \sigma(Y^{\mu } (t),\mu(t))dW_t\\
    Y(0)   & \xi \in  L^2(\Omega)
   \end{cases}
 .\end{align*}
 Let $\phi(\mu ) = \mathcal{L}(Y^{\mu } ) $ be the law of $Y^{\mu} $. \\[1ex]
 For the existence  and the uniqueness of $Y^{\mu } $ we need to check 
 \begin{align*}
   \abs{b(x,\mu(t)) - b(\tilde{x},\mu(t) )} + \abs{\sigma(x,\mu(t)) - \sigma(\tilde{x},\mu(t))} \le  L \abs{x-\tilde{x} }
 .\end{align*}
 Since it is the same measure the Wasserstein distance is 0 and the above is true by \autoref{sde_solution_assumption_strong}.\\
 If $\phi $ has a fixpoint $\overline{\mu } $, then $\overline{\mu } $ is the solution of MVE.
 We prove this by first bounding the difference between two measures, let $\mu ,\tilde{\mu }$ be arbitrary given measure i n
 $\mathcal{P}^2(\mathcal{C}^{d} )$, first note
 \begin{align*}
  Y^{\mu }(t) - \xi = \int_0^{t} b(Y^{\mu }(s),\mu(s) ) ds + \int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) dW_s \qquad \mu= \mu,\tilde{\mu} 
 .\end{align*}
 then by taking the difference 
 \begin{align*}
   &\sup_{0\le t \le \tau }\abs{Y^{\mu }(t) - Y^{\tilde{\mu } }(t)}^2 \\
   &= \sup_{0\le t \le s}\abs*{\int_0^{t} b(Y^{\mu }(s),\mu(s) )- b(Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) ds + \int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2\\
   &\le \sup_{0\le t \le \tau } 2t \int_0^{t} \abs{b(Y^{\mu }(s),\mu(s) )- b(Y^{\tilde{\mu}}(s),\tilde{\mu}(s) )}^2 ds \\
   &+ \sup_{0\le t \le \tau }2 \abs*{\int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2 \\
 .\end{align*}
 Now taking the expectation 
 \begin{align*}
   &\E[\sup_{0\le t \le \tau }\abs{Y^{\mu }(t) - Y^{\tilde{\mu } }(t)}^2] \\
   &\le 4\tau  L^2 \E\left[\int_0^{\tau } \abs{Y^{\mu }(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s) ,\tilde{\mu}(s) ) ds \right]\\
   &+ 16 L^2 \E[\int_0^{\tau } \abs{Y^{\mu}(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s),\tilde{\mu }(s) )  ds]
 .\end{align*}
 Where we used Doobs-$L^{p} $ inequality for the second term.
 \begin{align*}
   &\E[\sup_{0\le t\le \tau }\abs*{\int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2] \\
   &\le 8 \E[\int_0^{\tau } \abs{\sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) )}^2 ds]\\
   &\le 8 \E[\int_0^{\tau } \abs{Y^{\mu}(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s),\tilde{\mu }(s) )  ds]
 .\end{align*}
 All together 
 \begin{align*}
   \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_{\tau}^2] &\le C \int_0^{\tau } \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_s^2] ds + C \int_0^{\tau } \E[W_2^2(\mu(s),\tilde{\mu }(s) )]  ds  \\
 .\end{align*}
 So by GrÃ¶nwall inequality we get 
 \begin{align*}
   \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_{\tau}^2] &\le C(\tau )* \int_0^{\tau }  W_2^2(\mu(s) ,\tilde{\mu }(s) ) ds \\                                               
                                                 &\le C(\tau )* \int_0^{\tau }  \sup_{0\le t \le s} W_2^2(\mu(t) ,\tilde{\mu }(t) ) ds \\                                              
                                                 &\le C(\tau ) \int_0^{\tau } d_s(\mu ,\tilde{\mu } )ds
 .\end{align*}
 using  the inequality \autoref{c_d_wasserstein}\\[1ex]
 remember that $\phi(\mu) = \mathcal{L}(Y^{\mu } )$ and $\phi(\tilde{\mu } ) = \mathcal{L}(Y^{\tilde{\mu } } )$, then
 \begin{align*}
   d_{\tau }^2(\phi(\mu ),\phi(\tilde{\mu } )) =  \inf_{\pi  \in  \Pi(\phi(\mu) ,\phi(\tilde{\mu } ))} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}   } \|x-y\|_\tau^2 d\pi(x,y)
 .\end{align*}
 now if we take joint distribution of $Y^{\mu } $ and $Y^{\tilde{\mu } }$ . $\pi_1$ we can write 
 \begin{align*}
   \E[\|Y^{\mu }-Y^{\tilde{\mu } } \|_\tau^2 ] &= \int_{\mathcal{C}^{d},\mathcal{C}^{d}  } \|x-y\|_{\tau }^{2} d\pi_1(x,y)  \\
                                               &\le  C(\tau ) \int_0^{\tau } d_s(\mu ,\tilde{\mu } )ds
 .\end{align*}
 Lets summarize, for $\forall  \mu , \tilde{\mu } \mathcal{P}^2(\mathcal{C}^{d} ) $ we obtained 
 \begin{align*}
   d_t(\phi(\mu ),\phi(\tilde{\mu } )) \le C(t) \int_0^{t} d_s(\mu ,\tilde{\mu } ) ds \tag{*}
 .\end{align*}
To prove the uniqueness of solutions. If we have two solutions $\mu ,\tilde{\mu } $ i.e. 
\begin{align*}
  \phi(\mu ) &= \mu \\
  \phi(\tilde{\mu } ) &= \tilde{\mu } 
.\end{align*}
then the above estimate (*) says 
\begin{align*}
  d(\mu,\tilde{\mu } ) \le  C(t) \int_0^{t} ds(\mu ,\tilde{\mu } )  ds \implies d_t(\mu ,\tilde{\mu } )  = 0
.\end{align*}
To prove the existence. Take arbitrary $\mu_0 \in  \mathcal{P}^2(\mathcal{C}^{d} )$, (for example $\mu_0 = \mathcal{L}(\xi)$)
\begin{align*}
  \phi(\mu_0) &= \mu_1 \\
  \phi(\mu_1) &= \mu_2 \\
              &\vdots \\
  \phi(\mu_k) &= \mu_{k+1}
.\end{align*}
the estimate means that $(\mu_k)$ is Cauchy in $\mathcal{P}^2(\mathcal{C}^{d} )$
\begin{align*}
  d_t(\mu_{k+m},\mu_m) \le  \sum \ldots 
.\end{align*}
Then there exists a $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ such that 
\begin{align*}
  W_2^{2}(\mu_k,\mu ) \to  0 
.\end{align*}
\end{proof}
\begin{remark}
 That in our case the empirical measure $\mu_N$ is not exactly the law of $X^{N} $ and is 
 stochastic, such that the above proof does not exactly holds for our (SDEN)\\
 For our initial data we already know that 
 \begin{align*}
   \E[W_2^2(\mu_N(0),\mu_0  )] \xrightarrow{N\to \infty} 0
 .\end{align*}
 and we expect for any $t>0$
 \begin{align*}
   \E[W_{\mathcal{C}^{d},2 }^2(\mu_N(t),\mu )] \to 0
 .\end{align*}
\end{remark}
\begin{theorem}[Mean-Field-Limit]
  Let $b$ and $\sigma$ fulfill \autoref{sde_solution_assumption_strong} and use 
  $\mu_N$ the empirical measure, then there exists a measure $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ s.t. 
  \begin{align*}
    \lim_{N\to \infty}\E]W_{\mathcal{C}^{d},2 }^2(\mu_N,\mu ) = 0
  .\end{align*}
  and for any fixed $k \in  \mathbb{N}$ it holds
  \begin{align*}
    \left( X^{N}_1,\ldots ,X_k^{N}   \right)  \xRightarrow{(D)} \left( Y_{1},\ldots ,Y_{k}   \right) 
  .\end{align*}
\end{theorem}
\begin{proof}
  The proof is similar to what we have done in the \autoref{solution_vlasov}, the critical part is to 
  work with our stochastic empirical measure, we do so by introducing an intermediate empirical measure. We compute 
  \begin{align*}
    \abs{X_i^{N}(t) - Y_i(t) }^2 &\le 2 t \int_0^{t} \abs{b(X^{N}_i(s),\mu_N(s) ) -  b(Y_i(s),\mu(s) )} \\
    &+ 2 \abs*{\int_0^{t} \sigma(X_i^{N}(s),\mu_N(s) ) - \sigma(Y_i(s),\mu(s))  dW_s^{i} }^2
  .\end{align*}
  Now using GrÃ¶nwall's inequality implies 
  \begin{align*}
    \E[\sup_{0\le r\le t} \abs{X_i^{N}(r) - Y_i(r) }^2] &\le C \E{\int_0^{t} W_2^{2}(\mu_N(s),\mu(s))  } ds\\
                                                        &\le  C*\E[\int_0^{t} d_r^2(\mu_N,\mu ) dr]
  .\end{align*}
  Let $\overline{\mu }_N $ be the empirical measure of $Y_i$ 
  \begin{align*}
    \overline{\mu }_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{Y_i}  
  .\end{align*}
  And let $\mu \sim \mathcal{L}(Y_i)$ then 
  \begin{align*}
    \E[W_2^2(\overline{\mu }_N,\mu  )] \to  0
  .\end{align*}
  Now we consider for $\forall $ a.s. $\omega  \in  \Omega $
  \begin{align*}
    d_t^2(\overline{\mu }_N,\mu_N ) = \inf_{\pi  \in  \Pi(\mu_N,\overline{\mu }_N )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \|x-y\|^2_t d\pi(x,y) 
  .\end{align*}
  By taking $\pi  = \mu_N \otimes \overline{\mu }_N $ we can write the above integral explicitly 
  \begin{align*}
    \le \frac{1}{N} \sum_{i=1}^{N} \|X^{N}_i - Y_i \|_t^2 
  .\end{align*}
\end{proof}

