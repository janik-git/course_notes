\chapter{MEAN FIELD LIMIT FOR SDE SYSTEM}
\input{lectures_new/stochastic_primer.tex}
\input{lectures_new/ito_integral.tex}
\input{lectures_new/solving_sdes.tex}
\section{Stochastic Mean Field Limit}
First recall the metric we use to talk about distance between two measures i.e the Wasserstein Distance
\begin{definition}[Wasserstein Distance]
  For all $\mu , \nu  \in  \mathcal{P}_p(\mathbb{R}^{d} )$  , $(p\ge 1) $ the Wasserstein Distance of $\mu $ and $\nu $ is given by 
  \begin{align*}
    W^{p}(\mu ,\nu ) = \dist_{MK,p}(\mu ,\nu ) = \inf_{\pi \in  \Pi(\mu ,\nu )} \left( \int \int_{\mathbb{R}^{2d} } \abs{x-y}^{p} \pi(dxdy) \right)^{\frac{1}{p}}  
  .\end{align*}
  Where  
  \begin{align*}
    \Pi(\mu ,\nu ) = \Bigg\{\pi \in \mathcal{P}(\mathbb{R}^{d} \times  \mathbb{R}^{d}  ) : &\int_{\mathbb{R}^{d}\times E } \pi(dx,dy) = \nu(E) \\
                                                                                      &\int_{E \times  \mathbb{R}^{d} } \pi(dx,dy) = \mu(E)\Bigg\}  
  .\end{align*}
\end{definition}
\begin{remark}
 Note that 
 \begin{align*}
   W_1(\mu ,\tilde{\mu } ) \le  W_{2}(\mu ,\tilde{\mu } )
 .\end{align*}
 follows naturally by HÃ¶lders inequality, in fact this holds for all $p>q$
  \begin{align*}
   W_q(\mu ,\tilde{\mu } ) \le  W_{p}(\mu ,\tilde{\mu } )
 .\end{align*}
\end{remark}
\begin{remark}
  Let $(\mu_n)_{n \in  \mathbb{N}} \subset  \mathcal{P}_p(\mathbb{R}^{d} )$ be a sequence of measures,then following are equivalent
  \begin{enumerate}
   \item $W_p(\mu_n,\mu ) \to 0$
    \item  For $\forall f \in \mathcal{C}(\mathbb{R}^{d} )$ such that $\abs{f(x)} \le  C(1+\abs{x}^{p} )$
     \begin{align*}
      \int  f d\mu_n \to \int  f d\mu 
     .\end{align*} 
    \item $\mu_n \rightharpoonup \mu $
  \end{enumerate} 
\end{remark}
\newpage
\subsection{Stochastic Particle System}
Let us begin by shortly defining the stochastic particle systems we study. 
\begin{definition}[Empirical Measure (Stochastic version)]\label{empirical_stochastic}
  For random variables $(X_i)_{i\le N}$  we define the (stochastic) empirical measure by
  \begin{align*}
    \mu_N(\omega ) = \frac{1}{N}\sum_{i=1}^{N} \delta_{X_i(\omega)} 
  .\end{align*}
\end{definition}
Then our stochastic particle system is given by,
\begin{definition}[Stochastic Particle System ]\label{sden}
  For $N$ interacting particles $(X^{1} ,\ldots ,X^{N} )$ with i.i.d initial data $(X_i^{N}(0))_{i \in  \{1,\ldots ,N\}  } \subset  L^2(\Omega) $ and law $\mu_0$
\begin{align*}
  \text{(SDEN)}\begin{cases}
    d X_i^{N}(t) &=   b(X_i^{N}(t) , \mu_N(t) )dt + \sigma(X_i(t)^{N},\mu_N(t))dW^{i}_t \\
    X_i^{N}(0) &= X_{i,0}^{N}   
  \end{cases}
.\end{align*}
Where $\mu_N$ is the stochastic empirical measure and note $\mathcal{L}(X_{0}) = \mu_0$
\end{definition}
\begin{remark}
  The dimensions for our Stochastic-Particle-System are the same as in \autoref{sde}
\end{remark}
\begin{remark}
  For our initial measure we already have
\begin{align*}
  \E[W_2^{2}(\mu_N(0),\mu_0) ] \to  0
.\end{align*}
\end{remark}
\subsection{I.I.D Case}
Let us shortly consider the convergence of the empirical measure in the case where our random variables are i.i.d, note in the mean field limit 
this is only the case for our initial data all particles at $t>0$ are not i.i.d.
\begin{corollary}
  If  $(X_i)_{i \in  \{1,\ldots ,N\}  }$ are i.i.d random variables with law $\mu_{X}$ then  $\forall  \ f \in  \mathcal{C}_b(\mathbb{R}^{d} ) $ it holds that
  \begin{align*}
    \P(\lim_{N \to \infty} \int  f d \mu_N = \int f d\mu ) = 1
  .\end{align*}
\end{corollary}
We can actually prove the stronger statement that the choice of $f \in  \mathcal{C}_b$ does not matter for the convergence i.e.
we can pull the function selection into the probability similarly to the difference between modification and indistinguishable. 
\begin{corollary}
  If  $(X_i)_{i \in  \{1,\ldots ,N\}  }$ are i.i.d random variables with law $\mu_{X}$ then  it holds that
\begin{align*}
  \P(\mu_N \rightharpoonup \mu ) = 1
.\end{align*}
i.e 
\begin{align*}
 \P( \forall  f \in  \mathcal{C}_b(\mathbb{R}^{d} ) \ : \int f d\mu_N \to  \int  f d\mu )  = 1
.\end{align*}
\end{corollary}
\begin{proof}
  \textcolor{Red}{Needs revision, this should only work for $\mathcal{C}_b(K)$ $K$ compact in my opinion}\\
 The proof relies mainly on showing that $\mathcal{C}_b(\mathbb{R}^{d} )$  is separable for compact support 
 we can use the density of the polynomials. Then we can go from arbitrary $f$ to the union over a countable sequence of $f$ 
 and then argue through separability that this is equal to the entire space.
\end{proof}
\newpage
\begin{lemma}[General Dominated Convergence]\label{general_dct}
  Let $(X_n)_{n \in  \mathbb{N}} \subset   L^{p} $ be a sequence of random variables then the following are equivalent 
  \begin{enumerate}
    \item $(X_n)_{n \in  \mathbb{N}}$ are uniformly integrable  and $X_n \to X$ $\P$-a.s.
    \item $\|X_n - X\| \to  0$ for some $X \in  L^p$
  \end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}
\begin{remark}
  In general a sequence $(X_i)_{i \in  \mathbb{N}}$ is called uniform integrability means that 
 \begin{align*}
   \lim_{r \to  \infty} \sup_{i \in  \mathbb{N}} \E[\abs{X_i}*\cha_{\abs{X_i}\ge r}] = 0
 .\end{align*}
\end{remark}
\begin{lemma}[De la Vall\`ee Poussin Criterion]\label{de_la_valle}
 A sequence of random variables $(X_i)$  is uniformly integrable iff there 
 $\exists \phi $ convex with
 \begin{align*}
   \lim_{x \to \infty} \frac{\phi(x)}{x} = \infty
 .\end{align*}
s.t.
\begin{align*}
 \sup_i \E[\phi(\abs{X_i})] < \infty
.\end{align*}
\end{lemma}
\begin{proof}
  As the construction of $\phi $ is heavily technical we refer to xyz
\end{proof}
\begin{corollary}\label{wasserstein_convergence_arb}
  If $(X_i)_{i \in \{1,\ldots ,N\}  }$  are i.i.d random variables with law $\mu_X$ and $\int \abs{x}^{p} \mu  < \infty $ i.e $\mu  \in  \mathcal{P}^p(\mathbb{R}^{d} ) $
 \begin{align*} 
 W_p(\mu_N,\mu ) \to  0 \qquad \text{ a.s.}
 .\end{align*}
 and 
 \begin{align*}
   \E[W_p^{p}(\mu_N,\mu ) ] \to 0
 .\end{align*}
 Where 
 \begin{align*}
   \mu_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i} 
 .\end{align*}
\end{corollary}
\begin{proof}
 Remember that the following convergences are equivalent 
 \begin{enumerate}
   \item $W_p(\mu_N,\mu ) \to 0$
   \item $\mu_N \rightharpoonup \mu $ and $\int \abs{x}^{p} d\mu_N \to \int \abs{x}^{p} d\mu   $
   \item $\mu_n \rightharpoonup \mu $ and $\lim_{n\to \infty} \sup_r \int_{\abs{x} \ge r} \abs{x}^p d\mu_N = 0$
 \end{enumerate}
 Note that if we fix a.s. $\omega $ then we can treat this as the deterministic case. \\[1ex]
 We already know that 
 \begin{align*}
  \mu_N \rightharpoonup \mu \text{ a.s.}
 .\end{align*}
since $(X_i)$ are i.i.d then $\abs{X_i}^{p} $ is also i.i.d  and we use the Law of large numbers
 \begin{align*}
   \int \abs{x}^{p} d\mu_N   &= \frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}  \xrightarrow{L.L.N.} \E[\abs{X_i}^{p} ] < \infty
 .\end{align*} 
 And we get a.s. that $W_p(\mu_N,\mu ) \to  0$\\[1ex]
 For the stronger statement 
 \begin{align*}
   \E[W^{p}(\mu_n,\mu ) ] \to  0
 .\end{align*}
  we first note that  
 \begin{align*}
   W_p^{p}(\mu_N,\mu )  &\le  2^{p-1} (W^{p}_p(\mu_N,\delta_0)  + W_p^{p}(\delta_0,\mu )) \\
                        &= 2^{p-1} (\frac{1}{N} \sum_{i=1}^{N}   \abs{X_i}^{p}  + W_p^{p}(\delta_0,\mu ))
 .\end{align*}
 then it is sufficient to show the uniform integrability of the first part 
 \begin{align*}
  \frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}  
 .\end{align*}
 Since $\abs{X_i}^{p} $ is integrable then there exists a convex function  $\phi $ with $\lim_{x \to \infty} \frac{\phi(x)}{x} = \infty$ and 
 \begin{align*}
   \E[\phi(\abs{X_i}^{p} )] < \infty
 .\end{align*}
 Since $\phi $ is convex we apply Jensen's inequality to get 
 \begin{align*}
   \sup_N \E[\phi \left(\frac{1}{N}\sum_{i=1}^{N} \abs{X_i}^{p}\right)  ]  \myS{Jen.}{\le } \sup_N \sum_{i=1}^{N}\E[\phi (\abs{X_i}^{p} )] =  \E[\phi(\abs{X_i}^{p} )] < \infty
 .\end{align*}
 Finally \autoref{de_la_valle}  implies the uniform integrability and we conclude by \autoref{general_dct}
 \begin{align*}
   \E[W_p^{p}(\mu_N,\mu ) ]\to 0
 .\end{align*}
\end{proof}
\hspace{0mm}\\[1ex]
All the above statement only apply to arbitrary i.i.d sequences of random variables, but in our Mean-Field-Limit we only get 
the i.i.d property at $t=0$ such that we seek to prove that even as $N \to \infty$ we nonetheless get a convergence.
\begin{remark}
 Formally our goal is to prove the convergence 
 \begin{align*}
   \E[\sup_{t} W_2^{2}(\mu_N(t),\mu(t))  ] \to 0
 .\end{align*}
\end{remark}
\newpage
\subsection{Toy Example}
Let us first consider a simple stochastic particle system given by 
\begin{assumption}\label{sde_solution_assumption_strong}
Assume drift  $b : \mathbb{R}^{d} \times  \mathcal{P}^2(\mathbb{R}^{d} ) \to  \mathbb{R}^{d} $ and diffusion $\sigma : \mathbb{R}^{d} \times  \mathcal{P}^2(\mathbb{R}^{d} ) \to \mathbb{R}^{d \times  m}  $   are Lipschitz continuous i.e. $\exists  L >0$ s.t.
 \begin{align*}
  \abs{b(X,\mu ) - b(\tilde{X},\tilde{\mu }  )} + \abs{\sigma(X,\mu ) - \sigma(\tilde{X},\tilde{\mu }  )} \le  L \left( \abs{X - \tilde{X} } + W_2(\mu ,\tilde{\mu } ) \right) 
 .\end{align*}
\end{assumption}
\begin{example}[Stochastic Toy Model]
  Let our particle system be given as in \autoref{sden} with drift and diffusion for $\nabla V \in  \text{Lip}$
 \begin{align*}
   b(X,\mu )&= \nabla V \star  \mu(X)\\
   \sigma(X,\mu ) &= \sigma_0 >0
 .\end{align*}
\end{example}
\begin{exercise}
 Think about what happens if the initial data is i.i.d but the diffusion term is 0, can you prove a convergence ?
\end{exercise}
\begin{theorem}[Convergence Of Toy Model For Fixed $N$]
  Let our \hyperref[sden]{(SDEN)} be given with drift and diffusion as above and assume they fulfill \autoref{sde_solution_assumption_strong}, then  
  for fixed $N$ we get a unique strong solution in $\mathbb{L}^{2}_{dN}([0,T]) $
\end{theorem}
\begin{proof} 
  First we note that by \autoref{sde_solution_assumption_strong} we get
  \begin{align*}
    \abs{b(X,\mu ) - b(\tilde{X},\tilde{\mu }  )} &= \abs*{\int \nabla V(X-y) d\mu(y) - \int  \nabla V(\tilde{X}-y ) d \tilde{\mu }(y) }\\
                                                  &\ge \int \abs{\nabla V(X-y) - \nabla V(\tilde{X}-y )}d\mu (y) + \abs*{\int \nabla V(\tilde{X - y}(d\mu(y) - d \tilde{\mu }(y) ) )}\\
                                                  &\myS{Lip.}{\le }L*\abs{X - \tilde{X} } + LW_1(\mu ,\tilde{\mu } ) \\
                                                  &\le L*(\abs{X-\tilde{X} } + W_2(\mu,\tilde{\mu } ))
  .\end{align*}
  Let use the notation $\mathbb{X} = (X_1^{N},\ldots ,X_N^{N}  ) \in  \mathbb{R}^{dn} $ and $\mathbb{W} = (W^{1},\ldots ,W^{N}  )$ then
  \begin{align*}
    &B(\mathbb{X}) = \begin{pmatrix} \vdots \\ b(X_i^{N},\frac{1}{N}\sum_{k=1}^{N}  \delta_{X_k} ) \end{pmatrix}_{dN} \\ 
    &\Sigma(\mathbb{X})_{dN \times mN} \ : \ \text{diag}(\Sigma(\mathbb{X})) = \begin{pmatrix} \delta(X_1,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k} ), \ldots \delta(X_N,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k} ) \end{pmatrix}
  .\end{align*}
  Then our SDE is given by 
  \begin{align*}
    d \mathbb{X}(t) = B(\mathbb{X}(t)) dt + \Sigma(\mathbb{X}(t)) d \mathbb{W}_t
  .\end{align*}
  Now if $B$ and $\Sigma $ satisfy \autoref{assumption_sde_sol} we get a solution by \autoref{sde_solution_theorem}
  \begin{align*}
    \abs{B(\mathbb{X})-B(\mathbb{Y})}_{\mathbb{R}^{dn} }^2  &= \sum_{j=1}^{N} \abs{X_j,\frac{1}{N}\sum_{k=1}^{N} \delta_{X_k}  - b(Y_j,\frac{1}{N}\sum_{k=1}^{N} \delta_{Y_k} )}  \\
                                                            &\le \sum_{j=1}^{N} 2L^2 \left( \abs{X_j-Y_j}^2  + W_2^2(\mu_N(X),\mu_N(Y))\right)   \\
                                                            &\le  4L^2 \|\mathbb{X} - \mathbb{Y}\|^2 
  .\end{align*}
  For $\Sigma $ the argument is analog where for the Wasserstein distance we used 
  Then by \autoref{sde_solution_theorem} we get a solution $X \in  L^2([0,T])$ for fixed $N$\\[1ex]
\end{proof}
\begin{remark}
 To get a bound on the Wasserstein Distance we used the following  
  \begin{align*}
    \pi  = \frac{1}{N} \sum_{k=1}^{N} \delta_{(X_k,Y_k)}  \in  \Pi
  .\end{align*}
  then  the Wasserstein distance is given by 
  \begin{align*}
   \frac{1}{N}\sum_{k=1}^{N} \abs{X_k-Y_k}^2  
  .\end{align*}
  and one can further simplify to get the bound used.
\end{remark}
\begin{remark}
  As $N \to \infty$ we expect to get the following 
  \begin{align*}
    \begin{cases}
      dY^{i}(t) &= b(Y^{i}(t) ,\mu(t) ) dt + \sigma(Y^{i}(t),\mu(t) )dW_t^i \\
      Y^{i}(0)  &=  X_{i,0}^{N} \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}
  In fact since the above system beyond the initial data is independent of $N$, we may consider the simplified equation
  \begin{align*}
    \begin{cases}
      dY(t) &= b(Y(t) ,\mu(t) ) dt + \sigma(Y(t),\mu(t) )dW_t^i \\
      Y(0)  &=  \xi \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}
this equation is called Makean-Vlasov equation which is a non-linear non-local SDE
\end{remark}
\newpage
\subsection{Makean-Vlasov}
\begin{definition}[Makean-Vlasov Equation]\label{MVE}
  The following non-linear and non-local SDE is called Makean-Vlasov Equation 
\begin{align*}
    \text{(MVE)} \begin{cases}
      dY(t) &= b(Y(t) ,\mu(t) ) dt + \sigma(Y(t),\mu(t) )dW_t^i \\
      Y(0)  &=  \xi \in  L^2(\Omega ) \text{ i.i.d} 
    \end{cases}
  .\end{align*}  
\textcolor{Red}{Add Space of $Y$ and dimensions} 
\end{definition}

\begin{definition}[Space Of Continuous Sample Paths]
 The Space  $\mathcal{C}^{d} = \mathcal{C}([0,T];\mathbb{R}^{d} ) $ is called the continuous sample path space with norm 
 \begin{align*}
   \|X\|_t = \sup_{0\le t \le T} \abs{X(t)}
 .\end{align*}
 this norm $\|*\|_T$ induces a $\sigma$-algebra on $\mathcal{C}^{d} $ 
\end{definition}
\begin{definition}[Random Variable]
  A random Variable on $\mathcal{C}^{d} $ is a map 
  \begin{align*}
    X : \Omega_{\text{a.s.}} \to \mathcal{C}^{d} 
  .\end{align*}
\end{definition}
\begin{definition}[Measure]
 Since the norm $\|*\|_T$  induces a $\sigma$-algebra on $\mathcal{C}^{d} $ we can define measures $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ by 
 \begin{align*}
   \mu  \coloneqq  (\mu(t))_{t \in  [0,T]} \qquad \mu(t)
 .\end{align*}
 and by using the function 
 \begin{align*}
  l_t : \mathcal{C}^{d} \to  \mathbb{R}^{d} \ X \mapsto X(t)  
 .\end{align*}
 then we get a measure on $\mathbb{R}^{d} $ by using the pushforward
 \begin{align*}
  \mu_t \coloneqq \mathcal{B} \to \mathbb{R}^{d}   \ A \mapsto \mu(l^{-1}_t(A) )
 .\end{align*}
\end{definition}

\begin{definition}[Wasserstein Distance]\label{c_d_wasserstein}
And we can define for arbitrary measures $\mu ,\tilde{\mu } \in  \mathcal{P}^2(\mathcal{C}^{d} ) $ the Wasserstein distance by
\begin{align*}
  \sup_{t \in  [0,T]} W_{\mathbb{R}^{d},2 }(\mu(t),\tilde{\mu }(t) ) \le  W_{\mathcal{C}^{d},2 } (\mu ,\tilde{\mu } )
.\end{align*}
Where 
\begin{align*}
  W_{\mathcal{C}^{d},2 }(\mu ,\tilde{\mu } ) = \inf_{\pi  \in  \Pi(\mu ,\tilde{\mu } )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \|x-y \|^2 d\pi(x,y)
.\end{align*}
\end{definition}
\begin{corollary}
  Let us prove the inequality
\end{corollary}
We choose concrete $\pi_t =  l_t \# \pi $ for $\pi  \in  \Pi_{\mathcal{C}^{d}  }(\mu ,\tilde{\mu } )$
\begin{proof}
  \begin{align*}
    \sup_{t \in  [0,T]} W(\mu_t,\tilde{\mu }_t ) &\le \sup_{t \in  [0,T]} \int_{\mathbb{R}^{d} \times  \mathbb{R}^{d} } \abs{x-y}^2 d \pi_t(x,y)\\
                                                 &= \sup_{t \in  [0,T]} \int_{\mathbb{R}^{d} \times  \mathbb{R}^{d} } \abs{x-y}^2 d l_t^2 \# \pi_t(x,y)\\
                                                 &= \sup_{t \in  [0,T]} \int_{\mathcal{C}^{d}  \times  \mathcal{C}^{d} } \abs{* - *}^2 \circ l^2_t(x,y) d\pi(x,y) \\
                                                 &= \sup_{t \in  [0,T]} \int_{\mathcal{C}^{d} \times \mathcal{C}^{d}   } \abs{x(t)-y(t)}^2 d\pi(x,y)\\
                                                 &\le  \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \sup_{t \in  [0,T]} \abs{x(t)-y(t)}^2 d\pi(x,y) \\
                                                 &= \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \|x-y\|_\infty ^2 d\pi(x,y)
  .\end{align*}
  It remains to check that $p_t \in  \Pi(\mu_t,\tilde{\mu}_t )$, let $A \in  \mathcal{B}(\mathbb{R}^{d} )$
  \begin{align*}
    \pi_t(A \times  \mathbb{R}^{d} ) &= \pi (l_t^{-1}(A \times  \mathbb{R}^{d} )) \\
                                     &= \pi(\{(x,y) \in  \mathcal{C}^{d} \times  \mathcal{C}^{d} \ : \ l_t(x) \in  A, l_t(y) \in  \mathbb{R}^{d}   \}  )\\
                                     &=\pi (\{(x,y) \in  \mathcal{C}^{d} \times  \mathcal{C}^{d} \ : \ l_t(x) \in  A  , y \in  \mathcal{C}^{d}  \}  ) \\
                                     &= \pi(l_t^{-1}(A) \times \mathcal{C}^{d} )\\
                                     &= \mu(l_t^{-1}(A)) \\
                                     &= l_t \# \mu(A)\\
                                     &= \mu_t(A)
  .\end{align*}
\end{proof}
\begin{remark}
 Note that
 \begin{align*}
   \int_{\mathcal{C}^{d} } f(x) d\mu(x) = \int_{\mathbb{R}^{d} }  f(x(t)) d\mu_t
 .\end{align*}
\end{remark}
\begin{theorem}[Unique and Existence of Solution for Makean-Vlasov]\label{solution_vlasov}
  If $b$ and $\sigma $ satisfy \autoref{sde_solution_assumption_strong} then MVE has a unique and strong solution
  $Y \in  \mathbb{L}^2([0,T])$ and $\mu  \in  \mathcal{L}(Y)$
\end{theorem}
\begin{proof}
 We use the notation 
 \begin{align*}
   d_t^2 =  \inf_{\pi  \in  \Pi(\mu,\tilde{\mu } )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  }\|x-y\|^2_t d\pi(x,y)
 .\end{align*}
 For any given $\mu  \in \mathcal{P}^2(\mathcal{C}^{d} )$ we consider the following SDE 
 \begin{align*}
   \begin{cases} 
   dY^{\mu } (t) &= b(Y^{\mu} (t),\mu(t))dt + \sigma(Y^{\mu } (t),\mu(t))dW_t\\
    Y(0)   & \xi \in  L^2(\Omega)
   \end{cases}
 .\end{align*}
 Let $\phi(\mu ) = \mathcal{L}(Y^{\mu } ) $ be the law of $Y^{\mu} $. \\[1ex]
 For the existence  and the uniqueness of $Y^{\mu } $ we need to check 
 \begin{align*}
   \abs{b(x,\mu(t)) - b(\tilde{x},\mu(t) )} + \abs{\sigma(x,\mu(t)) - \sigma(\tilde{x},\mu(t))} \le  L \abs{x-\tilde{x} }
 .\end{align*}
 Since it is the same measure the Wasserstein distance is 0 and the above is true by \autoref{sde_solution_assumption_strong}.\\
 If $\phi $ has a fixpoint $\overline{\mu } $, then $\overline{\mu } $ is the solution of MVE.
 We prove this by first bounding the difference between two measures, let $\mu ,\tilde{\mu }$ be arbitrary given measure i n
 $\mathcal{P}^2(\mathcal{C}^{d} )$, first note
 \begin{align*}
  Y^{\mu }(t) - \xi = \int_0^{t} b(Y^{\mu }(s),\mu(s) ) ds + \int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) dW_s \qquad \mu= \mu,\tilde{\mu} 
 .\end{align*}
 then by taking the difference 
 \begin{align*}
   &\sup_{0\le t \le \tau }\abs{Y^{\mu }(t) - Y^{\tilde{\mu } }(t)}^2 \\
   &= \sup_{0\le t \le s}\abs*{\int_0^{t} b(Y^{\mu }(s),\mu(s) )- b(Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) ds + \int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2\\
   &\le \sup_{0\le t \le \tau } 2t \int_0^{t} \abs{b(Y^{\mu }(s),\mu(s) )- b(Y^{\tilde{\mu}}(s),\tilde{\mu}(s) )}^2 ds \\
   &+ \sup_{0\le t \le \tau }2 \abs*{\int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2 \\
 .\end{align*}
 Now taking the expectation 
 \begin{align*}
   &\E[\sup_{0\le t \le \tau }\abs{Y^{\mu }(t) - Y^{\tilde{\mu } }(t)}^2] \\
   &\le 4\tau  L^2 \E\left[\int_0^{\tau } \abs{Y^{\mu }(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s) ,\tilde{\mu}(s) ) ds \right]\\
   &+ 16 L^2 \E[\int_0^{\tau } \abs{Y^{\mu}(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s),\tilde{\mu }(s) )  ds]
 .\end{align*}
 Where we used Doobs-$L^{p} $ inequality for the second term.
 \begin{align*}
   &\E[\sup_{0\le t\le \tau }\abs*{\int_0^{t} \sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) ) dW_s}^2] \\
   &\le 8 \E[\int_0^{\tau } \abs{\sigma(Y^{\mu }(s),\mu(s) ) - \sigma (Y^{\tilde{\mu}}(s),\tilde{\mu}(s) )}^2 ds]\\
   &\le 8 \E[\int_0^{\tau } \abs{Y^{\mu}(s) - Y^{\tilde{\mu } }(s)  }^2 + W_2^2(\mu(s),\tilde{\mu }(s) )  ds]
 .\end{align*}
 All together 
 \begin{align*}
   \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_{\tau}^2] &\le C \int_0^{\tau } \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_s^2] ds + C \int_0^{\tau } \E[W_2^2(\mu(s),\tilde{\mu }(s) )]  ds  \\
 .\end{align*}
 So by GrÃ¶nwall inequality we get 
 \begin{align*}
   \E[\|Y^{\mu } - Y^{\tilde{\mu } }\|_{\tau}^2] &\le C(\tau )* \int_0^{\tau }  W_2^2(\mu(s) ,\tilde{\mu }(s) ) ds \\                                               
                                                 &\le C(\tau )* \int_0^{\tau }  \sup_{0\le t \le s} W_2^2(\mu(t) ,\tilde{\mu }(t) ) ds \\                                              
                                                 &\le C(\tau ) \int_0^{\tau } d_s(\mu ,\tilde{\mu } )ds
 .\end{align*}
 using  the inequality \autoref{c_d_wasserstein}\\[1ex]
 remember that $\phi(\mu) = \mathcal{L}(Y^{\mu } )$ and $\phi(\tilde{\mu } ) = \mathcal{L}(Y^{\tilde{\mu } } )$, then
 \begin{align*}
   d_{\tau }^2(\phi(\mu ),\phi(\tilde{\mu } )) =  \inf_{\pi  \in  \Pi(\phi(\mu) ,\phi(\tilde{\mu } ))} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}   } \|x-y\|_\tau^2 d\pi(x,y)
 .\end{align*}
 now if we take joint distribution of $Y^{\mu } $ and $Y^{\tilde{\mu } }$ . $\pi_1$ we can write 
 \begin{align*}
   \E[\|Y^{\mu }-Y^{\tilde{\mu } } \|_\tau^2 ] &= \int_{\mathcal{C}^{d},\mathcal{C}^{d}  } \|x-y\|_{\tau }^{2} d\pi_1(x,y)  \\
                                               &\le  C(\tau ) \int_0^{\tau } d_s(\mu ,\tilde{\mu } )ds
 .\end{align*}
 Lets summarize, for $\forall  \mu , \tilde{\mu } \mathcal{P}^2(\mathcal{C}^{d} ) $ we obtained 
 \begin{align*}
   d_t(\phi(\mu ),\phi(\tilde{\mu } )) \le C(t) \int_0^{t} d_s(\mu ,\tilde{\mu } ) ds \tag{*}
 .\end{align*}
To prove the uniqueness of solutions. If we have two solutions $\mu ,\tilde{\mu } $ i.e. 
\begin{align*}
  \phi(\mu ) &= \mu \\
  \phi(\tilde{\mu } ) &= \tilde{\mu } 
.\end{align*}
then the above estimate (*) says 
\begin{align*}
  d(\mu,\tilde{\mu } ) \le  C(t) \int_0^{t} ds(\mu ,\tilde{\mu } )  ds \implies d_t(\mu ,\tilde{\mu } )  = 0
.\end{align*}
To prove the existence. Take arbitrary $\mu_0 \in  \mathcal{P}^2(\mathcal{C}^{d} )$, (for example $\mu_0 = \mathcal{L}(\xi)$)
\begin{align*}
  \phi(\mu_0) &= \mu_1 \\
  \phi(\mu_1) &= \mu_2 \\
              &\vdots \\
  \phi(\mu_k) &= \mu_{k+1}
.\end{align*}
the estimate means that $(\mu_k)$ is Cauchy in $\mathcal{P}^2(\mathcal{C}^{d} )$
\begin{align*}
  d_t(\mu_{k+m},\mu_m) \le  \sum \ldots 
.\end{align*}
Then there exists a $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ such that 
\begin{align*}
  W_2^{2}(\mu_k,\mu ) \to  0 
.\end{align*}
\end{proof}
\begin{remark}
 That in our case the empirical measure $\mu_N$ is not exactly the law of $X^{N} $ and is 
 stochastic, such that the above proof does not exactly holds for our (SDEN)\\
 For our initial data we already know that 
 \begin{align*}
   \E[W_2^2(\mu_N(0),\mu_0  )] \xrightarrow{N\to \infty} 0
 .\end{align*}
 and we expect for any $t>0$
 \begin{align*}
   \E[W_{\mathcal{C}^{d},2 }^2(\mu_N(t),\mu )] \to 0
 .\end{align*}
\end{remark}
\begin{theorem}[Mean-Field-Limit]
  Let $b$ and $\sigma$ fulfill \autoref{sde_solution_assumption_strong} and use 
  $\mu_N$ the empirical measure, then there exists a measure $\mu \in \mathcal{P}^2(\mathcal{C}^{d} )$ s.t. 
  \begin{align*}
    \lim_{N\to \infty}\E]W_{\mathcal{C}^{d},2 }^2(\mu_N,\mu ) = 0
  .\end{align*}
  and for any fixed $k \in  \mathbb{N}$ it holds
  \begin{align*}
    \left( X^{N}_1,\ldots ,X_k^{N}   \right)  \xRightarrow{(D)} \left( Y_{1},\ldots ,Y_{k}   \right) 
  .\end{align*}
\end{theorem}
\begin{proof}
  The proof is similar to what we have done in the \autoref{solution_vlasov}, the critical part is to 
  work with our stochastic empirical measure, we do so by introducing an intermediate empirical measure. We compute 
  \begin{align*}
    \abs{X_i^{N}(t) - Y_i(t) }^2 &\le 2 t \int_0^{t} \abs{b(X^{N}_i(s),\mu_N(s) ) -  b(Y_i(s),\mu(s) )} \\
    &+ 2 \abs*{\int_0^{t} \sigma(X_i^{N}(s),\mu_N(s) ) - \sigma(Y_i(s),\mu(s))  dW_s^{i} }^2
  .\end{align*}
   We get %(Now using GrÃ¶nwall's inequality implies)
  \begin{align*}
    \frac{1}{N}\sum_{i=1}^{N} \E[\sup_{0\le r\le t} \abs{X_i^{N}(r) - Y_i(r) }^2] &\le C \E{\int_0^{t} W_2^{2}(\mu_N(s),\mu(s))  } ds\\
                                                        &\le  C*\E[\int_0^{t} d_r^2(\mu_N,\mu ) dr]
  .\end{align*}
  Let $\overline{\mu }_N $ be the empirical measure of $Y_i$ 
  \begin{align*}
    \overline{\mu }_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{Y_i}  
  .\end{align*}
  And let $\mu \sim \mathcal{L}(Y_i)$ for $\forall  t > 0$ then 
  \begin{align*}
    \E[W_2^2(\overline{\mu }_N,\mu  )] \to  0
  .\end{align*}
  Now we consider for $\forall $ a.s. $\omega  \in  \Omega $
  \begin{align*}
    d_t^2(\overline{\mu }_N,\mu_N ) = \inf_{\pi  \in  \Pi(\mu_N,\overline{\mu }_N )} \int_{\mathcal{C}^{d} \times  \mathcal{C}^{d}  } \|x-y\|^2_t d\pi(x,y) 
  .\end{align*}
  By taking $\pi  = \mu_N \otimes \overline{\mu }_N $ we can write the above integral explicitly 
  \begin{align*}
    \le \frac{1}{N} \sum_{i=1}^{N} \|X^{N}_i - Y_i \|_t^2 
  .\end{align*}
  We continue by taking the expectation
  \begin{align*}
    \E[d_t^2(\mu_N,\overline{\mu }_N )] &\le  \frac{1}{N} \sum_{i=1}^{N} \E[\sup_{0\le s \le t} \|X_i(s)^{N} - Y_i(s) \|_t^2] \\
                                        &\le 2C\int_0^{t} \E[d_r^2(\mu_N,\mu )] dr 
  .\end{align*}
  Goal is to get a GrÃ¶nwall inequality for 
  \begin{align*}
    \E[d_t^2(\mu_N,\mu )] &\le  2 \E[d_t^2(\mu_N,\overline{\mu }_N )] + 2 \E[d_t^2(\mu_N,\mu )]\\
                          &\le  C \int_0^{t} \E[d_r^2(\mu_N,\mu )] dr + C\E[d_t^2(\overline{\mu}_N,\mu )] \\
  .\end{align*}
  Then by GrÃ¶nwall 
  \begin{align*}
    \E[d_t^2(\mu_N,\mu )] &\le e^{CT} \E[\mu_{N,0}] + e^{CT} \E[d_t^2(\overline{\mu }_N,\mu  )] \xrightarrow{N\to \infty} 0 
  .\end{align*}
  and then for $\forall  1\le k < \infty$. 
  \begin{align*}
    \E[\max_{1\le i \le  k}\sup_{0\le r\le t} \|X_i^{N}(r) - Y_i(r) \|^2] &\le \max_{1 \le i\le k} \frac{1}{N}\sum_{i=1}^{k} \E[\|X_i^{N}-Y_i \|_t^2] \\
                                                                          &\le  C*k\E[d_t^2(\mu_N,\mu   )]\\
                                                                          &\xrightarrow{N\to \infty} 0
  .\end{align*}
  This concludes the proof. \textcolor{Red}{Add small summary}
\end{proof}
\section{PDE Approach To Solving the Makean-Vlasov Equation}
Let us shortly explain the connection between the PDE approach and the SDE approach to solving the Makean-Vlasov equation,
from now on assume $\sigma(Y(t),\mu(t)) = \sqrt{2} $constant, that means the 
 \begin{align*}
   Y(t) &= b(Y(t),\mu(t)) dt + \sqrt{2} dW_t\\
   Y(0) &= \xi \in  L^{2}(\Omega ) 
 .\end{align*}
 and $\mu_0 = \mathcal{L}(\xi)$, applying It\^os formula for $\forall \phi  \in  \mathcal{C}_0^{\infty}([0,T)\times \mathbb{R}^{d} ) $
\begin{align*}
  \phi(Y(t),t) - \phi(Y(0),0) &= \int_0^{t} \frac{\partial \phi }{\partial t}(Y(s),s) + \nabla \phi(Y(s),s)*b(Y(s),\mu(s))  \\
                              &+ \frac{1}{2} \underbrace{\sqrt{2}*\sqrt{2})}_{tr(\sigma *\sigma ^{T} )}* \Delta \phi(Y(s),s) ds \\
                              &+ \int_0^{t} \nabla \phi(Y(s),s)  \sqrt{2} dW_s 
.\end{align*}
We take expectation  on both sides (note the last term is 0)  
\begin{align*}
  &\int_{\mathbb{R}^{d} } \phi(x,t) d\mu(t) - \int_{\mathbb{R}^{d} } \phi(x,0) d\mu_0 \\
  &= \int_0^{t} \int_{\mathbb{R}^{d} } \frac{\partial \phi }{\partial t}(x,s) + \nabla \phi(x,s)*b(x,\mu(s))* \Delta \phi(x,s)  d\mu(s) ds
.\end{align*}
Formally if $\mu$ is regular enough i.e it has density and the density is good enough, then $\mu $ should satisfy the weak PDE
\begin{align*}
  \begin{cases}
    \partial_t \mu  - \Delta \mu  + \nabla * (b(x,\mu) * \mu )  &=0\\
    \mu(0) &= \mu_0
  \end{cases}
.\end{align*}
\textcolor{Red}{Write out the entire thing with distributions as a middle step between the PDE and the Integral}\\[1ex]
Next goal is to solve this PDE
\begin{definition}[Weak PDE]
  If $u$ is the density of $\mu $ we write 
  \begin{align*}
  \begin{cases}
    &\partial_t u  - \Delta u  + \nabla * (b(x,u)*u )  =0\\
                                                      &u(0) = u_0
  \end{cases}
.\end{align*}
\end{definition}
\begin{remark}
 If we manage to find such $u$ then we can plug it in to the \hyperref[MVE]{(MVE)}   equation
 \begin{align*}
   (PDE)\begin{cases}
    &dY_t = b(Y_t,u) dt + \sqrt{2} dW_t \\
    &Y(t) = \xi \in  L^2(\Omega ) \quad \mathcal{L}(\xi) = u
  \end{cases}
 .\end{align*}
 if $b$ is bounded and Lipschitz,then we have a solution, $Y_t $, let $\overline{u} $ be the Law of $Y_t$.
 Then we have by It\^o formula for $\forall  \phi $
\begin{align*}
  &\int_{\mathbb{R}^{d} } \phi (x,t) d\overline{\mu(t)}  - \int_{\mathbb{R}^{d} } \phi(x,0) u_0(x) dx \\
  &=  \int_0^{t} \int_{\mathbb{R}^{d} } \left(\frac{\partial \phi }{\partial t}(x,s) + \nabla \phi(x,s)*b(x,u) -  \Delta \phi(x,s)\right)\overline{u}(x,t) dx ds
.\end{align*}
Which means $\overline{\mu } $ satisfies
\begin{align*}
  \begin{cases}
    &\partial_t \overline{\mu }  - \Delta \overline{\mu }  + \nabla * (b(x,u)*\overline{\mu } ) = 0\\
    &\overline{\mu } \rvert_{t=0}  = u_0
  \end{cases}
.\end{align*}
This means in order to solve the \nameref{MVE} e.g. we need to prove $\overline{u } = u  $
\end{remark}
The Laplacian gives us sufficient regularity, even for bad $b$ i.e unbounded non Lipschitz, a standard example is 
\begin{align*}
  b(Y_t,u) = \int K(Y_t-y)u(y) dy = \int K(y)u(Y_t-y) dy
.\end{align*}
the regularity depends directly on the regularity of $K$  or rather $u$ by properties of convolution.
\begin{definition}[Sobolev Spaces]
 We define roughly  
 \begin{align*}
  H^1(\mathbb{R}^{d} ) = \{u \in  L^2(\mathbb{R}^{d}) \ : \ \nabla u \in  L^2(\mathbb{R}^{d}) \}  
 .\end{align*}
 where $\forall  \phi \in  \mathcal{C}_0^{\infty} $ , with norm $\|u\|_{H_{1}} = \|u\|_2 + \|\nabla u\|_2$
 \begin{align*}
   \nabla u = \braket{\nabla u , \phi } = -\braket{u,\nabla \phi }
 .\end{align*}
 And 
 \begin{align*}
  H^{-1}(\mathbb{R}^{d} )  =  (H^{1}(\mathbb{R}^{} ) )' =  \{l \ : \ l\text{ is bounded linear functionals of } H^{1}(\mathbb{R}^{d} )  \}  
 .\end{align*}
 Then  
 \begin{align*}
   L^2([0,T];H^{1}(\mathbb{R}^{d} ) ) = \{u \ : \ \int_0^{T} \|u(t)\|_{H^1} dt < \infty \}  
 .\end{align*}
\end{definition}
\begin{definition}[Weak Solution]
  We say that $u \in  L^2([0,T];H^{1}(\mathbb{R}^{d} )\cap L^{\infty}([0,T];L^2(\mathbb{R}^{d} ))  )$
  and $\partial_t u \in  L^2([0,T];H^{-1}(\mathbb{R}^{d} ))$ is a weak solution of the (PDE) if for 
  $\forall  \phi  \in \mathcal{C}^{\infty}_0([0,T]\times \mathbb{R}^{d} )  $ it holds 
  \begin{align*}
    \int_0^{T} \braket{\partial_t u , \phi }_{H^{-1},H^{1}  } dt &= \int_0^{T} \int_{\mathbb{R}^{d} }  \nabla \phi * b(x,u) u dx dt \\
                                                                 &- \int_0^{T} \int_{\mathbb{R}^{d} }  \nabla u * \nabla \phi  dx dt
  .\end{align*} 
\end{definition}
\subsection{Heat Kernel}
\begin{definition}[Heat Equation]
  The following PDE is called Heat Equation (last term is source term)
 \begin{align*}
 \partial_t u - \Delta u + \nabla * (b(x,u)*u) = 0
 .\end{align*}
  If $K(x,t)$ is the heat kernel, then we can formally rewrite the above equation into 
  \begin{align*}
    u(x,t) =  \int_{\mathbb{R}^{d} } K(x-y,t) u_0(y) dy - \int_0^{t} \int_{\mathbb{R}^{d} }  K(x-y,t-s) \nabla * (b(y,u(y,s)u(y,s))dy ds 
  .\end{align*}
  This is called Duhamels formula.
\end{definition}
For standard heat equation with source term $f$
\begin{align*}
  \text{(HE)} \coloneqq \partial_t u - \Delta u = f \quad u(0) = u_0
.\end{align*}
\begin{align*}
  u(x,t) = \int  K(x-y,t) u_0(y) dy + \int_0^{t} \int_{\mathbb{R}^{d} }  K(x-y,t-s)f(y,s) dy ds
.\end{align*}
gives the solution formula of the Heat Equation (HE).
\begin{remark}
 We say the heat kernel is the density of the Brownian Motion. 
\end{remark}
\begin{definition}[Heat equation]
 The following PDE is called Heat equation with source $f$ 
 \begin{align*}
   \text{(HE)}\begin{cases}
   \partial_t u - \Delta u &=f\\
   u \rvert_{t=0} &= u_0
 \end{cases} 
 .\end{align*}
\end{definition}
Using  $x \in  \mathbb{R}^{d} $ the Fourier transform 
\begin{align*}
  \mathcal{F} : L^2 \to  L^2 \ u \mapsto \hat{u} 
.\end{align*}
where
\begin{align*}
  \hat{u} (k) = \int_{\mathbb{R}^{d} } u(x) e^{ix*k}  dx 
.\end{align*}
\begin{exercise}
  Proof
\begin{align*}
  - \widehat{\Delta u}  = \abs{k}^2 \hat{u}(k)
.\end{align*}
\textit{Hint} 
\begin{align*}
  \widehat{\nabla u} = \frac{k}{i} \hat{u}(k)
.\end{align*}
\end{exercise}
then we get the equivalent HE 
 \begin{align*}
  \begin{cases}
    \partial_t \hat{u} - \Delta \hat{u} &=\hat{f}\\
   \hat{u} \rvert_{t=0} &= \hat{u}_0
 \end{cases} 
 .\end{align*}
 With 
 \begin{align*}
   \begin{cases}
     \partial_t \hat{u}(k)  + \abs{k}^2 \hat{u} (k) &= \hat{f}(k) \\
     \hat{u}_0(k)
   \end{cases}
 .\end{align*}
 \begin{align*}
  \hat{u} (k,t)= e^{-\abs{k}^2t} \hat{u}_0(k) + \int_0^{t}  e^{-\abs{k}^2(t-\tau )} \hat{f}(k,\tau ) d \tau 
 .\end{align*}
 inverse transform of the Fourier transformation
 \begin{align*}
   u(x,t) =  \frac{1}{(4\pi t)^{\frac{d}{2}} }\int_{\mathbb{R}^{d} } e^{-\frac{\abs{x-y}^2}{4t}} u_0(y) dy + \int_0^{t} \int_{\mathbb{R}^{d} }   \frac{1}{(4\pi(t-\tau ))^{\frac{d}{2}} } e^{\frac{-\abs{x-y}^2}{4(t-\tau )}} f(y,\tau )dy d\tau 
 .\end{align*}
 Then we get $K$ as 
 \begin{definition}[Heat Kernel]
 The following is called Heat Kernel
 \begin{align*}
  K(x,t) = \frac{1}{(4\pi t)^{\frac{d}{2}} } e^{-\frac{\abs{x}^2}{4t}} 
 .\end{align*}
 \end{definition}
