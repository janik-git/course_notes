\chapter{MEAN FIELD LIMIT FOR SDE SYSTEM}
\section{Basic On Probability Theory}
This section is dedicated to a small review of basic concepts 
in probability theory in preparations of SDE's
\begin{definition}[$\sigma$-Algebra]
 Let $\Omega $  be a given set, then a $\sigma-$algebra $\mathcal{F}$ on $\Omega $ is a
 family of subsets of $\Omega $ s.t.
 \begin{enumerate}
   \item $\emptyset \in  \mathcal{F}$
   \item $F \in  \mathcal{F} \implies F^{c} \in  \mathcal{F} $
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ countable, then 
     \begin{align*}
       A = \bigcup_{j=1}^{\infty} A_j \in \mathcal{F}
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Measure Space]
 A tuple $(\Omega ,\mathcal{F})$  is called a measurable space. The elements of $\mathcal{F}$ are 
 called measurable sets 
\end{definition}
\begin{definition}[Probability Measure]
 A probability measure $\P$ on $(\Omega ,\mathcal{F})$  is a function 
 \begin{align*}
   \P \ : \ \mathcal{F} \to [0,1]
 .\end{align*}
 s.t.
 \begin{enumerate}
   \item $\P(\emptyset) = 0$ , $\P(\Omega ) = 1$
   \item If $A_{1},A_{2},\ldots \in \mathcal{F}$ s.t. $A_i \cap A_j = \emptyset \ \forall  i \neq j$  then
     \begin{align*}
       \P(\bigcup_{j=1}^{\infty} A_j ) = \sum_{j=1}^{\infty} \P(A_j) 
     .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Probability Space]
 The triple $(\Omega ,\mathcal{F},\P)$  is called a probability space. $F \in  \mathcal{F}$ is called
 event. We say the probability space $(\Omega ,\mathcal{F},\P)$ is complete, if $\mathcal{F}$ contains all zero-measure sets i.e.
 if 
 \begin{align*}
  \inf \{\P(F) \ : \ F \in  \mathcal{F},G \subset  F\}  = 0
 .\end{align*}
 then $G \in  \mathcal{F}$ and $\P(G) = 0$. Without loss of generality we use in this lecture $(\Omega ,\mathcal{F},\P)$
 as complete probability space
\end{definition}
\begin{definition}[Almost Surely]
  If for some $F \in  \mathcal{F}$ it holds $\P(F) = 1$ the we say that $F$ happens with 
  probability 1 or almost surely (a.s.)
\end{definition}
\begin{remark}
 Let $\mathcal{H}$  be a family of subsets of $\Omega$, then there exists a smallest $\sigma-$algebra of 
 $\Omega$ called $\mathcal{U}_{\mathcal{H}}$ with 
 \begin{align*}
   \mathcal{U}_{\mathcal{H}} = \bigcap_{\substack{\mathcal{H} \subset \mathcal{U} \\ \mathcal{H} \ \sigma-\text{alg.}}} \mathcal{H}  
 .\end{align*}
\end{remark}
\begin{example}
  The $\sigma-$algebra generated by a topology $\tau $ of $\Omega$ , $\mathcal{U}_{\tau } \triangleq \mathcal{B}$ is called 
  the Borel $\sigma-$algebra, the elements $B \in  \mathcal{B}$ are called Borel sets.
\end{example}
\begin{definition}[Measurable Functions]
 Let $(\Omega ,\mathcal{F},\P)$  be a probability space, a function 
 \begin{align*}
  Y \ : \ \Omega  \to \mathbb{R}^{d} 
 .\end{align*}
 is called measurable if and only if 
 \begin{align*}
  Y^{-1}(B) \in  \mathcal{F} 
 .\end{align*}
 holds for all $B \in  \mathcal{B}$ or equivalent for all $B \in  \tau $
\end{definition}
\begin{example}
 Let $X : \Omega  \to  \mathbb{R}^{d} $  be a given function, then the $\sigma-$algebra $\mathcal{U}(X)$ generated by X is 
 \begin{align*}
  \mathcal{U}(X) = \{X^{-1}(B) \ : \ B \in  \mathcal{B} \}  
 .\end{align*}
\end{example}
\begin{lemma}[Doob-Dynkin]
 If $X,Y \ : \ \Omega  \to \mathbb{R}^{d} $  are given then $Y$ is $\mathcal{U}(X)$ measurable if and only if 
 there exists a Boreal measurable function $g \ : \ \mathbb{R}^{d} \to  \mathbb{R}^{d}  $ such that 
 \begin{align*}
  Y = g(x)
 .\end{align*}
\end{lemma}
\begin{exercise}
  Proof the above lemma
\end{exercise}
From now on we denote $(\Omega ,\mathcal{F},\P)$ as a given probability space.
\begin{definition}[Random Variable]
 A random variable $X \ : \ \Omega  \to \mathbb{R}^{d} $  is a $\mathcal{F}-$measurable function.
 Every random variable induces a probability measure or $\mathbb{R}^{d} $ 
 \begin{align*}
  \mu_X(B) = \P(X^{-1}(B) ) \quad \forall B \in  \mathcal{B}
 .\end{align*}
This measure is called the distribution of X
\end{definition}
\begin{definition}[Expectation and Variance]
 Let $X$ be a random variable, if 
 \begin{align*}
   \int_{\Omega } \abs{X(\omega )}d\P(\omega ) < \infty
 .\end{align*}
 then 
 \begin{align*}
   \E[X] = \int_{\Omega } X(\omega ) d\P(\omega ) =  \int_{\mathbb{R}^{d} }x d\mu_X(x)
 .\end{align*}
 is called the expectation of $X$ (w.r.t. $\P$) \\[1ex]
 \begin{align*}
   \V[X] = \int_{\Omega } \abs{X - \E[X]}^2 d\P(\omega )
 .\end{align*}
 is called variance and there exists the simple relation 
 \begin{align*}
   \V[X] = \E[\abs{X-\E[X]}^2] = \E[\abs{X}^2] - \E[X]^2
 .\end{align*}
\end{definition}
\begin{remark}
 If $f : \mathbb{R}^{d} \to  \mathbb{R} $ measurable and 
 \begin{align*}
   \int_{\Omega } \abs{f(X(\omega ))} d\P(\Omega ) <\infty
 .\end{align*}
 then 
 \begin{align*}
   \E[f(x)] = \int_{\Omega }f(X(\omega ))d\P(\omega ) = \int_{\mathbb{R}^{d} }f(x) d\mu_X(x)
 .\end{align*}
\end{remark}
\begin{definition}[$L^p$ spaces]
  Let $X : \Omega  \to  \mathbb{R}^{d} $  be a random variable and $p \in [1,\infty)$.
  With the norm 
  \begin{align*}
    \|X\|_p = \|X\|_{L^{p}(\P ) } = \left( \int_{\Omega} \abs{X(\omega )}^{p} d\P(\omega )  \right)^{\frac{1}{p}} 
  .\end{align*}
  If $p=\infty$ 
  \begin{align*}
    \|X\|_{\infty} = \inf \{N \in  \mathbb{R} : \abs{X(\omega )} \le  N \text{ a.s.}\}  
  .\end{align*}
  the space $L^{p}(\P ) = L^{p}(\Omega ) = \{X \ : \ \Omega  \to  \mathbb{R}^{d}  \ | \ \|X\|_p \le \infty \}    $ is a Banach space.
\end{definition}
\begin{remark}
 If $p=2$ then $L^{2}(\P) $ is a Hilbert space with inner product 
 \begin{align*}
   \braket{X,Y} = \E[X(\omega )*Y(\Omega )] = \int_{\Omega }X(\omega )*Y(\omega )d\P(\omega )
 .\end{align*}
\end{remark}
\begin{definition}[Distribution Functions]
 Note for $x,y \in  \mathbb{R}^{d} $  we write $x\le y$ if $x_i \le  y_i$ for $\forall i$
 \begin{enumerate}
   \item $X: (\Omega ,\mathcal{F},\P) \to \mathbb{R}^{d} $ is a random variable the ints distribution function $F_x : \mathbb{R}^{d} \to [0,1] $
     is defined by 
     \begin{align*}
      F_X(x) = \P(X\le x) \quad x \in  \mathbb{R}^{d} 
     .\end{align*}
    \item If $X_{1},\ldots ,X_m : \Omega \to \mathbb{R}^{d} $ are random variables, their joint distribution function is
      \begin{align*}
        F_{X_{1},\ldots ,X_m} &: (\mathbb{R}^{d} )^m \to [0,1]\\
        F_{X_{1},\ldots ,X_M} &= \P(X_{1}\le x_{1},\ldots ,X_m\le x_m) \quad \forall x_i \in \mathbb{R}^{d} 
      .\end{align*}
 \end{enumerate}
\end{definition}
\begin{definition}[Density Function Of X]
 If there exists a non-negative function $f(x) \in  L^{1}(\mathbb{R}^{d} ; \mathbb{R} ) $   such that 
 \begin{align*}
   F(x) = \int_{-\infty}^{x_{1}}  \ldots \int_{-\infty}^{x_n} f(y) dy \quad y = (y_{1},\ldots ,y_n)
 .\end{align*}
 then f is called density function of $X$ and 
 \begin{align*}
  \P(X^{-1}(B) ) = \int_B f(x) dx \quad \forall  B \in  \mathcal{B}
 .\end{align*}
\end{definition}
\begin{example}
 Let $X$ be random variable with density function  $x \in  \mathbb{R}$
 \begin{align*}
  f(x) = \frac{1}{\sqrt{2\pi \sigma ^2}e^{-\frac{\abs{x-m}^2}{2\sigma ^2}}  }
 .\end{align*}
 then we say that $X$ has a Gaussian (or Normal) distribution with mean m and variance $\sigma^2$ and write
 \begin{align*}
  X \sim \mathcal{N}(m,\sigma^2)
 .\end{align*}
 Obviously 
 \begin{align*}
   \int_{\mathbb{R}} xf(x) dx = m \qquad \int_{\mathbb{R}}\abs{x-m}^2f(x) dx = \sigma^2
  .\end{align*}
\end{example}
\begin{definition}[Independent Events]
  Events $A_{1},\ldots ,A_{n} \in  \mathcal{F}$ are called independet if $\forall 1 \le k_{1} < \ldots  < k_m \le  n$ it holds 
  \begin{align*}
    \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
  .\end{align*}
\end{definition}
\begin{definition}[Independent $\sigma-$Algebra]
 Let $\mathcal{F}_j \subset  \mathcal{F}$   be $\sigma-$algebras for $j=1,2,\ldots $. Then we say $\mathcal{F}_j$ are independet if 
 for $\forall 1 \le k_{1}<k_{2}<\ldots <k_m$ and $\forall A_{k_j} \in  \mathcal{F}_{k_j}$ it holds
 \begin{align*}
  \P(A_{k_{1}}\cap A_{k_2} \cap \ldots \cap A_{k_m} )=\P(A_{k_{1}})\P(A_{k_{2}})\ldots \P(A_{k_m})
 .\end{align*}
\end{definition}
\begin{definition}[Independent Random Variables]
 We say random variables $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $  are independet if 
 for $\forall  B_{1},\ldots ,B_{m} \subset  \mathcal{B}$ in $\mathbb{R}^{d} $ it holds 
 \begin{align*}
   \P(X_{j_{1}}\in B_{j_{1}},\ldots, X_{j_k}\in B_{j_k} ) = \P(X_{j_{1}} \in  B_{j_{1}})\ldots \P(X_{j_k} \in  B_{j_k})
 .\end{align*}
 which is equivalent to proving that $\mathcal{U}(X_{1}),\ldots ,\mathcal{U}(X_k)$ are independet
\end{definition}
\begin{theorem}
 $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R}^{d} $ are independet if and only if 
 \begin{align*}
   F_{X_{1},\ldots ,X_m}(x_{1},\ldots ,x_m) =F_{X_{1}}(x_{1})\ldots F_{x_m}(x_m) \quad \forall  x_i \in \mathbb{R}^{d} 
 .\end{align*}
\end{theorem}
\begin{exercise}
 Proof the above theorem 
\end{exercise}
\begin{theorem}
  If $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independet and $\E[\abs{X_i}] < \infty$ then 
  \begin{align*}
    \E[\abs{X_{1},\ldots ,X_m}]<\infty
  .\end{align*}
  and 
  \begin{align*}
    \E[X_{1}\ldots X_m] = \E[X_{1}]\ldots \E[X_m]
  .\end{align*}
\end{theorem}
\begin{exercise}
 Proof the above theorem 
\end{exercise}
\begin{theorem}
  $X_{1},\ldots ,X_m \ : \ \Omega  \to \mathbb{R} $ are independet and $\V[X_i] <\infty$ then 
  \begin{align*}
    \V[X_{1} + \ldots  + X_m] = \V[X_{1}] + \ldots  + \V[X_m]
  .\end{align*}
\end{theorem}
\begin{exercise}
 Proof the above theorem 
\end{exercise}


