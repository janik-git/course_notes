\chapter{Mean Field Limits For Systems With Non Lipschitz Interaction Potential}
\section{Motivation}
In this chapter we study what happens for "less nice" interaction potentials, let us first 
remember how our Mean Field Particle System and the associated Mean Field Problem are defined.
\begin{Definition}[Mean Field problem]
 As $N\to \infty$  what happens to 
 \begin{align*}
\begin{cases}
  &\partial_t u - \Delta u + \nabla * (\nabla V \star  u * u) = 0\\
  &u(0) = u_{0}
\end{cases}
 .\end{align*}
\end{Definition}
\begin{Definition}
\begin{align*}
  (\text{SDE})\begin{cases}
    &dX_i^n = \nabla V \star  \mu_N(X_i^{N} ) dt + \sqrt{2}dW_t^i\\
    &X_i^N(0) = \xi_i \text{ i.i.d } u_{0}=\mathcal{L}(\xi_i)
  \end{cases}
.\end{align*}  
where $\mu_N$ is the empirical measure
\begin{align*}
  \mu_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i^N}
.\end{align*}
where $\nabla V$ is not Lipschitz continuous
\end{Definition}
To solve this there are two approaches, one of which is to first consider the mollified problem i.e 
replace $V$ with
\begin{align*}
  V_\epsilon = j_{\epsilon}\star V
.\end{align*}
and consider 
\begin{Definition}[Mollified SDE]
  For a mollificator $j_\epsilon$ we consider 
  \begin{align*}
  (\text{SDE}_\epsilon)\begin{cases}
    &dX_i^n = \nabla V_\epsilon \star  \mu_N(X_i^{N} ) dt + \sqrt{2}dW_t^i\\
    &X_i^N(0) = \xi_i \text{ i.i.d } u_{0}=\mathcal{L}(\xi_i)
  \end{cases}
.\end{align*}  
where 
\begin{align*}
  V_\epsilon = j_{\epsilon}\star V
.\end{align*}
\end{Definition}
now we can either consider a limit for fixed $\epsilon > 0$ take $N\to \infty$ and see what happens, this is a simpler case since we get that 
\begin{align*}
  \nabla V_\epsilon \star  u  \text{ is bounded }
.\end{align*}
\\[1ex]beta
Or the more complex case is to consider a combined limit $\epsilon(N) \xrightarrow{N\to \infty} 0$ we consider two possible epsilon
\begin{align*}
  \begin{cases}
    &\epsilon \sim (\frac{1}{\ln N})^{a} \\
    &\epsilon \sim N^{-\beta }  \quad \beta  > 0
  \end{cases}
.\end{align*}
The second approach is to consider a many particle PDE 
\begin{definition}[Many Particle PDE/High Dimensional PDE]
\begin{align*}
  \begin{cases}
    &dX_i^n = \frac{1}{N} \sum_{j=1}^{N}   \nabla V(X_i^{N}-X_j^{N}  )dt + \sqrt{2}dW_t^i 
  \end{cases}
.\end{align*}  
let $u^{N} (x_{1},x_{2},\ldots,x_N ) $ be the law of $(X_1^{N},X_2^{N},\ldots ,X_N^{N})$ then we get an equation for $u$ by 
\begin{align*}
  \text{(HPDE) }\partial_t u^{N}  - \sum_{i=1}^{N}  \Delta_{x_i} u + \nabla * (\frac{1}{N} \sum_{j=1}^{N} \nabla V(x_i-x_j) u^{N} ) = 0
.\end{align*}
As $N\to \infty$ we expect 
\begin{align*}
  \int_{\mathbb{R}^{(n-1)d} } u^{N} (x_{1},x_{2},\ldots,x_N ) dx_{2}\ldots dx_N \to u
.\end{align*}
Where $u$ is a solution to 
\begin{align*}
  \partial_t u - \Delta  u  + \nabla * (\nabla V \star u * u) = 0
.\end{align*}
\end{definition}
\begin{remark}
 We know that if the drift term in the above is bounded, then we get for fixed $N$ a solution i.e. if 
 \begin{align*}
  \nabla V < \infty
 .\end{align*}
\end{remark}
\begin{remark}
 The above approach is called relative entropy method, by considering the tensor product
 \begin{align*}
  u^{\otimes N}  = u(x_{1})u(x_{2})\ldots u(x_N)
 .\end{align*}
and the tensor product then satisfies 
\begin{align*}
  \partial_t u^{\otimes N}  -\sum_{i=1}^{N}  \Delta_{x_i}u^{\otimes N} + \nabla * (\nabla V \star  u * u^{\otimes N} )  = 0
.\end{align*}
then one needs to consider the difference between the (HPDE) and the above.
\end{remark}
\section{$\epsilon$-Problem Approach}
\begin{definition}[Mollified SDE]\label{eps_sde}
  For a mollifier  $j_\epsilon$ we consider 
  \begin{align*}
  (\text{SDE}_\epsilon)\begin{cases}
    &dX_i^{N,\epsilon} = \nabla V_\epsilon \star  \mu_N(X_i^{N,\epsilon} ) dt + \sqrt{2}dW_t^i\\
    &X_i^{N,\epsilon}(0) = \xi_i \text{ i.i.d } u_{0}=\mathcal{L}(\xi_i)
  \end{cases}
.\end{align*}  
where 
\begin{align*}
  V_\epsilon = j_{\epsilon}\star V
.\end{align*}
and 
\begin{align*}
  \mu_N = \frac{1}{N} \sum_{i=1}^{N} \delta_{X_i^{N,\epsilon}}
.\end{align*}
\end{definition}
\begin{definition}
 And our problem is to solve 
\begin{align*}
  \widehat{\text{(SDE)}}=\begin{cases}
   &d \hat{X}_i = \nabla V \star  u(\hat{X}_i )  dt + \sqrt{2}dW_t^i\\
   &\hat{X}_i = \xi_i \\
   &u = \mathcal{L}(\hat{X}_i )
  \end{cases} 
  \longleftarrow\begin{cases}
  &\partial_t u - \Delta  u+ \nabla* (\underbrace{\nabla V \star  u}_{\text{Lip}} u) = 0 \marginnote{If $V=\delta $ then $\nabla V \star  u = u$}\\
  &u \rvert_{t=0} = u_{0}
  \end{cases}
.\end{align*}
\end{definition}
We introduce the intermediate problem 
\begin{definition}
For fixed $\epsilon$ we already know that \autoref{eps_sde} converges against 
\begin{align*}
  \begin{cases}
  &\partial_t u^{\epsilon}  - \Delta  u^{\epsilon} + \nabla* (\nabla V_\epsilon \star  u^{\epsilon}  u^{\epsilon} ) = 0 \\
  &u^{\epsilon}  \rvert_{t=0} = u_{0}
  \end{cases}
.\end{align*}
with the corresponding SDE 
\begin{align*}  
  \overline{\text{(SDE)}}=\begin{cases}
   &d \overline{X}_i^{\epsilon} = \nabla V_{\epsilon} \star  u^{\epsilon} (\overline{X}^{\epsilon} _i )  dt + \sqrt{2}dW_t^i\\
   &\overline{X}^{\epsilon} _i = \xi_i \\
   &u_{0} = \mathcal{L}(\xi_i )
  \end{cases}
.\end{align*}
\end{definition}
Now we choose 
\begin{align*}
  \epsilon(N) \xrightarrow{N\to \infty} 0   \quad &\text{SDE}_\epsilon - \overline{\text{SDE}} \to 0 \\ 
                                                  &\overline{\text{SDE}} - \widehat{\text{SDE}} \to 0
.\end{align*}
For that end we compute 
\begin{align*}
  \abs{X_i^{N,\epsilon}(t) - \overline{X}_i^{\epsilon}(t)   }^2 &=  \abs*{\int_0^{t} \nabla V_\epsilon \star  \mu_N(X_{i}^{N,\epsilon}(s) ) - \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}_i^{\epsilon}(s)  )  ds }^2\\
                                                                &\le t \int_0^{t} \abs{\nabla V_\epsilon \star  \mu_N(X_{i}^{N,\epsilon}(s) ) - \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}_i^{\epsilon}(s)  ) }^2 ds \\
                                                                &\le t \int_0^{t} \abs{\nabla V_{\epsilon} \star  \mu_N(X_i^{N,\epsilon}(s))-\nabla V_{\epsilon} \star \mu_N(\overline{X}_i^{\epsilon}(s)  )  }^2 ds\\
                                                                &+ t \int_0^{t} \abs{\nabla V_\epsilon \star  \mu_N(\overline{X}_i^{\epsilon}(s)) - \nabla V_{\epsilon} \star  \overline{\mu }_N(\overline{X}_i^{\epsilon}(s)  ) }^2 ds \\
                                                                &+ t \int_0^{t} \abs{\nabla V_\epsilon \star  \overline{\mu}_N(\overline{X}_i^{\epsilon}(s)) - \nabla V_{\epsilon} \star  u_{\epsilon}(\overline{X}_i^{\epsilon}(s)   ) }^2 ds \\
                                                                &\le t* \|\nabla^2 V_\epsilon \star  \mu_N\|_{\infty}  \int_0^{\infty}  \abs{X_i^{N,\epsilon}(s)-\overline{X}_i^{\epsilon}(s)   }^2 ds\\
                                                                &+ t \int_0^{t} \|\nabla ^2 V_\epsilon\|^2_{\infty} (\frac{1}{N} \sum_{j=1}^{N} \abs{X_j^{N,\epsilon}(s) - \overline{X}^{\epsilon}_j(s)   })^2 ds + III
.\end{align*}
\begin{exercise}
 Show that the $III$ term above can be bounded as follows
 \begin{align*}
  III \le  C(T)\frac{1}{N}\|\nabla V_\epsilon\|_{\infty}^2
 .\end{align*}
\textit{Hint:} Law of Large numbers
\end{exercise}
where 
\begin{align*}
  \overline{\mu}_N = \frac{1}{N} \sum_{i=1}^{N}   \delta_{\overline{X}_i^{\epsilon}  }
.\end{align*}
\begin{align*}
  \abs{\nabla V_\epsilon \star  \mu_N(\overline{X}_i^{\epsilon}(s)) - \nabla V_{\epsilon} \star  \overline{\mu }_N(\overline{X}_i^{\epsilon}(s)  ) } = \frac{1}{N}\sum_{j=1}^{N} \nabla V_\epsilon(\overline{X}_i^{\epsilon}(s) - X_j^{N,\epsilon}(s)   )  - \frac{1}{N}\sum_{j=1}^{N} \nabla V_{\epsilon}(\overline{X}_i^{\epsilon}(s)-\overline{X}_j^{\epsilon}(s)    ) 
.\end{align*}
Taking the expectation yields 
\begin{align*}
  \max_{1 \le i \le N}\E[\abs{X_i^{N,\epsilon}(t) - \overline{X}_{i}^{\epsilon}(t)}^2  ]\le \|D^2 V_{\epsilon}\|_{\infty}^2 T \int_0^{t}  \max_{1\le j\le N} \E[\abs{X_j^{N,\epsilon}(s) - \overline{X}_j^{\epsilon}(s)  }^2] ds + C(T)\frac{1}{N}\|\nabla V_\epsilon\|_{\infty}^2
.\end{align*}
by GrÃ¶nwall we can obtain 
\begin{align*}
  \max_{1 \le i \le N}\E[\abs{X_i^{N,\epsilon}(t) - \overline{X}_{i}^{\epsilon}(t)}^2  ] &\le e^{C_{1} * \|D^2 V_{\epsilon}\|_{\infty}^2 t+1} *\frac{C_{2}}{N}\|\nabla V_\epsilon\|^2_{\infty}
.\end{align*}
\textbf{Goal} choose $\epsilon(N)$ s.t. the above quantity converges to zero, remember 
\begin{align*}
  V_\epsilon = \int_{\mathbb{R}^{d} }\frac{1}{\epsilon^d}j(\frac{x-y}{\epsilon})V(y) dy
.\end{align*}
\begin{example}
 Two examples for $V$ are 
 \begin{enumerate}
   \item $V= \delta_0$
    \item $V = \frac{1}{\abs{x}^{d-2} }$
 \end{enumerate}
\end{example}
\begin{remark}
  We then get  for 1.
 \begin{align*}
  &\|D^2 V_{\epsilon}\|_{\infty}^2 \le  \frac{C}{\epsilon^{d+2}}\\
  &\|\nabla V_{\epsilon}\|_{\infty}^2 \le  \frac{C}{\epsilon ^{(d+1)2} }
 .\end{align*}
 and  for $ \forall \eta  \in  (0,1)$ choose $\epsilon$ s.t.
 \begin{align*}
   e^{\frac{1}{\epsilon ^{2(d+2)} }} &= N^{\eta} \\
   \epsilon &= (\frac{1}{\eta  \ln  N})^{\frac{1}{2(d+2)}}  
 .\end{align*}
 then 
 \begin{align*}
  J \le  e^{\frac{C}{\epsilon ^{(d+1)2} }} \frac{\frac{1}{\epsilon ^{(d+1)2} }}{N}  \le \frac{C}{N^{1-\eta } }
 .\end{align*}
\end{remark}
\begin{exercise}
 For the second example we have, $V_\epsilon(x) = \frac{1}{\epsilon}^{d} \int  j(\frac{x-y}{\epsilon})\frac{1}{\abs{y}^{d-2} } dy $,
 proof that the following two bounds holds
 \begin{align*}
   \|D^2 V_{\epsilon}\|_{\infty}&\le C*\epsilon^{-d}\\
   \|D V_{\epsilon}\|_{\infty}&\le C*\epsilon ^{-(d-1)}  \\
   \|V_{\epsilon}\|_{\infty}&\le C*\epsilon ^{-(d-2)}  \\
 .\end{align*}
 \textit{Hint:} split the domain $\mathbb{R}^{d} = \{y | \abs{y} \le 1\}  \cup \{y | \abs{y} >1\}  ] $
\end{exercise}
Now for reasonably chosen $\epsilon \sim (\ln N)^{-\alpha } $ one can prove that 
\begin{align*}
  \max_{1\le 1\le N} \E[\abs{X_i^{N,\epsilon} -\overline{X}^{\epsilon}_i  }^2]\le \frac{C}{N^{1-\eta } } \to  0
.\end{align*}
This concludes the first step
\begin{align*}
  \epsilon(N) \xrightarrow{N\to \infty} 0   \quad &\text{SDE}_\epsilon - \overline{\text{SDE}} \to 0 
.\end{align*}
For the second step 
\begin{align*}
  \abs{\overline{X}_i^{\epsilon}(t) - \hat{X}_i(t)    }^2 &= \abs*{\int_0^{t} \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}^{\epsilon} (s))- \nabla V  \star  u(\hat{X}(s)  ) ds }^2\\
                                                                     &\le t \int_0^{t} \abs{\nabla V_\epsilon \star  u^{\epsilon}(\overline{X}^{\epsilon}(s)  ) - \nabla V_{\epsilon} \star  u(\overline{X}^{\epsilon}(s)  ) }^2    \\
                                                                     &+ \abs{\nabla V_\epsilon \star  u(\overline{X}^{\epsilon}(s)  ) - \nabla V_{\epsilon} \star  u(\hat{X}(s)  )}^2 \\
                                                                     &+ \abs{\nabla V_\epsilon \star u(\hat{X}^{\epsilon}(s)  ) - \nabla V \star  u(\hat{X}(s)  )}^2 ds\\
                                                                     &= \int_0^{t}  I + II + III dt \\
                                                                     &\le C \underbrace{\|\nabla V_\epsilon \star (u^{\epsilon}-u )\|^2_{\infty}}_{\le  \|\nabla(u^{\epsilon}-u )\|_{\infty}^{2} } +C\underbrace{\|D^2 V_\epsilon \star  u\|}_{\le \|V \star  D^2u\|_{\infty}^2}  \int_0^{t} \abs{\overline{X}_i^{\epsilon}(s) - \hat{X}_i(s) }^2 ds + \|(V_\epsilon - V) \star  \nabla u\|_{\infty}^2\\
                                                                     &\le  \|V\star D^2u\|_{\infty}^2 \int_0^{t} \abs{\overline{X}_i^{\epsilon}(s)-\hat{X}_i(s)   }^2 ds + \|V \star (u^{\epsilon}-u )\|_{\infty}^2 + \|(V_{\epsilon}-V) \star  \nabla u\|_{\infty}^2
.\end{align*}
Then GrÃ¶nwall 
\begin{align*}
  \E[\abs{\overline{X}_i^{\epsilon}(t) - \hat{X}_i(t)    }^2] &\le  e^{\|V \star  D^2u\|_{\infty}^2t} \left( \|V \star (u^{\epsilon}-u )\|_{\infty}^2 + \|(V_{\epsilon}-V) \star  \nabla u\|_{\infty}^2 \right) \\
                                                              &=e^{\|V \star  D^2u\|_{\infty}^2t} (II + III)
.\end{align*}
we get for 
\begin{align*}
  II \le  C*\epsilon ^{2}  \quad  \text{ PDE estimates}
.\end{align*}
and 
\begin{align*}
  III &\le\bigg[\frac{1}{\epsilon ^{d} }  \int \abs*{j(\frac{x-y}{\epsilon})\left((V\star \nabla u)(y) -  (V \star  \nabla u)(x)\right)} dy\bigg]^2\\
      &\le \bigg[ \|V \star  D^2u\|_{\infty} \epsilon * \frac{1}{\epsilon ^{d} } \int  j(\frac{x-y}{\epsilon})*\frac{\|x-y\|}{\epsilon} dy \bigg ]^2
.\end{align*}
As a summary:  we need  the following PDE estimates
\begin{align*}
  \| V \star  D^2u\|_{\infty} < \infty, \|V \star (u^{\epsilon}- u )\|_{\infty} \xrightarrow{\epsilon\to 0} 0
.\end{align*}
All together this gives us 
\begin{align*}
  I &\coloneqq \max_{i} \E[\abs{X_i^{N,\epsilon} -\overline{X}_i^{\epsilon}  }^2] \le \ldots \\
  II &\coloneqq \max_{i} \E[\abs{\overline{X}_i^{\epsilon}-\hat{X}_i   }^2] \le \ldots \\
.\end{align*}
What we now want is 
\begin{align*}
  \max_i \E[\abs{X_i^{N,\epsilon} -\hat{X}_i }] \le  I +II \to 0
.\end{align*}
\textcolor{Red}{I think above only holds for $\delta $ case , so it should be grouped up appropriately, Formulate a Theorem for it}
\begin{exercise}
 Do the $\log$ scaling by repeating  the proof with this $ W_{2,\mathcal{C}^{d} }$
\end{exercise}
\section{Part 2}
We consider the same problem setting but instead of $\log$ scaling ($\epsilon = \ln(N)^{-alp} $) we instead consider $\epsilon = N^{-\beta } $ for $\beta  \in  (0,\frac{1}{2})$ to be determined
\begin{remark}
  For $\epsilon \sim (\frac{1}{\ln(n)})^{C(d)} $ we got the following result
 \begin{align*}
   \sup_{t,i} \E[\abs{X^{N,\epsilon}_i(t) - \overline{X}^{N}_i(t)} ] \le  \frac{C}{N^{\eta } } \text{ for some } \eta  \in (0,1)
 .\end{align*} 
\end{remark}
\begin{remark}
 Since the growth of $\ln $  is so slow, we can get such a strong result, for the faster growing $N^{-\beta } $ we expect a weaker convergence  
 \begin{align*}
   \sup_{t} \P(\max_{i} \abs{X^{N,\epsilon}_i(t) - \overline{X}_i^{N}(t)   } \ge N^{-\alpha } ) \le \frac{C(\gamma )}{N^{\gamma } } \quad \forall  \gamma  > 0
 .\end{align*}
\end{remark}
\begin{theorem}[$\epsilon = N^{-\beta}$ Convergence]
  For $\epsilon = N^{-\beta }, \beta  \in  (0,\beta_1) $, and   $\forall  \gamma  > 0$ , $0 < \alpha  < \frac{1}{2}$, if $\|u_\epsilon\|_{L^{1}} + \|u_\epsilon\|_{L^{\infty} } \le  C $ 
 \begin{align*}
   \sup_{t} \P(\max_{i} \abs{X^{N,\epsilon}_i(t) - \overline{X}_i^{N}(t)   } \ge N^{-\alpha } ) \le \frac{C(\gamma )}{N^{\gamma } } 
 .\end{align*}
\end{theorem}
\begin{proof}
  First remember that both $X_i^{N,\epsilon} $ and $\overline{X}^{N}_i  $ are solutions to SDE's, they are continuous (in fact they are up to $\frac{1}{2}$ HÃ¶lder continuous) \\[1ex]
 We define first a stopping time 
 \begin{align*}
   \tau(\omega )  = \inf  \{t \in  (0,T] : \max_{i} \abs{X^{N,\epsilon}_i(t) - \overline{X}_i^{N}(t)} \ge  N^{-\alpha}  \}   \land T 
 .\end{align*}
Then using the stopping time we define , the cut-off process 
\begin{align*}
  S(\omega,t) =  N^{2\alpha k } \max_{i} \abs{(X^{N,\epsilon}_i - \overline{X}_i^{N})(t \land \tau(w))}^{2k}  \le  1
.\end{align*}
 We have by Markovs inequality  
 \begin{align*}
   \sup_{0 < t \le  T} \P( \max_{i} \abs{X^{N,\epsilon}_i(t) - \overline{X}_i^{N}(t)   } \ge N^{-\alpha } ) &\le  \sup_{0 < t \le T} \P(S_t \equiv 1)\\
                                                                                                            &\myS{Mrkv.}{\le } \sup_{0 < t \le T} \E[S_t] \\
 .\end{align*}
 First note that
\begin{align*}
  X_i^{N,\epsilon} (t \land \tau ) - X_i^{N,\epsilon} (0) &= \int_0^{t \land \tau }  \nabla V_{\epsilon} \star  \mu_N(X_i^{N,\epsilon}(s) ) ds + \int_0^{t \land \tau }  \sqrt{2} dW_s^{i}  \\
  \overline{X}_i^{N} (t \land \tau ) - \overline{X}_i^{N} (0) &= \int_0^{t \land \tau }  \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}_i^{N}(s) ) ds + \int_0^{t \land \tau }  \sqrt{2} dW_s^{i}  
.\end{align*}
Then 
\begin{align*}
  \E[S(t)] &= N^{2\alpha k } \E[\max_{i} \abs{(X^{N,\epsilon}_i - \overline{X}_i^{N})(t \land \tau(w))}^{2k}] \\
           &\le  N^{2 \alpha  k}  \E\left[ \max_{i}\abs*{\int_0^{t \land \tau }  \nabla V_{\epsilon} \star  \mu_N(X_i^{N,\epsilon}(s) ) -  \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}_i^{N}(s) )ds}^{2k}  \right]\\
           &\myS{HÃ¶ld.}{\le} N^{2 \alpha  k} C(t,k)  \E\left[ \max_{i}\int_0^{t \land \tau }  \abs*{\nabla V_{\epsilon} \star  \mu_N(X_i^{N,\epsilon}(s) ) -  \nabla V_{\epsilon} \star  u^{\epsilon}(\overline{X}_i^{N}(s) ) }^{2k} ds  \right]\marginnote{Add smart 0 $\pm \nabla V_\epsilon \star  \overline{\mu }_N $ }\\
           &= N^{2 \alpha  k}  C(t,k)  \E\left[ \max_{i}\int_0^{t \land \tau }  \abs*{\frac{1}{N} \sum_{j} \nabla V_{\epsilon}(X_i^{N,\epsilon}(s) - X_{j}(s)^{N,\epsilon}  ) - \frac{1}{N} \sum_{j} \nabla V_{\epsilon}(\overline{X}_i^{N} - \overline{X}_j^{N} ) }^{2k} ds  \right]\\
           &+ N^{2 \alpha  k} C(t,K) \E\left[ \max_{i}\int_0^{t \land \tau }  \abs*{\frac{1}{N} \sum_{j} \nabla V_{\epsilon}(\overline{X}_i^{N}(s) - \overline{X}_{j}(s)^{N}  ) - \nabla V_{\epsilon} \star  u_{\epsilon}(\overline{X}_i^{N}(s) )}^{2k} ds  \right]\\
           &= I + II
.\end{align*}
we bound individually , \textcolor{Red}{fix the superscript position}
\begin{align*}
  I  &\le   N^{2 \alpha  k } C(t,k)   \E\left[ \max_{i}\int_0^{t \land \tau }  \abs*{\frac{1}{N} \sum_{j} D^{2} V_{\epsilon}(\overline{X}_i^{N} (s) - \overline{X}_j^{N}(s)   )*(X_i^{N,\epsilon} -\overline{X}_i^{N} - (X_j^{N,\epsilon} - \overline{X}_j^{N}   )  ) }^{2k}  ds  \right]\\ 
     &\qquad + N^{2 \alpha  k}   C(t,k)   \E\left[ \max_{i}\int_0^{t \land \tau }\abs*{ \|D^{3} V_{\epsilon} \|_{\infty} \frac{1}{N} \sum_{j} \abs{X_i^{N,\epsilon} -\overline{X}_i^{N} - (X_j^{N,\epsilon} - \overline{X}_j^{N}   )  }^2}^{2k}  ds \right]  \\
     &= I_1 +  I_2
.\end{align*}
where 
\begin{align*}
  I_2 \le  \frac{C(t,k)}{N^{2 \alpha  k} } \underbrace{\|D^{3}V_{\epsilon} \|_{\infty}^{2k}}_{\frac{1}{\epsilon ^{(d+3)2k} }}  \int_0^{t} \E[S(s)] ds \marginnote{\footnotesize   $\epsilon = \frac{1}{N^{\beta } }$}
.\end{align*}
For $I_{1}$ we get 
\begin{align*}
  I_{1} &\le  N^{2 \alpha  k}  C(t,k) \E\left[\max_{i} \int_0^{t \land \tau } \abs*{\frac{1}{N} \sum_{j=1}^{N} D^2V_{\epsilon}(\overline{X}_i^{N} - \overline{X}_j^{N}    )(X_i^{N,\epsilon}-\overline{X}_i^{N}  - (X_j^{N,\epsilon} - \overline{X}_j^{N}  ) ) }^{2k} ds \right]\\
        &\le  N^{2 \alpha  k } C(t,k)  \E\left[\max_{i} \int_0^{t \land \tau } \frac{1}{N} \sum_{j=1}^{N} \abs*{D^2V_{\epsilon}(\overline{X}_i^{N} - \overline{X}_j^{N})}^{2k}  2\max_i \abs{(X_i^{N,\epsilon}-\overline{X}_i^{N}  }^{2k} ds \right]\\
        &\le C(t,k)  \E \left[ \max_i \int_0^{t \land \tau } (D^{2}V_{\epsilon} \star  u^{\epsilon})(\overline{X}_i^{N}  )^{2k}   S(s) ds \right] \marginnote{\footnotesize By adding in the one term we can use the regularity of the SDE solution $u^{\epsilon} $ to bound} \\
        &\qquad + 2 C(t,k) \E\left[\max_{i} \int_0^{t \land \tau } \left( \frac{1}{N} \sum_{j=1}^{N} \abs{D^2 V_\epsilon}(\overline{X}_i^{N} - \overline{X}_{j}^{N}   ) - \abs{D^{2}V_\epsilon } \star  u^{\epsilon}(\overline{X}_i^{N}  )  \right)^{2k} S(s) ds \right] \\
        &\le  2 C(t,k) \|\abs{D^2 V_\epsilon} \star  u ^{\epsilon} \|_{\infty}^{2k}  \int_0^{t} \E[(S(s))]  ds \\
        &\qquad + \underbrace{2 C(t,k) \E\left[\max_{i} \int_0^{t \land \tau } \left( \frac{1}{N} \sum_{j=1}^{N} \abs{D^2 V_\epsilon}(\overline{X}_i^{N} - \overline{X}_{j}^{N}   ) - \abs{D^{2}V_\epsilon } \star  u^{\epsilon}(\overline{X}_i^{N}  )  \right)^{2k} S(s) ds \right]}_{I_{12}} \\
.\end{align*}
Using \autoref{version_of_lln} with $U = \abs{D^2V_{\epsilon}}$ and $v = u^{\epsilon}(*,s) $, define 
\begin{align*}
  A_{\theta }(\abs{D^2 V_{\epsilon}},u^{\epsilon} )= \bigcup_{i=1}^{N}  A^{i}_{\theta } \implies A_{\theta }^{c} (\abs{D^2 V_{\epsilon}},u^{\epsilon} ) = \bigcap_{i=1}^{N} (A_{\theta }^{i} )^{C} 
.\end{align*}
\begin{align*}
  I_{12} &=  2 C(t,k) \int_0^{t}  \E\left[\max_{i} \left( \frac{1}{N} \sum_{j=1}^{N} \abs{D^2 V_\epsilon}(\overline{X}_i^{N} - \overline{X}_{j}^{N}   ) - \abs{D^{2}V_\epsilon } \star  u^{\epsilon}(\overline{X}_i^{N}  )  \right)^{2k} S(s) ds \right]\\
        &=  2 C(t,k) \int_0^{t}  \E\left[(\cha_{A_\theta} + \cha_{A^{C}_{\theta} })\max_{i} \left( \frac{1}{N} \sum_{j=1}^{N} \abs{D^2 V_\epsilon}(\overline{X}_i^{N} - \overline{X}_{j}^{N}   ) - \abs{D^{2}V_\epsilon } \star  u^{\epsilon}(\overline{X}_i^{N}  )  \right)^{2k} S(s) ds \right]\\
        &\le  2 C(t,k) \frac{1}{N^{2\theta \tilde{k}  } }\int_0^{t}   \E[S(s)] ds  + 2 \|D^2 V_\epsilon\|_{\infty}^{2 k }  \P(A_{\theta }^{c} )\\
        &\le 2 C(t,k) \int_0^{t}   \E[S(s)] ds  + 2 \|D^2 V_\epsilon\|_{\infty}^{2 k }*\|D^2 V_{\epsilon}\|_{\infty}^{2 \tilde{k} } C(\tilde{k} ) N^{2 \tilde{k}(\theta -\frac{1}{2})+1 }  \\
.\end{align*}
For $II$ we again apply \autoref{version_of_lln} 
\begin{align*}
  A_{\theta_{1} }(\nabla V_{\epsilon},u_\epsilon) =\bigcup_{i=1}^{N} A_{\theta_1}^{i}  = \bigcup_{i=1}^{N}  \{\omega  \in  \Omega  : \abs{\ldots } \ge  \frac{1}{N^{\theta_1} }\}  
.\end{align*}
By inserting the indicator
\begin{align*}
  II &= N^{2 \alpha  k} C(t,K) \E\left[(\cha_{A_{\theta_{1}}^{c} } + \cha_{A_{\theta_{1}}}) \max_{i}\int_0^{t \land \tau }  \abs*{\frac{1}{N} \sum_{j} \nabla V_{\epsilon}(\overline{X}_i^{N}(s) - \overline{X}_{j}(s)^{N}  ) - \nabla V_{\epsilon} \star  u_{\epsilon}(\overline{X}_i^{N}(s) )}^{2k} ds  \right]\\
     &\le  C(t,k)N^{2 \alpha  k}  \left[ (\frac{1}{N^{\theta_1} })^{2k} + \|\nabla V_{\epsilon}\|_{\infty}^{2k} \P(A_{\theta_1})   \right]  \\
     &\le C(t,k)(N^{(\alpha -\theta_1)2k} + N^{2 \alpha  k}  \|\nabla V_{\epsilon}\|_{\infty}^{2k}  \|\nabla V_\epsilon\|^{2 \tilde{k} } C(\tilde{k} )N^{2 \tilde{k}(\theta_1 - \frac{1}{2}) + 1 }) 
.\end{align*}
We can put everything together now 
\begin{align*}
  \E[S(t)] &\le  C(1 + N^{-2 \alpha  k} \|D^{3}V_\epsilon \|_{\infty}^{2k}   + \|D^2 V_{\epsilon} \star  u^{\epsilon} \|_{\infty}^{2k} ) \int_0^{t} \E[S(s)] ds\\
           &+ C(t,k)N^{(\alpha  - \theta_1)2k} +  C(t,k)N^{2 \alpha  k + 2 \tilde{k}(\theta_1 - \frac{1}{2}) + 1 } \|\nabla V_{\epsilon}\|_{\infty}^{2(k+\tilde{k} )} \\
           &+ C(t,k)N^{2 \tilde{k}(\theta  - \frac{1}{2}) + 1 } \|D^2V_{\epsilon}\|_{\infty}^{2(k+\tilde{k} )}  
.\end{align*}
Remember that $\epsilon = N^{-\beta } $ and $\|\abs{D^2 V_\epsilon} \star  u^{\epsilon}  \|_{\infty}^{2k} \le C $ , an example for $V$ includes 
\begin{align*}
  V  &= \frac{1}{\abs{x}^{\eta }  }   ,\quad 0 < \eta  < d-2\\
  \abs{D^2 V_\epsilon} &= \abs{j_{\epsilon} \star  D^2V} = \abs*{\int j_{\epsilon}(x-y) D^2 V(y)} dy  \\
                       &\le  j_\epsilon \star \abs{D^2 V} \\
.\end{align*}
\marginnote{\footnotesize The case that $\eta  = d-2$ needs to be handled differently, since then our potential twice differentiated is not integrable anymore (at the origin) and we need an  extra cutoff}
Then 
\begin{align*}
  \abs{D^2 V_{\epsilon}} \star  u^{\epsilon}  &= \int \abs{D^2 V_{\epsilon}(y)} u^{\epsilon}(x-y)dy\\
                                              &\le  \int  j_{\epsilon} \star  \abs{D^2 V}(y) u^{\epsilon}(x-y)dy \\
                                              &\le  \int  \abs{D^2V}(y)u^{\epsilon}(x-y)dy \\
                                              &\le  \int \frac{1}{\abs{y}^{\eta  + 2} } u^{\epsilon}(x-y)dy\\
                                              &\le  \int_{\abs{y} \le 1} \ldots  + \int_{\abs{y}\ge 1} \\
                                              &\le C\|u^{\epsilon} \|_{\infty}+\|u^{\epsilon} \|_{L^{1} } \marginnote{$2 < \eta  < d$}
.\end{align*}
Where for $V \sim \frac{1}{\abs{x}^{\eta } }$
\begin{align*}
  \|\nabla V_\epsilon\|_{\infty} &\le  \frac{1}{\epsilon ^{C_{1}(\eta ,d)} } = N^{\beta C(\eta ,d)} \\
  \|D^2 V_\epsilon\|_{\infty} &\le  \frac{1}{\epsilon ^{C_{1}(\eta ,d)} }
.\end{align*}
\end{proof}
\begin{remark}[Video Discussion Paper]
 In the relevant Paper   they assumed 
 \begin{align*}
   \abs{\nabla V_{\epsilon}(x) - \nabla V_{\epsilon}(y)} \le  C \abs{D^{2} V_{\epsilon}(y) }\abs{x-y}
 .\end{align*}
\end{remark}
\begin{lemma}[A version of l.l.n]\label{version_of_lln}
  Suppose $(\overline{Y}_i )_{i \le N}$ is a collection of i.i.d random variables with law $\mathcal{L}(\overline{Y}_0) = v \in  L^{1}(\mathbb{R}^{d} ) $,
  and $U \in  L^{\infty} $ be a given function, then 
  \begin{align*}
    A_{\theta }^{i} (U,v) &= \{w \in  \Omega  : \abs{\frac{1}{N} \sum_{j} U(\overline{Y}_i - \overline{Y}_j  )- U \star  v (\overline{Y}_i )} \ge \frac{1}{N^{\theta} }\}  \\
    A_{\theta }^{N} &= \bigcup_{i=1}^{N}   A_{\theta }^{i} (U,v)
  .\end{align*}
  then $\forall  \tilde{k} \in  \mathbb{N} $. $\theta \in  (0,\frac{1}{2})$ it holds 
  \begin{align*}
    \P(A_{\theta }^{N}(U,v) ) &\le N \max_{1\le i \le  N} \P(A_{\theta }^{i}(U,v) )\\
                              &\le N * N^{2 \tilde{k}(\theta  - \frac{1}{2}) }  C(\tilde{k} ) \|U\|_{L^{\infty} }^{2 \tilde{k} } 
  .\end{align*}
\end{lemma}
\begin{proof}
 Markovs inequality   implies that 
 \begin{align*}
   \P(A_{\theta }^{i}(U,v) )&\le  N^{2 \tilde{k} \theta }  \E\left[\abs{\frac{1}{N} \sum_{j} U(\overline{Y}_i - \overline{Y}_j) - U \star  v (\overline{Y}_i ) }^{2 \tilde{k} } \right]\\
                            &=  N^{2 \tilde{k} \theta }  \E\left[\left(\frac{1}{N^2} \sum_{j,k=1}^{N}  h(\overline{Y}_i,\overline{Y}_j  )h(\overline{Y}_i,\overline{Y}_k  ) \right)^{\tilde{k} } \right]\\
 .\end{align*}
 where 
 \begin{align*}
  h(\overline{Y}_i,\overline{Y}_j) =    U(\overline{Y}_i - \overline{Y}_j) - U \star  v (\overline{Y}_i)
 .\end{align*}
 In the terms where $j$ appeared once. 
 \begin{align*}
   \E[h(\overline{Y}_i,\overline{Y}_j  ) \prod_{\substack{m=1\\ m\neq j}}^{2 \tilde{k}-1 } h(\overline{Y}_i , \overline{Y}_{l_m}  )] 
 .\end{align*}
 We have 
 \begin{align*}
    = \int dx v(x) \int  dy v(y)h(x,y) \E[\prod_{\substack{m=1\\ m\neq j}}^{2 \tilde{k}-1 } h(x, \overline{Y}_{l_m} )] &= 0 
 .\end{align*}
 since  for fixed $x \in  \mathbb{R}^{d} $
 \begin{align*}
   \int dy v(y)h(x,y) =  \int dy v(y) (U(x,y) - U \star  v(x)) = 0
 .\end{align*}
 The number of the non vanishing terms.
 \begin{align*}
   \mathcal{N} \coloneqq  \{\prod_{j=1}^{2 \tilde{k} }h(\overline{Y}_i,\overline{Y}_{i_j}) : i_j \in  \{1 , \ldots , N\} \text{ s.t. } i_j \text{ appeared at least twice}   \}  
 .\end{align*}
 It is easy to obtain that the number is bounded by $C(\tilde{k} )N^{\tilde{k} } $\\[1ex]
 On the other hand, the non vanishing terms 
 \begin{align*}
   \E\left[\prod_{j=1}^{2 \tilde{k} }h(\overline{Y}_i,\overline{Y}_{i_j})\right] \le  C(\tilde{k} )* \|U\|_{\infty}^{2 \tilde{k} } 
 .\end{align*}
 We get that 
 \begin{align*}
   \P(A_{\theta }^{i}(U,v) ) \le N^{2 \tilde{k}\theta  } \frac{C(\tilde{k} )}{N^{2 \tilde{k} } } * N^{\tilde{k} } \|U\|_{\infty}^{2 \tilde{k} }  = C(\tilde{k}) N^{2 \tilde{k}(\theta  - \frac{1}{2}) } 
 .\end{align*}
 Summing up the terms, completes the proof
\end{proof}
\begin{corollary}
 Combined with the result 
 \begin{align*}
   \E[\max_i \abs{\overline{X}_i^{N}- \hat{X}_i^{N}  }]\le C*\epsilon
 .\end{align*}
 or. 
 \begin{align*}
   \sup_{t} \P(\max_{i} \abs{\overline{X}_i^{N} - \hat{X}_i^{N}    } > \epsilon ^{\frac{1}{2}} ) \le  \epsilon ^{-\frac{1}{2}}  \sup_{t} \E[\max_i \abs{\overline{X}_i^{N}- \hat{X}_i^{N}  }] \le  C \epsilon ^{\frac{1}{2}} 
 .\end{align*}
 and obtain 
 \begin{align*}
   &\sup_t \P(\max_{i} \abs{X_{i}^{N,\epsilon} - \hat{X}_i^{N} } > \epsilon ^{\frac{1}{2}} )\\
   &\le  \sup_{0 \le  t \le  T} \P(\max_{i} \abs{X_{i}^{N,\epsilon} - \overline{X}_i^{N} } > \epsilon ^{\frac{1}{2}} ) + \max_{i} \abs{\overline{X}_{i}^{N} - \hat{X}_i^{N} } > \epsilon ^{\frac{1}{2}} \\
   &\le \frac{C}{N^{\nu } } + C*\epsilon ^{\frac{1}{2}} 
 .\end{align*}
\end{corollary}
  
