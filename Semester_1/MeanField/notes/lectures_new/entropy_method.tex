\chapter{Relative Entropy Method}
In this chapter, we introduce the newly developed method from PDE point of view in the study of mean field limit. Relative  entropy has been long investigated in the PDE theory to study the long time behaviour or singular limits of solutions, it has been introduced in the multi particle setting to handle non-Lipschitz interaction forces only recently. To describe the main idea, we review the $N$-particle system
\begin{align}\label{SDE}
  (\text{SDE})\begin{cases}
   dX_i^N(t) &= \frac{1}{N}\sum_{j=1}^{N} \nabla V (X_i^{N}(t)-X_j^{N}(t)  )  dt + \sqrt{2}dW_t^i\\
    X_i^N(0) & \text{ are given i.i.d random variables with law } u_{0}(x) \in L^{1} ((1 + \abs{x}^2) dx) 
  \end{cases}
\end{align} 
where the whole stochastic setting are the same as in Chapter 3. We know that if $\nabla V$ is bounded Lipschitz continuous, then the $\mathbb{L}^2$ solution exists. Instead of direct studying the stochastic equation \autoref{SDE}, we study the PDE which the joint law of $(X_1^N(t),\ldots ,X_N^{N}(t))$ satisfies. 

Now let $u_N(*,t)$ be the corresponding joint law, then by It\^o s formula and taking the expectation we know that $u_N(t,x_1,\ldots,x_N)$ satisfies the following higher dimensional PDE in the weak sense,
\begin{align}\label{PDEuN}
  \partial_t u_{N} &= \sum_{i=1}^{N} \Delta_{x_i} u_N + \sum_{i=1}^{N} \nabla_{x_i} * (u_N * \frac{1}{N} \sum_{j=1}^{N} \nabla V (x_i-x_j ))\\
  \label{IDHD}
  u_{N}\rvert_{t=0} &= u_0^{\otimes N}  = u_{0}(x_{1})u_{0}(x_{2})\ldots u_0(x_N)
.\end{align}
Notice that if $\|\nabla V\|_{\infty} \le C$, we have proved this linear transport equation with bounded drift has a unique solution $u_N \in  L^{\infty}([0,T] ; L^{1}(\mathbb{R}^{dN} ) ) $ in chapter 4. 
\begin{remark}
	By classical theory of linear PDE, which will be demonstrated in the PDE lecture, $\nabla V$ has enough regularity, then $u_N$ is a classical solution.
\end{remark}

\begin{goal}
We want to study the limit $N\to \infty$. The expect limiting PDE, as in the particle setting, should be the corresponding PDE of the Mckean-Vlasov equation, namely 
\begin{align*}
  \partial_t u - \Delta u = \nabla * (u\nabla V \star  u)
.\end{align*}
The question is how to express the limit of $u_N$ to be $u$. The idea in the relative entropy method is to ``upgrade'' $u$ into the higher dimension version, i.e. $u^{\otimes N}$ and study the limit
\begin{align*}
  u_N - u^{\otimes N}  \xrightarrow{N\to \infty} 0 
.\end{align*}
One can obtain directly that $u^{\otimes N}=u(t,x_1)u(t,x_2)\ldots u(t,x_N) $ satisfies
\begin{align}\label{PDEuHD}
  \partial_t u^{\otimes N}  = \sum_{i=1}^{N} \Delta _{x_i}  u^{\otimes N}  + \sum_{i=1}^{N} \nabla_{x_i}  * (u^{\otimes N} \nabla V \star u(t,x_i) )
.\end{align}
with the same initial data as in \autoref{IDHD}.
The remaining task is to compare these two higher dimensional equations \autoref{PDEuN} and \autoref{PDEuHD}.
\end{goal}

In the analysis of diffusion type of equations, the key technique is to get {\it a priori} estimates of the solution. There are in general many possible ``free energy''s for a given equation. As one can see from the heat equation,  $\partial_t u- \Delta  u = 0$, the following standard $L^p, p>1$ estimates holds:
 \begin{align*}
   \hspace{-20mm}  L^2 :\qquad &\frac{d}{dt} \int_{\mathbb{R}^{d} }  u ^2 dx + 2\int_{\mathbb{R}^{d} }  \abs{\nabla u}^2 dx = 0.\\
      \hspace{-20mm} L^p :\qquad &\frac{d}{dt} \int_{\mathbb{R}^{d} } |u| ^pdx + \frac{4(p-1)}{p}\int_{\mathbb{R}^{d} }  \abs{\nabla u^{\frac{p}{2}}}^2 dx = 0.
 .\end{align*}
These can be achieved by using using $2u$ and $pu^{p-1}$ with $p>1$ as test function. Actually one can obtain the so called entropy estimate by choosing the test function $\phi  = \log u$, namely
\begin{align*}
  \int_{\mathbb{R}^{d} } \partial_t u \log u dx- \int_{\mathbb{R}^{d} } \Delta  u \log u dx &= 0
.\end{align*}  
By using the product rule we have $\partial_t u \log u=\partial_t(u\log u)-\partial_t u$. Since $\frac{d}{dt}\int_{\mathbb{R}^{d} } udx=0$ due to the conservation of mass, we know that 
\begin{align*}
\dfrac{d}{dt}\int_{\mathbb{R}^{d} } u \log u dx+ \int_{\mathbb{R}^{d} } \frac{|\Delta  u|^2}{u} dx &= 0
.\end{align*}
The term $\int_{\mathbb{R}^{d} } \frac{|\Delta  u|^2}{u} dx$ is called the entropy production, which can also be written as $ \int_{\mathbb{R}^{d} } |\nabla \sqrt{u}|^2 dx =0$ or $\int_{\mathbb{R}^{d} }  \abs{\nabla \log u} ^2 u dx $. 

In the next we concentrate to study so called relative entropy for two density functions: it is generally defined
\begin{definition}[Relative Entropy]
The relative entropy between two integrable functions $u_1$ and $u_{2}$ are given by
\begin{align*}
\int_{\mathbb{R}^{d} }\frac{u_{1}}{u_{2}} \log \frac{u_{1}}{u_{2}}*u_{2} dx=\int_{\mathbb{R}^{d} }{u_{1}} \log \frac{u_{1}}{u_{2}} dx
\end{align*}
\end{definition}
\begin{remark}
	It is not hard to compute that if $u_1$ and $u_2$ are both solutions of the heat equation, then
\begin{align*}
   \partial_t \int_{\mathbb{R}^{d} } u_{1} \log \frac{u_{1}}{u_{2}}dx&= \int_{\mathbb{R}^{d} } \partial_t u_{1} \log \frac{u_{1}}{u_{2}}dx + \int_{\mathbb{R}^{d} } u_{1} \frac{u_{2}}{u_{1}}(\partial_t \frac{u_{1}}{u_{2}})dx \\
   &=\int_{\mathbb{R}^{d} } (\partial_t u_{1} \log \frac{u_{1}}{u_{2}}+\partial_t u_1-\frac{u_{1}}{u_{2}}\partial_t u_2 ) dx\\
   &=- \int_{\mathbb{R}^{d} } \nabla u_{1} \nabla\log \frac{u_{1}}{u_{2}}dx+ \int_{\mathbb{R}^{d} } \nabla\frac{u_{1}}{u_{2}}\nabla u_2 dx\\
   &=- \int_{\mathbb{R}^{d} } \nabla\log\frac{u_{1}}{u_{2}}(\nabla u_1-\frac{u_1}{u_2}\nabla u_2) dx\\
   &=- \int_{\mathbb{R}^{d} } |\nabla\log\frac{u_{1}}{u_{2}}|^2 u_1 dx
 .\end{align*}
\end{remark}


\section{Relative entropy inequality for high dimensional PDE}
Now we come back to the two higher order PDE's \autoref{PDEuN} and \autoref{PDEuHD}, and obtain the following relative entropy identity.
\begin{lemma} Let 
	\begin{align}
	\label{REntropyN} \mathcal{H}(u_N | u^{\otimes N} )= \frac{1}{N} \mathcal{H}_N(u_N | u^{\otimes N} ) = \frac{1}{N} \int_{\mathbb{R}^{dN} } u_N \log \frac{u_N}{u^{\otimes N} }   dx_{1}\ldots dx_N
	\end{align}
	If $u_N$ and $u^{\otimes N}$ are solutions of  \autoref{PDEuN} and \autoref{PDEuHD} separately, then the following identity and inequality hold
	\begin{align}
	\label{relativeentropyID} 
	&\frac{d}{dt} \mathcal{H}(u_{N} | u^{\otimes N}  ) = -   \frac{1}{N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N}  \abs*{\nabla_{x_i} \log \frac{u_N}{u^{\otimes N} }}^2 u_N dx_{1}\ldots dx_N\\
\nonumber	&\qquad - \frac{1}{N} \sum_{i=1}^{N} \int_{\mathbb{R}^{dN} }  \nabla_{x_i} \log \frac{u_N}{u^{\otimes N} } \left[ u_N \left(\nabla V \star u(x_i) - \frac{1}{N} \sum_{j=1}^{N} \nabla V(x_i-x_j) \right) \right]   dx_{1}\ldots dx_N\\
\label{REntropyNM}  &\le - \frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs{\nabla_{x_i} \log \frac{u_N}{u^{\otimes N} }}^2 u_N dx_{1}\ldots dx_N\\
\nonumber &\quad + \frac{1}{2} \int_{\mathbb{R}^{dN} } u_N \frac{1}{N} \sum_{i=1}^{N}  \abs*{\nabla V \star  u (x_i) - \frac{1}{N} \nabla V(x_i-x_j) }^2 dx_{1}\ldots dx_N
	\end{align}
\end{lemma}
\begin{proof}
We directly compute the time derivative, similar to the case of Heat equation, and obtain
\begin{align*}
  \frac{d}{dt} \mathcal{H}(u_N | u^{\otimes N}) &= \frac{1}{N} \int_{\mathbb{R}^{dN} } \Big(\partial_t u_N  \log \frac{u_N}{u^{\otimes N} }+ \partial_t u_N - u_N \frac{u^{\otimes N}}{(u^{\otimes N} )^2}\partial_t u^{\otimes N}\Big) dx_{1}\ldots dx_N.
\end{align*}
Now we input the equation for $\partial_t u_N$ and $\partial_t u^{\otimes N}$ and do integral by parts,
\begin{align*}
   &\frac{d}{dt} \mathcal{H}(u_N | u^{\otimes N}) \\
                       = & \frac{1}{N} \int_{\mathbb{R}^{dN} }- \sum_{i=1}^{N}(\nabla_{x_i} u_N + u_N \frac{1}{N} \sum_{j=1}^{N} \nabla V (x_i-x_j) ) \nabla_{x_i} \log \frac{u_N}{u^{\otimes N} } dx_{1}\ldots dx_N\\
                                                &\quad - \frac{1}{N} \int_{\mathbb{R}^{dN} }-  \sum_{i=1}^{N}(\nabla_{x_i} u^{\otimes N}  + u^{\otimes N} \nabla V \star u(x_i) ) \underbrace{\nabla_{x_i}(\frac{u_N}{u^{\otimes N} })}_{= \frac{u_N}{u^{\otimes N} }*\nabla_{x_i} \log \frac{u_N}{u^{\otimes N} }}dx_{1}\ldots dx_N\\
                                                &= \frac{1}{N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N}(-\nabla_{x_i}u_N + \nabla_{x_i} u^{\otimes N}*\frac{u_N}{u^{\otimes N} } ) \nabla_{x_i} \log \frac{u_N}{u^{\otimes N} }dx_{1}\ldots dx_N\\
                                                &\quad + \frac{1}{N} \int_{\mathbb{R}^{dN} } \left(\sum_{i=1}^{N} \nabla V \star  u(x_i)  - \frac{1}{N} \sum_{j=1}^{N}  \nabla V(x_i - x_j) \right)u_N \nabla_{x_i} \log \frac{u_N}{u^{\otimes N} } dx_{1}\ldots dx_N
.\end{align*}
Then the equation \autoref{relativeentropyID} follows directly by rewriting the entropy production term, and inequality \autoref{REntropyNM} follows from the H\"older's inequality.
\end{proof}

Notice that the second term on the right hand side of \autoref{REntropyNM} needs directly the mean field limit estimates. We will apply the estimates obtained in the previous chapters for bounded Lipschitz forces and forces with singular potential and obtain separately the convergence rate estimate for the relative entropy.
More precisely, we can rewrite the second term by using the stochastic setting:
\begin{align}\label{ReE}
  &\frac{d}{dt} \mathcal{H}(u_{N} | u^{\otimes N}  ) \\
 \nonumber &\leq  -\frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs*{\nabla_{x_i} \log  \frac{u_N}{u^{\otimes N} }}^2 u_N  dx_{1}\ldots dx_N+ \frac{1}{2} \E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{\nabla V \star  u(X^N_i)-\nabla V \star  \mu_N(X^N_i)}^2 \right]\\
 \nonumber &\leq  -\frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs*{\nabla_{x_i} \log  \frac{u_N}{u^{\otimes N} }}^2 u_N  dx_{1}\ldots dx_N+ \frac{1}{2} \E[\braket{\mu_N , \abs{\nabla V \star (\mu_N - u)}^2}]
.\end{align}
Remember the notation of $X^N_i(t)$ are solutions of \eqref{SDE} and $\mu_N$ is the corresponding random empirical measure.

\begin{exercise}
	Calculate the relative entropy for the second order system 
	\begin{align*}
	\begin{cases}
	&dX_t^i = V_t^i dt\\
	&dV_t^i = \frac{1}{N} = \sum_{j=1}^{N} \nabla V(X_i-X_j)  dt + \sqrt{2}dW_t^i 
	\end{cases}
	.\end{align*}
\end{exercise}

\section{From Relative Entropy to Strong L1 norm}

Before we proceed further for relative entropy estimates, let's mention two lemmata.

\begin{lemma}[Csisz\`ar-Kullback-Pinsker]
	For $0\le f,g \in  L^{1}(\mathbb{R}^{d} ) $, let the relative entropy between $f$ and $g$ be given by
	\begin{align*}
	\mathcal{H}(f | g) = \int_{\mathbb{R}^{d} } f \log \frac{f}{g}
	.\end{align*}
	Then the negative part of $\mathcal{H}(f|g)$ is bounded.
	
	If furthermore $\|f\|_{L^{1}(\mathbb{R}^{d} ) } = \|g\|_{L^{1} (\mathbb{R}^{d} )}$ then it holds 
	\begin{align*}
	\|f-g\|_{L^{1}(\mathbb{R}^{d} ) }^2 \le 2 \mathcal{H}(f | g)
	.\end{align*}
\end{lemma}
\begin{proof}
First of all, the negative part of $\mathcal{H}(f|g)$ is bounded in the following:
\begin{align*}
\int_{f \le g} f * \Big|\log \frac{f}{g}\Big| =  \int_{f \le  g} g \frac{f}{g} \Big|\log  \frac{f}{g}\Big| \le  \frac{1}{e} \|g\|_{L^{1}(\mathbb{R}^{d} ) }, \mbox{ where }	r \log  r \ge  -\frac{1}{e} \mbox{ has been applied.}
\end{align*}
If $\|f\|_{L^{1}} = \|g\|_{L^{1} } $, then
\begin{align*}
\int_{\mathbb{R}^{d} } \frac{f}{g} \log  \frac{f}{g} g dx \ge \int_{\mathbb{R}^{d} }   \frac{f}{g} * g dx - \int_{\mathbb{R}^{d} }  g dx = 0 \mbox{ where }	r \log  r \ge r-1 \mbox{ has been applied.}
\end{align*}
this means for two probability densities ($\|\mu \|_{L^{1} }=\|\nu \|_{L^{1} } = 1$) the entropy is always non-negative.

Furthermore, by direct computation we obtain, considering   $\|f\|_{L^{1}} = \|g\|_{L^{1} } =1$, that
	\begin{align*}
	\left( \int_{\mathbb{R}^{d} }  \abs{f-g} dx \right)^2 &=  \left( \int_{\mathbb{R}^{d} }   \Big|\frac{f}{g} - 1\Big| g  dx\right) ^2\\
	&= \left( \int_{\mathbb{R}^{d} }   \frac{\abs{\frac{f}{g} - 1}}{\sqrt{\frac{f}{g} + 2}} g^{\frac{1}{2}}(\sqrt{\frac{f}{g}+2})g^{\frac{1}{2}} dx  \right) ^2\\
	&\myS{Höld.}{\le } \int_{\mathbb{R}^{d} }  \frac{\abs{\frac{f}{g}-1}^2}{(\frac{f}{g}+2)}g dx *\int_{\mathbb{R}^{d} } (\frac{f}{g} + 2)g dx\\
	&\le   \frac{2}{3} \left( \int_{\mathbb{R}^{d} } \frac{f}{g} \log \frac{f}{g} g dx - \int_{\mathbb{R}^{d} }  (\frac{f}{g} - 1) g dx \right) \cdot 3\\
	&= 2 \int_{\mathbb{R}^{d} }  f \log \frac{f}{g} dx\\
	&= 2 \mathcal{H}(f|g)
	,\end{align*}
where we used 
	\begin{align*}
	r \log  r - r + 1 \ge  \frac{3}{2} \frac{(r-1)^2}{r+2} \quad \forall  r \ge 0
	.\end{align*}
\end{proof}
\begin{exercise}
	Prove that $	r \log  r - r + 1 \ge  \frac{3}{2} \frac{(r-1)^2}{r+2}$, $\forall  r \ge 0$ holds.
\end{exercise}

\begin{lemma}[super-Additivity of Relative Entropy] \label{SAEntropy}
	Let $F_N$  be symmetric probability density on $\mathbb{R}^{dN} $ and denote 
	by $F_N^{(k)} $ its $k$-th marginal density, let $u$ be any probability density on $\mathbb{R}^{d} $,  and the relative entropy for the $k$-th marginal density is defined by
	\begin{align*}
	\mathcal{H}_{k}(F_N^{(k)},u^{\otimes k}  ) = \int_{\mathbb{R}^{dk} } F_N^{(k)}  \log \frac{F_N^{(k)} }{u^{\otimes k} } dx_{1}\ldots dx_k
	.\end{align*}
	Then it holds that
	\begin{align*}
	\mathcal{H}_{k}(F_N^{(k)},u^{\otimes k}  )  + \mathcal{H}_{N-k}(F_N^{(N-k)} | u^{\otimes (N-k)}  ) \le \mathcal{H}_{N}(F_N | u^{\otimes N} )
	.\end{align*}
	and 
	\begin{align*}
	\frac{1}{N} \mathcal{H}_N(F_N | u^{\otimes N} )  \ge \frac{1}{N} \floor{\frac{N}{k}} \mathcal{H}_k(F_N^{(k)}|u^{\otimes k}  ) \ge  \frac{1}{2k} \mathcal{H}_k(F_N^{(k)} | u^{\otimes k}  )
	\end{align*}
	where $\floor{\cdot}$ means the Gauss bracket.
\end{lemma}
\begin{proof}
By direct computation, we obtain
	\begin{align*}
	&\mathcal{H}_N - \mathcal{H}_{N-k}  - \mathcal{H}_{k}\\
	&= \int_{\mathbb{R}^{dN} }F_N \log  \frac{F_N}{u^{\otimes N} } dx_{1}\ldots dx_N - \int_{\mathbb{R}^{dk} }\Big(\int_{\mathbb{R}^{d(N-k)} } F_N \log \frac{F_N^{(N-k)}}{u^{\otimes (N-k)} } dx_{k+1}\ldots dx_N\Big)dx_{1}\ldots dx_k\\
	& \quad - \int_{\mathbb{R}^{d(N-k)}}\Big( \int_{\mathbb{R}^{dk} } F_N  \log  \frac{F_N^{(k)} }{u^{\otimes k} } dx_{1}\ldots dx_k\Big) dx_{k+1}\ldots dx_N \\
	&= \int_{\mathbb{R}^{dN} }F_N (\log  \frac{F_N}{u^{\otimes N} } - \log  \frac{F_N^{(N-k)} }{u^{\otimes N-k} } - \log \frac{F_N^{(k)} }{u^{\otimes k} }) dx_{1},\ldots dx_N\\
	&= \int _{\mathbb{R}^{dN} } F_N(x_1,\ldots,x_N) \log \frac{F_N(x_{1},\ldots ,x_N)}{F_N^{(N-k)}(x_{k+1},\ldots ,x_N)F_N^{(k)}(x_{1},\ldots ,x_k)}  dx_{1},\ldots dx_N \ge 0,
	\end{align*}
	where in the last step we use the fact that 
	$$
	\int_{\mathbb{R}^{dN} }F_Ndx_{1},\ldots dx_N=\int_{\mathbb{R}^{dN} }F_N^{(k)}(x_1,\ldots,x_k) F_N^{(N-k)}(x_{(k+1)},\ldots,x_N)dx_{1},\ldots dx_N.
	$$
\end{proof}


As a summary from the superadditivity of the relative entropy and the Csisz\`ar-Kullback-Pinsker, we obtain that
\begin{corollary}
\begin{align*}
\|u_N^{(1)}-u \|_{L^1(\mathbb{R}^d)}^{2} \le  2 \mathcal{H}_1(u_{N}^{(1)} | u )\leq \frac{4}{N} \mathcal{H}(u_N | u^{\otimes N} )
.\end{align*}	
\end{corollary}

\section{Completion the Relative Entropy Estimate by Mean Field Limit}

If we can estimate the rest term in the relative entropy inequality \autoref{ReE}, 
\begin{align*}
  \E[\braket{\mu_N , \abs{\nabla V \star (\mu_N - u)}^2}]  \xrightarrow{N\to \infty} 0 
\end{align*}
we will obtain the mean field limit in strong $L^1$ sense.
 
We first show a complete result for bounded Lipschitz force, and then describe the idea in how to handling the case with singular force.
\begin{theorem}
  If $\|D^2V\|_{L^\infty(\mathbb{R}^d)} \le  C$  then 
  \begin{align*}
    \|u_N^{(1)} - u \|_{L^{1}(\mathbb{R}^{d} ) } \rightarrow 0, \mbox{ as } N\to \infty.
  .\end{align*}
\end{theorem}
\begin{proof}
	According to the previous discussions, it is enough to estimate the mean field term appeared in \autoref{ReE}. We review the notation $Y^N_i$ to be the solution of Mckean-Vlasov equation
 \begin{align*}
\text{(MVE*)}\begin{cases}
Y^N_i(t) &= \nabla V\star u(Y^N_i(t),t)dt + \sqrt{2} dW_t\\
Y^N_i(0) &= X^N_i(0) \in  L^{2}(\Omega ) \mbox{ given i.i.d. random variables}\\
u_0 &= \mathcal{L}(X^N_i(0)).
\end{cases}
\end{align*}
Then the mean field term in \autoref{ReE} can be estimated by using the results from Chapter 3 or 4,	
\begin{align*}
  &\E\Big[\frac{1}{N} \sum_{i=1}^{N} \abs{\nabla V \star  u (X_i^{N} ) - \frac{1}{N} \sum_{j=1}^{N} \nabla V (X_i^{N}-X_j^{N}  ) }^2 \Big] \\
  \le & \E\Big[\frac{1}{N} \sum_{i=1}^{N} \abs{\nabla V \star  u(X_i^{N}) - \nabla V \star  u(Y_i^{N}  )}^2 \Big] + \E\Big[\frac{1}{N} \sum_{i=1}^{N} \abs{\nabla V \star  u(Y_i^{N}  ) - \frac{1}{N} \sum_{j=1}^{N} \nabla V(Y_i^{N} - Y_j^{N}    ) }^2 \Big]\\
  &\quad +  \E\Big[\frac{1}{N} \sum_{i=1}^{N} \abs{ \frac{1}{N} \sum_{j=1}^{N}\nabla V(Y_i^{N} - X_j^{N} ) -\nabla V (X_i^{N} - X_j^{N}  )}^2 \Big]  \\
  \le & \|D^2 V \star  u \|_{L^\infty(\mathbb{R}^d)}^{2} \max_{1\leq i\leq N} \E\big[\abs{X_i^{N} -Y_i^{N}  }^2\big]  +  \frac{\|\nabla V\|^2_{L^\infty(\mathbb{R}^d)}}{N} + \|D^2V\|^2_{L^\infty(\mathbb{R}^d)} \max_{1\leq i\leq N} \E\big[\abs{Y^{N}_i - X_i^{N}   }^2\big]\\
  \le & C*\frac{1}{N}.
\end{align*}
In the above estimates we use the law of large number result for the second term, and the mean field limit result for the first and last term.
\end{proof}

\begin{remark}
Actually the above estimate also shows the convergence rate of the relative entropy
\begin{align*}
\|u_N^{(1)}-u \|_{L^1(\mathbb{R}^d)}\leq \sqrt{ \frac{4}{N} \mathcal{H}(u_N | u^{\otimes N} )}\leq \frac{C(t)}{\sqrt{N}}.
\end{align*}
\end{remark}
\vskip1cm

If the interaction potential $V$ has singularity, we will explain the idea by using example $V=\frac{1}{\abs{x}^{\lambda}}$ where $\lambda\in (0,d-2)$. Notice that in the previous chapter, we have proved that 
% \begin{align*}
%&\sup_{0\leq t\leq T}   \E[\max_{1\leq i\leq N} \abs{X_{i}^{N,\epsilon} - \overline{X}_i^{\epsilon} } > N^{-\alpha}]\leq %C*N^{-\gamma} 
%,\end{align*}
 where we started with a mollified version  of the many particle system, if we use the notation $u_{N}^\epsilon$ as the joint law of $(X^{N,\epsilon}_i(t))$, $i=1,\ldots,N$, from \autoref{SDEepsilon}. Then the relative entropy inequality work exactly 
 \begin{align}\label{ReEepsilon}
 &\frac{d}{dt} \mathcal{H}(u_{N}^\epsilon | u^{\otimes N}  ) \\
 \nonumber &\leq  -\frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs*{\nabla_{x_i} \log  \frac{u_N^\epsilon}{u^{\otimes N} }}^2 u_N^\epsilon  dx_{1}\ldots dx_N+ \frac{1}{2} \E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{\nabla V \star  u(X^{N,\epsilon}_i)-\nabla V_\epsilon \star  \mu_N(X^{N,\epsilon}_i)}^2 \right]\\
 \nonumber &\leq  -\frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs*{\nabla_{x_i} \log  \frac{u_N^\epsilon}{u^{\otimes N} }}^2 u_N^\epsilon  dx_{1}\ldots dx_N+ \frac{1}{2} \E[\braket{\mu_N , \abs{\nabla V_\varepsilon \star (\mu_N - u^\epsilon)}^2}]\\
 \nonumber &\hspace{2cm} + \frac{1}{2}\|V_\epsilon\star\nabla (u^\epsilon-u)\|^2_\infty+\frac{1}{2}\|(V-V_\epsilon)\star\nabla u\|^2_\infty
 .\end{align}
 We will show the idea how to estimate the mean field rest term in \autoref{ReE} with the help of convergence in probability result in Chapter 5. The last two terms above can be easily controlled by $\epsilon^2$ based on the \autoref{AssPDESol}.

For the potential $V=\frac{1}{\abs{x}^{\lambda}}$ where $\lambda\in (0,d-2)$ we have already shown that, for $\epsilon =\frac{1}{N^{\beta}}$, it holds
 \begin{align*}
  & \sup_{0\leq t\leq T}\P(\max_{1\leq i\leq N} \abs{X_i^{N,\epsilon} -\overline{X}_i^{\epsilon}  } > N^{-\alpha} ) \le \frac{C}{N^{\gamma}}  \quad \forall \gamma>0.
 \end{align*}
Then we have that for the first term on the right hand side of \autoref{ReEepsilon},
 \begin{align*}
   &\E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{\nabla V_{\epsilon} \star  u(X_i^{N,\epsilon} ) - \frac{1}{N} \sum_{j=1}^{N} \nabla V_{\epsilon}(X_i^{N,\epsilon} - X_j^{N,\epsilon} )}^2  \right]\\
   &\le \E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{\nabla V_{\epsilon} \star  u(X_i^{N,\epsilon} ) - \nabla V_{\epsilon} \star  u^\epsilon(\overline{X}_i^{\epsilon}  )}^2  \right]\\
 & \quad +  \E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{\nabla V_{\epsilon} \star  u^\epsilon(\overline{X}_i^{\epsilon} ) - \frac{1}{N} \sum_{j=1}^{N} \nabla V_{\epsilon}(\overline{X}_i^{\epsilon} -\overline{X}_j^{\epsilon})}^2  \right]\\
 & \quad +  \E\left[\frac{1}{N} \sum_{i=1}^{N} \abs*{ \frac{1}{N} \sum_{j=1}^{N} \Big(\nabla V_{\epsilon}(\overline{X}_i^{\epsilon} - \overline{X}_j^{\epsilon} )- \nabla V_{\epsilon}(X_i^{N,\epsilon} - X_j^{N,\epsilon})\Big)}^2  \right]\\
   &= I + II + III
 .\end{align*}
 We again consider  the subset of $\Omega$
 \begin{align*}
   A(t) = \Big\{\omega  \in  \Omega  | \max_{1\leq i\leq N} \abs{X_i^{N,\epsilon} (t) - \overline{X}_i^{\epsilon}(t)  } > N^{-\alpha}\Big\}   \implies \P(A(t)) \le  \frac{C}{N^{\gamma} }, \forall\gamma>0
.\end{align*}
 Then the first and third term $I+III$ can be bounded by splitting the domain $\Omega$. ($1=\cha_{A^{c}} + \cha_{A}  $ ),
 \begin{align*}
  I+ III &\le  2\Big( \|\nabla V_{\epsilon}\star u\|_{\infty}^{2}+\|\nabla V_{\epsilon}\|_{\infty}^{2}\Big) \E[\cha_{A}]  + 2\Big( \|D^2 V_{\epsilon}\star u\|_{\infty}^{2}+\|D^2 V_{\epsilon}\|_{\infty}^{2}\Big)\frac{1}{N^{2\alpha}} \\
      &\le  \frac{4 \|\nabla V_{\epsilon}\|_{\infty}^{2}}{N^{\gamma} } +  \frac{4 \|D^2 V_{\epsilon}\|_{\infty}^{2}}{N^{2\alpha} } \\
     &  \leq C(\gamma)N^{2\beta(\lambda+1)-\gamma}+CN^{2\beta(\lambda+2)-2\alpha}\to 0,
\end{align*}
where we have used $\|D^2 V_{\epsilon}\|_{\infty} \le  \frac{C}{\epsilon^{\lambda+2}},\|\nabla V_{\epsilon}\|_{\infty} \le  \frac{C}{\epsilon ^{(\lambda+1)} }$.
And the second term can be bounded simply by using the law of large numbers.
We arrive at in the end by choosing $0<\beta<\min {\beta_1,\alpha/(\lambda+2)}$ that
\begin{align*}
&\frac{d}{dt} \mathcal{H}(u_{N}^\epsilon | u^{\otimes N}  ) +\frac{1}{2N} \int_{\mathbb{R}^{dN} } \sum_{i=1}^{N} \abs*{\nabla_{x_i} \log  \frac{u_N^\epsilon}{u^{\otimes N} }}^2 u_N^\epsilon  dx_{1}\ldots dx_N\leq CN^{-\beta}
.\end{align*}
