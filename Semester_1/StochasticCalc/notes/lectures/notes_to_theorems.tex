
\begin{Lemma}[3.2 It\^o s isometry simple version]
 For $f \in  \mathcal{H}_0^{2} $  we have 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \|I(f)\|_{L^2}
 .\end{align*}
\end{Lemma}
\begin{proof}
 We have 
 \begin{align*}
   \|I(f)\|_{L^2} &= \E[(\sum_{i=1}^{n} a_i (B_{t_i} - B_{t_{i-1}}))^2]\\
                  &= \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2 + \sum_{i\neq j}^{n} a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ]\\
                  &=  \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[\E[a_i^2(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2]\E[(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2](t_i-t_{i-1})\\
 .\end{align*}
 Since  w.l.o.g take $i<j$
 \begin{align*}
   \E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ] &= \E[\E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
                                                              &= \E[a_i \underbrace{\E[(B_{t_{i}}-B_{t_{i-1}})]}_{=0}\E[a_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
 .\end{align*}
 But 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \E[\int_0^{T} f^2 ds ] = \sum_{i=1}^{n}\E[a_i^2](t_i-t_{i-1})
 .\end{align*}
\end{proof}
\begin{Prop}[3.5]
  For every $f \in  \mathcal{H}^2$  there exists a sequence $(f_n)_{n \in  \mathbb{N}}) \subset  \mathcal{H}_0^{2} $ such that 
  \begin{align*}
    \|f_n - f\|_{\mathcal{H}^2}\xrightarrow{n\to \infty} 0
  .\end{align*}
\end{Prop}
\begin{proof}
 The rough outline of the proof can be split up as follows
 \begin{enumerate}
  \item Show that we can assume $f$ is bounded 
  \item Show that we can assume $f$ is bounded adapted and continuous
  \item Construct simple sequence that approximates $f$
 \end{enumerate}
 For step 1 we can simply consider that for any (potentially ) unbounded function $f$ 
 \begin{align*}
  f_n = -n \lor (f \land n)
 .\end{align*}
 is bounded and apply DCT
 \begin{align*}
   \lim_{n\to \infty}\|f_n - f\|_{\mathcal{H}^2} = \lim_{n\to \infty}\E[\int_0^{T}(f_n-f)^2 dt ] =  \E[\int_0^{T} \lim_{n \to \infty}(f_n-f)^2 dt ] = 0
 .\end{align*}
 Thus we may assume $f$ bounded \\[1ex]
 For step 2 we can use that we can get the height of a rectangle by dividing by its base s.t.
 \begin{align*}
   f_n(*,t) \coloneqq  \frac{1}{(t-(t-\frac{1}{n})_+)} \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds =  n \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds
 .\end{align*}
 Now we note that since we work with random variables we condition $\omega $ on the set where
 \begin{align*}
   \lim_{n\to \infty}  f_n(\omega ,t) = f(\omega ,t)
 .\end{align*}
 We need this set to have measure 1, such that 
 \begin{align*}
   A \coloneqq  \{(\omega ,t) \in  \Omega  \times  [0,T] : \lim_{n\to \infty} f_n(\omega ,t) \neq  f(\omega ,t)\}  
 .\end{align*}
 Has measure zero with respect to $\P \otimes \lambda $, we have by the fundamental theorem of calculus that
 \begin{align*}
   \lambda(\{t \in  [0,T] : (\omega,t) \in  A\}) = 0
 .\end{align*}
Or rather Lebesgue Differentiation theorem, otherwise we'd need the condition that $f(\omega ,*)$ only has countable many discontinuities.
Thus we get that we may assume $f$ is bounded and continuous.\\[1ex]
We now construct our simple function $f_n$ as 
\begin{align*}
  f_{n,s}(\omega ,t) \coloneqq  f(\omega ,(s+\phi_n(t-s)_+))
.\end{align*}
where
\begin{align*}
  \phi_n = \sum_{i=1}^{2^n} \frac{j-1}{2^{n} }\cha_{(\frac{j-1}{2^{n}  },\frac{j}{2^{n} }]}
.\end{align*}
makes our time interval discrete (standard argument really), then we wanna show 
\begin{align*}
  \|f_n-f\|_{\mathcal{H}^2}&= \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] \to 0
.\end{align*}
We have 
\begin{align*}
  \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] &\le \E[\int_0^{T} \int_0^{1} \abs{f_{n,s}-f}^2 ds dt]\\
                                       &= \E[\int_0^{T} \int_0^{1} \abs{f(*,(s+\phi_n(t-s))_+)-f}^2 ds dt]\\
                                       &=  \sum_{j\in \mathbb{Z}}\E[\int_0^{T} \int_{[t-\frac{j}{2^{n} },t-\frac{j-1}{2^{n} }] \cap [0,1]} \abs{f(*,(s+\frac{j-1}{2^{n} })-f}^2 ds dt]\\
                                       &\le (2^{n}+1 )2^{-n}  \int_{(0,1]} \E[\int_0^{T}\abs{f(*,t-2^{-n}h )-f(*,t)}^2 dt ]dh 
.\end{align*}
Where we can show that the Expectation term goes to 0
\end{proof}
\begin{Theorem}[3.7.]
  For any $f \in  \mathcal{H}^2$ there is a continuous martingale $X = (X_t)_{t \in  [0,T]}$ with 
  respect to $\mathcal{F}_t$ such that for all $t \in  [0,T]$
  \begin{align*}
    X_t = I(f \cha_{[0,t]})
  .\end{align*} 
\end{Theorem}
\begin{proof}
 We first consider the simple case with 
 \begin{align*}
   f_n = \sum_{i=0}^{m_n - 1} a_i^{n}  \cha_{(t_i^{n},t_{i+1}^{n}  ]}
 .\end{align*}
 Then 
 \begin{align*}
   \E[X_{t}^{n} - X_{s}^{n} | \mathcal{F}_s  ] &= \E[\sum_{t_i > s} a_i(B_{t_{i+1}}-B_{t_i})]\\
                                               &=  0
 .\end{align*}
Our end goal is to use a triangular inequality to use the simple case to bound the normal one i.e.
\begin{align*}
  \abs{\E[X_t-X_s|\mathcal{F}_s]} &= \abs{\E[X_t-X_{t}^{n}+X_{t}^{n} -X_s +X_{s}^{n} -X_{s}^{n}  |\mathcal{F}_s]}\\
                                  &\le \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \underbrace{\abs{\E[X_{t}^{n} -X_{s}^{n}  |\mathcal{F}_s]}}_{=0} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}\\
                                  &= \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}\\
                                  &\myS{Jen}{\le } \E[\abs{X_t-X_{t}^{n}}|\mathcal{F}_s] + \E[\abs{X_{s}^{n}-X_s}|\mathcal{F}_s ]\\
.\end{align*}
We consider $A \in  \mathcal{F}_s$
\begin{align*}
  \E[\abs{X_t-X_{t}^{n}}\cha_A] + \E[\abs{X_{s}^{n}-X_s}\cha_A ] &\le \E[\abs{X_t-X_{t}^{n}}] + \E[\abs{X_{s}^{n}-X_s} ]\\
                                                                 &\le 2*\|f-f_n\|_{\mathcal{H}^2}
.\end{align*}
So we have $\E[X_t |\mathcal{F}_s] = X_s$\\[1ex]
For $(f_n)_{n \in  \mathbb{N}} \subset \mathcal{H}^2_0 $ we have 
\begin{align*}
  \lim_{n\to \infty}f_n = f \in \mathcal{H}^2
.\end{align*}
And 
\begin{align*}
  \lim_{n \to \infty} I(f_n) = I(f) \in L^2
.\end{align*}
Which should be equivalent to for fixed $t \in  [0,T]$
\begin{align*}
  X_t^n &\xrightarrow{L^2}  X_t \\
  I(f_n\cha_{[0,t]}) &\xrightarrow{L^2} I(f\cha_{[0,t]})
.\end{align*}
We need to make the argument uniform in $t \in  [0,T]$, which is Step 2 in the script i guess.\\[1ex]
We have
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  \epsilon)  \le  \epsilon^{-2} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2}
.\end{align*}
since  for all $p>1$
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t} \ge  \epsilon) \frac{1}{\epsilon^p} \E[\abs{X}_T^p]
.\end{align*}
By choosing a subequence we can get 
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} )  \le  2^{2k} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2} \le  2^{-k} 
.\end{align*}
Then we can apply Borel-Cantelli since
\begin{align*}
  \sum_{k=0}^{\infty}   \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} ) < \infty
.\end{align*}
and get $\Omega_0 \in  \mathcal{F}$ such that $\P(\Omega_0) = 1$ and $X^{n_k} $ is a pathwise cauchy sequence.
\end{proof}
\begin{Prop}[3.10]
 Let $f \in  \mathcal{H}^2$ and $\nu $ be a stopping time satisfying 
 \begin{align*}
   f \cha_{[0,\nu]} = 0
 .\end{align*}
 The integral process $X=(X_t)_{t \in  [0,T]}$ with $X_t = \int_0^{t} f(*,s) dB_s $, the fulfills 
 \begin{align*}
   X \cha_{[0,\nu ]} = 0
 .\end{align*}
 In particular for two functions $f,g \in  \mathcal{H}^2$ with $f \cha_{[0,\nu ]} = g \cha_{[0,\nu ]}$ the integral processes coincide on $[0,\nu ]$
\end{Prop}
\begin{remark}
  This proposition is mostly used to prove that the same holds for $\mathcal{H}^2_{\text{loc}}$  functions as well since this allows us 
  to use localizing sequences $\tau_m$ and use the fact that on 
  \begin{align*}
    \{\tau_m = T\}  
  .\end{align*}
  The processes must coincide.
\end{remark}
\begin{proof}
 The proof follows similarly to before where we first prove the simple case, for that suppose
 \begin{align*}
   X_t &= I(f\cha_{[0,t]})\\
   Y_t &= I(f\cha_{[0,\nu ]}\cha_{[0,t]})
 .\end{align*}
 Then it suffices to consider the simplification $f = a\cha_[(r,s]] $ for $0\le r < s \le T$.\\
 The important thing to note now is that 
 \begin{align*}
   Y_t =  I(f\cha_{[0,\nu ]}\cha_{[0,t]}) = \int_0^{t}\underbrace{(f \cha_{[0,\nu ]}}_{\notin \mathcal{H}_0^2}
 .\end{align*}
 For a function $h = f\cha_{[0,\nu ]}$ to be in $\mathcal{H}_0^2$ there needs to be a representation 
 \begin{align*}
   h =  \sum_{i=0}^{n-1} a_i \cha_{[t_{i-1},t_{i}]}  
 .\end{align*}
 But $\nu$ is continuous, such that there cannot exist such a representation, which means we first 
 need to consider the discrete stopping time $\nu^n$ as follows 
 \begin{align*}
   s_{i,n} &= r+(s-r)\frac{i}{2^{n} }\\
   \nu ^{n} &= \sum_{i=0}^{2^{n} - 1 } s_{i+1,n}\cha_{(s_{i,n},s_{i+1,n}]}(\nu )
 .\end{align*}
 We show 
 \begin{align*}
   f \cha_{[0,\nu ^{n} ]} &=  f- f \cha_{\nu ^{n},T }\\
                          &= f - f \sum_{i=0}^{2^{n} - 1 } \cha_{(s_{i,n},s_{i+1,n}]}(\nu)\cha_{(s_{i+1,n},T]} \in  \mathcal{H}_0^{2} 
 .\end{align*}
 then by definition of the integral for simple functions it follows
 \begin{align*}
   Y_t^N = \int_0^{t} f(*,s) \cha_{[0,\nu^n]}(u) dB_u = a(B_{s \land \nu ^{n} \land t } - B_{r \land \nu ^{n} \land t })
 .\end{align*}
 And by continuity of $B$ we can do
 \begin{align*}
   Y_t  = \lim_{n\to \infty}Y_t^n = a(B_{s \land \nu \land t } - B_{r \land \nu  \land t })
 .\end{align*}
 But this is clearly $X_t\cha_{[0,\nu ]}$, it follows
 \begin{align*}
   X\cha_{[0,\nu ]} =  Y\cha_{[0,\nu ]}
 .\end{align*}
 Note that for $X\cha_{[0,\nu ]}$  there is no difficulty since we can first apply the definition of the simple integral,
 and then consider the stopping time\\[1ex]
 For general $f \in  \mathcal{H}^2$ we choose $(f_n) \subset  \mathcal{H}_0^{2} $ such that 
 \begin{align*}
   \|f -f_n\|_{\mathcal{H}^2} \to 0
 .\end{align*}
 then we already know that $X^n = Y^n$ such that 
 \begin{align*}
   X \cha_{[0,\nu ]} = \lim_{n\to \infty}X^n\cha_{[0,\nu ]} = \lim_{n \to \infty} Y^n\cha_{[0,\nu ]} = Y \cha_{[0,\nu ]}
 .\end{align*}
\end{proof}
\begin{Prop}[3.13]
  For every $f \in  \mathcal{H}^2_{\text{loc}}$  there is a localizing sequence $(\nu_n)_{n \in  \mathbb{N}}$
\end{Prop}
\begin{proof}
  We want $\nu_n$ such that for $f \in  \mathcal{H}^2_{\text{loc}}$ it holds 
  \begin{align*}
    f \cha_{[0,\nu_n]} \in  \mathcal{H}^2
  .\end{align*}
  and 
  \begin{align*}
    \P(\bigcup_{n=1}^{\infty} \{\nu_n = T\}   ) = 1
  .\end{align*}
  We already have 
  \begin{align*}
    \P(\int_0^{T} f^2(*,s) ds < \infty) = 1 
  .\end{align*}
  a natural choice of localizing sequence is then 
  \begin{align*}
    \nu_n = \inf \{t \in  [0,T] : \int_0^{t}  f^2 ds \ge  n\}  
  .\end{align*}
 we have
  \begin{align*}
    \|f\cha_{[0,\nu_n]}\|_{\mathcal{H}^2} < \infty
  .\end{align*}
  since $\nu_n$ conditions on a set such that we only have $\omega$ where we are bounded.
  Since $f$ is adapted the hitting time is a stopping time, and we consider
  \begin{align*}
    \bigcup_{n=1}^{\infty} \{\nu_n = T\}  =  \{\int_0^{T} f^2  < \infty\}   = 1
  .\end{align*}
\end{proof}
\begin{Definition}[3.14]
  Let $f \in  \mathcal{H}^2_{\text{loc}} $ and $\nu_n$ be a localizing sequence for $f$. The It\^o integral process 
  $(\int_0^{t} f(*,s) dB_s )$ is defined as the continuous process $X = (X_t)_{t \in  [0,T]}$ such that 
  \begin{align*}
    \int_0^{t} f(*,s) dB_s = X_t = \lim_{n\to \infty}\int_0^{t} f(*,s) \cha_{[0,\nu_n]}(s) dB_s \quad \P\text{-a.s.}
  .\end{align*}
\end{Definition}
\begin{Theorem}[3.15]
  For $f \in  \mathcal{H}^2_{\text{loc}}$  there exists a continous local martingale $(X_t)_{t \in  [0,T]}$ such that 
  for any localizing sequence $(\nu_n)_{n \in  \mathbb{N}}$ of $f$ it holds 
  \begin{align*}
    \int_0^{t} f(*,s) \cha_{[0,\nu_n]}(s) dB_s \xrightarrow{n\to \infty}  X_t
  .\end{align*}
  for $t \in  [0,T]$. In particular this is well defined and $(X_t)$ does not depend on the choice of localizing sequence.
\end{Theorem}
\begin{proof}
  Remember that any proof involving local Martingales or $\mathcal{H}^2_{\text{loc}}$ processes we need to work through 
  a localizing sequence. Let $f \in  \mathcal{H}_\text{loc}^2$ and $(\nu_n)$ be a corresponding localizing sequence (it exists),
  define the localized integral process 
  \begin{align*}
    X_t^n = \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s)dB_s 
  .\end{align*}
  we first show that $X_t^n$ has a continuous limit $(X_t)$ i.e 
  \begin{align*}
    &X_t = \lim_{n\to \infty}X_t^n\\
    &X_t = \lim_{n\to \infty} \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s) dB_s
  .\end{align*}
  The expression above is only useful if we view it on the set such that 
  \begin{align*}
    &t \mapsto X_t^n(\omega ) \text{ is continous} \\
    &\min \{n \in  \mathbb{N} :  \nu_n(\omega ) = T\} <\infty
  .\end{align*}
  The first gives us that 
  \begin{align*}
    t \mapsto \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s) dB_s
  .\end{align*}
  Show $X$ is continuous \\ 
  For $\epsilon > 0 $ and $t \in  [0,T]$ find $\delta  > 0 $ such that
  \begin{align*}
    s \in  B_\delta(t) \implies \abs{X_t(\omega ) - X_s(\omega )} < \epsilon
  .\end{align*}
  suppose  w.l.o.g $s<t$
  \begin{align*}
    \abs{X_t(\omega ) -X_s(\omega )} &= \abs*{\lim_{n\to \infty} \left(  \int_0^{t} f\cha_{[0,\nu_n]}  - \int_0^{s} f\cha_{[0,\nu_n]} \right)  } \\
                                     &=  \abs*{\lim_{n\to \infty} \int_s^{t} f\cha_{[0,\nu_n]} dB_s} \\ 
                                     &\le \lim_{n\to \infty} \int_s^t \abs{f\cha_{[0,\nu_n]}}  dB_s
  .\end{align*}
  Since $X_t^n$ is continuous the integral can be made arbitrarily small and $f*\cha_{[0,\nu_n]} \in  \mathcal{H}^2$.
  Thus the limit exists and is continuous, for the independence of localizing sequence we get immediately by prop 3.10 (identity) that for $\tau_n \coloneqq  \nu_n \land \tilde{\nu }_n $ 
  \begin{align*}
    X^n \cha_{[0,\tau_m]} =   \tilde{X}^n \cha_{[0,\tau_m ]}
  .\end{align*}
  where 
  \begin{align*}
    \tilde{X}^n = \int_0^{*}  f(*,s)\cha_{[0,\tilde{\nu }_n ]}(s)
  .\end{align*}
then
\begin{align*}
  \lim_{n\to \infty}X^{n}  = \lim_{n\to \infty}\tilde{X}^{n}  
.\end{align*}
on $[0,\tau_m]$ and $\tau_m \uparrow T$.\\[1ex]
It remains to show that $(X_t)$ is a local martingale, 
this is simply to show that $f \in \mathcal{H}^2$ then we already know that $\int  f$ is a martingale,
\begin{align*}
  \sigma_n = \inf \{t \in [0,T]  : \int_0^{t} f^2(*,s) \ge n \}   \land T
.\end{align*}
clearly this is a localizing sequence for $f$ then 
\begin{align*}
  X_{t \land \sigma_n} = \int_{0}^{t} \underbrace{f(*,s)\cha_[0,\sigma_n]}_{\in  \mathcal{H}^2} dB_s
.\end{align*}
and we conclude with Theorem 3.7
\end{proof}
\begin{Theorem}[3.17]
  Let $f,g \in  \mathcal{H}^2_{\text{loc}}$   and $\nu $ be a stopping time such that 
  \begin{align*}
    f\cha_{[0,\nu ]} = g \cha_{[0,\nu ]}
  .\end{align*}
  then 
  \begin{align*}
    \int_0^{t} f(*s)dB_s \cha_{[0,\nu ]}  \myS{\P}{=} \int_0^{t} g(*s)dB_s \cha_{[0,\nu ]}  
  .\end{align*}
\end{Theorem}
\begin{proof}
 You know the drill, localizing sequence and then apply the result for the normal, we have that 
 \begin{align*}
   \tau_n  = \inf \{t \in  [0,T] : \int_0^{t} f(*,s)^2 ds \ge  n \lor \int_0^{t} g(*,s)^2 ds \ge n   \}  \land T
 .\end{align*}
 Clearly $f\cha_{[0,\tau_n]} \in  \mathcal{H}^2$ since we stop the moment one of the integral processes exceeds $n$ then we have 
 \begin{align*}
   X^{n}  &= \int_0^{*}  \underbrace{f(*,s)\cha_{[0,\tau_n]}}_{\in  \mathcal{H}^2} \\
   Y^{n}  &= \int_0^{*}  \underbrace{g(*,s)\cha_{[0,\tau_n]}}_{\in  \mathcal{H}^2} 
 .\end{align*}
 then prop 3.10 gives 
 \begin{align*}
   X^{n}\cha_{[0,\nu ]} = Y^{n} \cha_{[0,\nu ]}
 .\end{align*}
 Theorem 3.15 gives us the existence of the limit 
 \begin{align*}
   \lim_{n\to \infty}X^{n} 
 .\end{align*}
\end{proof}
\begin{Theorem}[3.17 Riemann sum approximation]
 If $f : \mathbb{R} \to \mathbb{R} $  is a continuous function and  $t_i = \frac{i}{n}T$ then for $n\to \infty$ we have
 \begin{align*}
   \sum_{i=1}^{n} f(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) \xrightarrow{\P} \int_0^{T} f(B_s) dB_s
 .\end{align*}
\end{Theorem}
\begin{proof}
By Remark 3.12. we know that for any continuous function $g : \mathbb{R} \to  \mathbb{R}$ 
\begin{align*}
  f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} 
.\end{align*}
This follows since for a.s. $\omega  \in  \Omega $ the map 
\[\phi(\omega ) : [0,T] \to  \mathbb{R} : t \mapsto B_t(\omega )\]
is bounded. This gives us that 
\begin{align*}
  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))} = \sup_{x \le \abs{m}} \abs{g(x)} \le  C
.\end{align*}
Where the last inequality follows from the fact that $g$ is continuous and attains a maximum on the compact set $[-m,m]$ then we can check that 
for 
\begin{align*}
  \omega  \in  \{\phi \text{ is bounded } \}  
.\end{align*}
The integral 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &\le  \int_0^{T}  \abs{g(B_t(\omega ))}\abs{g(B_t(\omega ))} dt\\
                                   &\le  \underbrace{\sup_{t \in [0,T]} \abs{g(B_t(\omega ))}}_{\le C} \int_0^{T}  \abs{g(B_t(\omega ))} dt\\
                                   &\le  C^2T
.\end{align*}
Since $\P(\{\phi \text{ is bounded } \}) = 1$ we get  immediately 
\begin{align*}
  \P(\int_0^{T} g^2(B_t(\omega ))  < \infty) = 1
.\end{align*}
This tells us that  for any continuous $f$ and Brownian motion $B$ 
\begin{align*}
  f(B) \in  \mathcal{H}_{\text{loc}}^2
.\end{align*}
we can rewrite $\{\phi \text{ is bounded } \}$ as a stopping time instead and get 
\begin{align*}
  \tau_m = \inf \{t \in [0,T] :  \abs{B_t} \ge  m\}  
.\end{align*}
which is a localizing sequence for $f(B)$ since by similar argument to above we have 
\begin{align*}
  \abs{f(B_{* \land \tau_m})} \le  \sup_{\abs{x} \le m} \abs{f(x)} < \infty
.\end{align*}
and we get 
\begin{align*}
  f_m = f*\cha_{[-m,m]}  = f\rvert_{[-m,m]}
.\end{align*}
Where 
\begin{align*}
  f_m(B) \in  \mathcal{H}^2
.\end{align*}
By definition of the It\^o integral for $f \in \mathcal{H}^2$ we already get that  
\begin{align*}
  I(f_m^{(n)} ) = \sum_{i=1}^{n} a_i (B_{t_{i}}  - B_{t_{i-1}}) \xrightarrow{L^2} \int_0^{T}  f_m(B_t) dt 
.\end{align*}
where $L^2$ convergence implies $\P$ convergence. \\[1ex]
Thus our goal in Step 2 is to show that  in fact
\begin{align*}
  f_m^{(n)}   = \sum_{i=1}^{n}  f_m(B_{t_{i-1}})(\omega )\cha_{(t_{i-1},t_i]}(s) 
.\end{align*}
we clearly have
\begin{align*}
  f_m^{(n)}   \in  \mathcal{H}_{0}^2
.\end{align*}
Then it  remains to show $f_m^{(n)} \xrightarrow{\mathcal{H}^{2} } f_m $ 
% \begin{align*}
%   \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[\int_0^{T} 2\sum_{i=1}^{n} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_0^{T} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 \cha_{(t_{i-1},t_i]}(s)]\marginnote{$t_i$ is partition of $[0,T]$}\\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
%                                           &\le   2\sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
%                                           &\le   2 \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
% .\end{align*}
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(B_s))^2 ] \\
                                          &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - \sum_{i=1}^{n} f_m(B_s) \cha_{t_{{i-1}},t_i})^2 ] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 \\
                                          &+ \underbrace{\sum_{i,j=1}^{n} \underbrace{(f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))(f_m(B_{t_{j-1}})-f_{m}(B_s)\cha_{(t_{j-1},t_j]}(s))}_{{[t_{i-1}},t_i] \cap [t_{j-1},t_{j}] = \emptyset }}_{=0}  ds \bigg] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 ds] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(B_s))^2 ] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
                                          &\le   \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
                                          &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
.\end{align*}
Where we can bound 
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 
.\end{align*}
further by considering that $f$ is continuous and thus for 
\begin{align*}  
  \mu_{f_m}(h)\coloneqq  \sup \{\abs{f_m(x)-f_m(y)} : x,y \in  \mathbb{R} \text{ with } \abs{x-y} \le h \}
.\end{align*}
we get that
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2  \le \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})  
.\end{align*}
putting it together
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
                                          &\le  \frac{T}{n} \sum_{i=1}^{n} \E[ \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  \frac{T}{n} \E[n* \mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  T \E[\mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
.\end{align*}
Since $f_m$ is continuous the modulus of continuity must tend to 0 as $n\to \infty$.
Thus we have shown that $f_m^{(n)} \xrightarrow{\mathcal{H}^2} f_m  \implies I(f_m^{(n)} ) \xrightarrow{L^2} I(f_m)$\\
Now on the set $\{\tau_m = T\}  $ we have 
\begin{align*}
  f(B) = f_m(B) 
.\end{align*}
and  by persistence of identity also 
\begin{align*}
  \int_0^{T} f(B_s) dB_s = \int_0^{T}   f_m(B_s) dB_s
.\end{align*}
For 
\begin{align*}
  A_{n,\epsilon } = \{\abs{\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  - \int_0^{T} f(B_s) dB_s } \ge  \epsilon\}  
.\end{align*}
Then we get 
\begin{align*}
\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  \xrightarrow{\P} \int_0^{T} f(B_s) dB_s 
.\end{align*}
if $\P(A_{n,\epsilon}) \to 0$ 
\begin{align*}
  \P(A_{n,\epsilon}) &= \P(A_{n,\epsilon} \cap \{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \marginnote{This inequality is just $\P(A) \le  \P(B)$ if $A \subset B$ } \\
                     &\le  \P(\{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \\ 
                     &\xrightarrow{n\to \infty} 0 
.\end{align*}
\end{proof}
\spewnotes
\begin{remark}[3.12]
  For any continuous $g: \mathbb{R}\to \mathbb{R}$  we have $f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} $  since $B$ is a.s. pathwise
  bounded on $[0,T]$
\end{remark}
\begin{proof}
 Consider $\omega \in  \Omega $ a.s., then 
 \begin{align*}
   \sup_{t \in  [0,T]}\abs{g(B_t(\omega ))} \le C 
 .\end{align*}
 for some $C\ge 0$, then we have 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &=  \int_0^{T}  g(B_t(\omega ))g(B_t(\omega)) dt\\
                                   &\le \int_0^{T}  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}* \abs{g(B_t(\omega))} dt\\
                                   &\le  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}\int_0^{T} \abs{g(B_t(\omega))} dt\\
                                   &\le C^2*T 
.\end{align*}
\end{proof}
\begin{Theorem}[3.18]
 For any twice continuous differentiable function $f : \mathbb{R} \to \mathbb{R}$  we have 
 \begin{align*}
   f(B_t) = f(0) + \int_0^{t}  f'(B_s)dB_s + \frac{1}{2} \int_0^{t} f^{''}(B_s) ds  \quad \P \text{-a.s.}
 .\end{align*}
\end{Theorem}
\begin{proof}
 The main tools used are a second order Taylor expansion then using our Riemann Sum convergence for the first integral 
 and for the second we have a normal (pointwise) Riemann sum.\\
 We write 
 \begin{align*}
   f(B_t) - f(0) &= \sum_{i=0}^{n-1} f(B_{t_i})-f(B_(t_{i-1})))\\
                 &= \sum_{i=0}^{n-1} f'(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) + \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})^2 + \sum_{i=0}^{n} r(B_{t_i},B_{t_{i-1}}) 
 .\end{align*}
 The first sum converges by 3.17  to 
 \begin{align*}
  \sum_{i=0}^{n-1} f'(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) \to  \int f'(B_s) dB_s
 .\end{align*}
 The second one we write as 
 \begin{align*}
   \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})^2 = \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})((B_{t_i}-B_{t_{i-1}})^2-(t_i-t_{i-1})) + \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(t_i-t_{i-1})
 .\end{align*}
 Then the second term is the integral we want i.e.
 \begin{align*}
   \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(t_i-t_{i-1}) \to  \frac{1}{2}\int f''(B_{s}) ds
 .\end{align*}
 Such that we need to show the first part converges against 0 $\P$-a.s., we do so by showing it converges to 0 in $L^2$ instead
 where we again make use of the independence of Brownian increments , and that they have mean 0, plus the fact that 
 \begin{align*}
   \E[(B_{t_i}-B_{t_{i-1}})^2] = t_i - t_{i-1}
 .\end{align*}
 the remainder term is a little more complicated, we rewrite 
 \begin{align*}
   r(x,y) &= \int_{x}^{y} (y-u)(f''(u)-f''(x)) du\\
          &= (y-x)^2 \int_0^{1} (1-t)(f''(x+t(y-x))f''(x))dt
 .\end{align*}
 then  if $f$ has compact support it holds 
 \begin{align*}
  \abs{r(x,y)} \le \abs{y-x}^2 \abs{h(x,y)}
 .\end{align*}
 for bounded $h$ with $h(x,x) = 0$ and compact support (that is $\supp f$),
 we show that the error term converges to 0 in $L^{1} $ by bounding $h$, that consists of splitting up $\Omega $ as follows
 \begin{align*}
  \Omega  = \{\abs{x-y} < \delta  \}  \cup \{\abs{x-y} \ge \delta \}  
 .\end{align*}
 on the first set we know by continuity $h(x,y) < \epsilon$ on the second we bound by using the $\|h\|_{\infty}$ which exists
 since $h$ continuous and of compact support,bounding the probability of the second set by Markov inequality ($f(x) = x^2$).
 Since we can choose $\delta $ as we want we may take $\epsilon=0$\\[1ex]
 Now we need to argue that we are allowed to assume $f$ compact support, this follows by similar argument to Riemann approximation.
\end{proof}
\begin{example}[5.1]
 Consider the SDE 
 \begin{align*}
  dX_t = \mu X_t dt + \sigma X_t dB_t
 .\end{align*}
 then we can solve this SDE by making the ansatz $X_t = f(B_t,t)$ and using It\^os formula
 \begin{align*}
   df(B_t,t) &= f(0,0) + f_x(B_t)dB_t + (\frac{1}{2}f_{x x}+f_{t}) dt\\
           &\triangleq  \mu  f(B_t) dt + \sigma f(B_t)dB_t
 .\end{align*}
 This implies 
 \begin{align*}
  f_x(B_t) = \sigma  f
 .\end{align*}
 Such that 
 \begin{align*}
  f = \exp(\sigma*x + g(t))
 .\end{align*}
 then 
 \begin{align*}
   g'(t)*f + \frac{1}{2} \sigma^2 f &= \mu  f\\
                                g'(t)   &= \mu  - \frac{\sigma^2}{2}\\
                                g(t)&= (\mu  - \frac{\sigma^2}{2})t + g_{0}
 .\end{align*}
 Which gives the solution 
 \begin{align*}
  X_t = \exp(\sigma B_t + (\mu -\frac{\sigma^2}{2})t + g_{0})
 .\end{align*}
\end{example}
\begin{Definition}
 In general a linear SDE has the form
 \begin{align*}
  X_t = (\alpha (t)X_t + \beta(t) )dt + (\phi(t)X_t + \psi(t))dB_t
 .\end{align*}
 and a solution is given by 
 \begin{align*}
  X_t  = x_{0}\exp(Y_t) + \int_0^{t} \exp(Y_t-Y_s)(\beta(s)-\psi(s)\phi(s))  ds + \int_0^{t}  \exp(Y_t-Y_s)\psi(s)dB_s
 .\end{align*}
 Where
 \begin{align*}
  Y_t = \int_0^{t} \phi(s) dB_s + \int_0^{t}(\alpha(s)-\frac{1}{2}\phi^2(s))   ds
 .\end{align*}
\end{Definition}
