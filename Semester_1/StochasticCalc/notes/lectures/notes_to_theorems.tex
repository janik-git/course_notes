
\begin{Lemma}[3.2 It\^o s isometry simple version]
 For $f \in  \mathcal{H}_0^{2} $  we have 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \|I(f)\|_{L^2}
 .\end{align*}
\end{Lemma}
\begin{proof}
 We have 
 \begin{align*}
   \|I(f)\|_{L^2} &= \E[(\sum_{i=1}^{n} a_i (B_{t_i} - B_{t_{i-1}}))^2]\\
                  &= \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2 + \sum_{i\neq j}^{n} a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ]\\
                  &=  \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[\E[a_i^2(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2]\E[(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2](t_i-t_{i-1})\\
 .\end{align*}
 Since  w.l.o.g take $i<j$
 \begin{align*}
   \E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ] &= \E[\E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
                                                              &= \E[a_i \underbrace{\E[(B_{t_{i}}-B_{t_{i-1}})]}_{=0}\E[a_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
 .\end{align*}
 But 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \E[\int_0^{T} f^2 ds ] = \sum_{i=1}^{n}\E[a_i^2](t_i-t_{i-1})
 .\end{align*}
\end{proof}
\begin{Prop}[3.5]
  For every $f \in  \mathcal{H}^2$  there exists a sequence $(f_n)_{n \in  \mathbb{N}}) \subset  \mathcal{H}_0^{2} $ such that 
  \begin{align*}
    \|f_n - f\|_{\mathcal{H}^2}\xrightarrow{n\to \infty} 0
  .\end{align*}
\end{Prop}
\begin{proof}
 The rough outline of the proof can be split up as follows
 \begin{enumerate}
  \item Show that we can assume $f$ is bounded 
  \item Show that we can assume $f$ is bounded adapted and continuous
  \item Construct simple sequence that approximates $f$
 \end{enumerate}
 For step 1 we can simply consider that for any (potentially ) unbounded function $f$ 
 \begin{align*}
  f_n = -n \lor (f \land n)
 .\end{align*}
 is bounded and apply DCT
 \begin{align*}
   \lim_{n\to \infty}\|f_n - f\|_{\mathcal{H}^2} = \lim_{n\to \infty}\E[\int_0^{T}(f_n-f)^2 dt ] =  \E[\int_0^{T} \lim_{n \to \infty}(f_n-f)^2 dt ] = 0
 .\end{align*}
 Thus we may assume $f$ bounded \\[1ex]
 For step 2 we can use that we can get the height of a rectangle by dividing by its base s.t.
 \begin{align*}
   f_n(*,t) \coloneqq  \frac{1}{(t-(t-\frac{1}{n})_+)} \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds =  n \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds
 .\end{align*}
 Now we note that since we work with random variables we condition $\omega $ on the set where
 \begin{align*}
   \lim_{n\to \infty}  f_n(\omega ,t) = f(\omega ,t)
 .\end{align*}
 We need this set to have measure 1, such that 
 \begin{align*}
   A \coloneqq  \{(\omega ,t) \in  \Omega  \times  [0,T] : \lim_{n\to \infty} f_n(\omega ,t) \neq  f(\omega ,t)\}  
 .\end{align*}
 Has measure zero with respect to $\P \otimes \lambda $, we have by the fundamental theorem of calculus that
 \begin{align*}
   \lambda(\{t \in  [0,T] : (\omega,t) \in  A\}) = 0
 .\end{align*}
Or rather Lebesgue Differentiation theorem, otherwise we'd need the condition that $f(\omega ,*)$ only has countable many discontinuities.
Thus we get that we may assume $f$ is bounded and continuous.\\[1ex]
We now construct our simple function $f_n$ as 
\begin{align*}
  f_{n,s}(\omega ,t) \coloneqq  f(\omega ,(s+\phi_n(t-s)_+))
.\end{align*}
where
\begin{align*}
  \phi_n = \sum_{i=1}^{2^n} \frac{j-1}{2^{n} }\cha_{(\frac{j-1}{2^{n}  },\frac{j}{2^{n} }]}
.\end{align*}
makes our time interval discrete (standard argument really), then we wanna show 
\begin{align*}
  \|f_n-f\|_{\mathcal{H}^2}&= \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] \to 0
.\end{align*}
We have 
\begin{align*}
  \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] &\le \E[\int_0^{T} \int_0^{1} \abs{f_{n,s}-f}^2 ds dt]\\
                                       &= \E[\int_0^{T} \int_0^{1} \abs{f(*,(s+\phi_n(t-s))_+)-f}^2 ds dt]\\
                                       &=  \sum_{j\in \mathbb{Z}}\E[\int_0^{T} \int_{[t-\frac{j}{2^{n} },t-\frac{j-1}{2^{n} }] \cap [0,1]} \abs{f(*,(s+\frac{j-1}{2^{n} })-f}^2 ds dt]\\
                                       &\le (2^{n}+1 )2^{-n}  \int_{(0,1]} \E[\int_0^{T}\abs{f(*,t-2^{-n}h )-f(*,t)}^2 dt ]dh 
.\end{align*}
Where we can show that the Expectation term goes to 0
\end{proof}
\begin{Theorem}[3.7.]
  For any $f \in  \mathcal{H}^2$ there is a continuous martingale $X = (X_t)_{t \in  [0,T]}$ with 
  respect to $\mathcal{F}_t$ such that for all $t \in  [0,T]$
  \begin{align*}
    X_t = I(f \cha_{[0,t]})
  .\end{align*} 
\end{Theorem}
\begin{proof}
 We first consider the simple case with 
 \begin{align*}
   f_n = \sum_{i=0}^{m_n - 1} a_i^{n}  \cha_{(t_i^{n},t_{i+1}^{n}  ]}
 .\end{align*}
 Then 
 \begin{align*}
   \E[X_{t}^{n} - X_{s}^{n} | \mathcal{F}_s  ] &= \E[\sum_{t_i > s} a_i(B_{t_{i+1}}-B_{t_i})]\\
                                               &=  0
 .\end{align*}
Our end goal is to use a triangular inequality to use the simple case to bound the normal one i.e.
\begin{align*}
  \abs{\E[X_t-X_s|\mathcal{F}_s]} &= \abs{\E[X_t-X_{t}^{n}+X_{t}^{n} -X_s +X_{s}^{n} -X_{s}^{n}  |\mathcal{F}_s]}\\
                                  &\le \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \underbrace{\abs{\E[X_{t}^{n} -X_{s}^{n}  |\mathcal{F}_s]}}_{=0} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}\\
                                  &= \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}\\
                                  &\myS{Jen}{\le } \E[\abs{X_t-X_{t}^{n}}|\mathcal{F}_s] + \E[\abs{X_{s}^{n}-X_s}|\mathcal{F}_s ]\\
.\end{align*}
We consider $A \in  \mathcal{F}_s$
\begin{align*}
  \E[\abs{X_t-X_{t}^{n}}\cha_A] + \E[\abs{X_{s}^{n}-X_s}\cha_A ] &\le \E[\abs{X_t-X_{t}^{n}}] + \E[\abs{X_{s}^{n}-X_s} ]\\
                                                                 &\le 2*\|f-f_n\|_{\mathcal{H}^2}
.\end{align*}
So we have $\E[X_t |\mathcal{F}_s] = X_s$\\[1ex]
For $(f_n)_{n \in  \mathbb{N}} \subset \mathcal{H}^2_0 $ we have 
\begin{align*}
  \lim_{n\to \infty}f_n = f \in \mathcal{H}^2
.\end{align*}
And 
\begin{align*}
  \lim_{n \to \infty} I(f_n) = I(f) \in L^2
.\end{align*}
Which should be equivalent to for fixed $t \in  [0,T]$
\begin{align*}
  X_t^n &\xrightarrow{L^2}  X_t \\
  I(f_n\cha_{[0,t]}) &\xrightarrow{L^2} I(f\cha_{[0,t]})
.\end{align*}
We need to make the argument uniform in $t \in  [0,T]$, which is Step 2 in the script i guess.\\[1ex]
We have
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  \epsilon)  \le  \epsilon^{-2} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2}
.\end{align*}
since  for all $p>1$
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t} \ge  \epsilon) \frac{1}{\epsilon^p} \E[\abs{X}_T^p]
.\end{align*}
By choosing a subequence we can get 
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} )  \le  2^{2k} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2} \le  2^{-k} 
.\end{align*}
Then we can apply Borel-Cantelli since
\begin{align*}
  \sum_{k=0}^{\infty}   \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} ) < \infty
.\end{align*}
and get $\Omega_0 \in  \mathcal{F}$ such that $\P(\Omega_0) = 1$ and $X^{n_k} $ is a pathwise cauchy sequence.
\end{proof}
\begin{Prop}[3.10]
 Let $f \in  \mathcal{H}^2$ and $\nu $ be a stopping time satisfying 
 \begin{align*}
   f \cha_{[0,\nu]} = 0
 .\end{align*}
 The integral process $X=(X_t)_{t \in  [0,T]}$ with $X_t = \int_0^{t} f(*,s) dB_s $, the fulfills 
 \begin{align*}
   X \cha_{[0,\nu ]} = 0
 .\end{align*}
 In particular for two functions $f,g \in  \mathcal{H}^2$ with $f \cha_{[0,\nu ]} = g \cha_{[0,\nu ]}$ the integral processes coincide on $[0,\nu ]$
\end{Prop}
\begin{remark}
  This proposition is mostly used to prove that the same holds for $\mathcal{H}^2_{\text{loc}}$  functions as well since this allows us 
  to use localizing sequences $\tau_m$ and use the fact that on 
  \begin{align*}
    \{\tau_m = T\}  
  .\end{align*}
  The processes must coincide.
\end{remark}
\begin{proof}
 The proof follows similarly to before where we first prove the simple case, for that suppose
 \begin{align*}
   X_t &= I(f\cha_{[0,t]})\\
   Y_t &= I(f\cha_{[0,\nu ]}\cha_{[0,t]})
 .\end{align*}
 Then it suffices to consider the simplification $f = a\cha_[(r,s]] $ for $0\le r < s \le T$.\\
 The important thing to note now is that 
 \begin{align*}
   Y_t =  I(f\cha_{[0,\nu ]}\cha_{[0,t]}) = \int_0^{t}\underbrace{(f \cha_{[0,\nu ]}}_{\notin \mathcal{H}_0^2}
 .\end{align*}
 For a function $h = f\cha_{[0,\nu ]}$ to be in $\mathcal{H}_0^2$ there needs to be a representation 
 \begin{align*}
   h =  \sum_{i=0}^{n-1} a_i \cha_{[t_{i-1},t_{i}]}  
 .\end{align*}
 But $\nu$ is continuous, such that there cannot exist such a representation, which means we first 
 need to consider the discrete stopping time $\nu^n$ as follows 
 \begin{align*}
   s_{i,n} &= r+(s-r)\frac{i}{2^{n} }\\
   \nu ^{n} &= \sum_{i=0}^{2^{n} - 1 } s_{i+1,n}\cha_{(s_{i,n},s_{i+1,n}]}(\nu )
 .\end{align*}
 We show 
 \begin{align*}
   f \cha_{[0,\nu ^{n} ]} &=  f- f \cha_{\nu ^{n},T }\\
                          &= f - f \sum_{i=0}^{2^{n} - 1 } \cha_{(s_{i,n},s_{i+1,n}]}(\nu)\cha_{(s_{i+1,n},T]} \in  \mathcal{H}_0^{2} 
 .\end{align*}
 then by definition of the integral for simple functions it follows
 \begin{align*}
   Y_t^N = \int_0^{t} f(*,s) \cha_{[0,\nu^n]}(u) dB_u = a(B_{s \land \nu ^{n} \land t } - B_{r \land \nu ^{n} \land t })
 .\end{align*}
 And by continuity of $B$ we can do
 \begin{align*}
   Y_t  = \lim_{n\to \infty}Y_t^n = a(B_{s \land \nu \land t } - B_{r \land \nu  \land t })
 .\end{align*}
 But this is clearly $X_t\cha_{[0,\nu ]}$, it follows
 \begin{align*}
   X\cha_{[0,\nu ]} =  Y\cha_{[0,\nu ]}
 .\end{align*}
 Note that for $X\cha_{[0,\nu ]}$  there is no difficulty since we can first apply the definition of the simple integral,
 and then consider the stopping time\\[1ex]
 For general $f \in  \mathcal{H}^2$ we choose $(f_n) \subset  \mathcal{H}_0^{2} $ such that 
 \begin{align*}
   \|f -f_n\|_{\mathcal{H}^2} \to 0
 .\end{align*}
 then we already know that $X^n = Y^n$ such that 
 \begin{align*}
   X \cha_{[0,\nu ]} = \lim_{n\to \infty}X^n\cha_{[0,\nu ]} = \lim_{n \to \infty} Y^n\cha_{[0,\nu ]} = Y \cha_{[0,\nu ]}
 .\end{align*}
\end{proof}
\begin{Prop}[3.13]
  For every $f \in  \mathcal{H}^2_{\text{loc}}$  there is a localizing sequence $(\nu_n)_{n \in  \mathbb{N}}$
\end{Prop}
\begin{proof}
  We want $\nu_n$ such that for $f \in  \mathcal{H}^2_{\text{loc}}$ it holds 
  \begin{align*}
    f \cha_{[0,\nu_n]} \in  \mathcal{H}^2
  .\end{align*}
  and 
  \begin{align*}
    \P(\bigcup_{n=1}^{\infty} \{\nu_n = T\}   ) = 1
  .\end{align*}
  We already have 
  \begin{align*}
    \P(\int_0^{T} f^2(*,s) ds < \infty) = 1 
  .\end{align*}
  a natural choice of localizing sequence is then 
  \begin{align*}
    \nu_n = \inf \{t \in  [0,T] : \int_0^{t}  f^2 ds \ge  n\}  
  .\end{align*}
 we have
  \begin{align*}
    \|f\cha_{[0,\nu_n]}\|_{\mathcal{H}^2} < \infty
  .\end{align*}
  since $\nu_n$ conditions on a set such that we only have $\omega$ where we are bounded.
  Since $f$ is adapted the hitting time is a stopping time, and we consider
  \begin{align*}
    \bigcup_{n=1}^{\infty} \{\nu_n = T\}  =  \{\int_0^{T} f^2  < \infty\}   = 1
  .\end{align*}
\end{proof}
\begin{Definition}[3.14]
  Let $f \in  \mathcal{H}^2_{\text{loc}} $ and $\nu_n$ be a localizing sequence for $f$. The It\^o integral process 
  $(\int_0^{t} f(*,s) dB_s )$ is defined as the continuous process $X = (X_t)_{t \in  [0,T]}$ such that 
  \begin{align*}
    \int_0^{t} f(*,s) dB_s = X_t = \lim_{n\to \infty}\int_0^{t} f(*,s) \cha_{[0,\nu_n]}(s) dB_s \quad \P\text{-a.s.}
  .\end{align*}
\end{Definition}
\begin{Theorem}[3.15]
  For $f \in  \mathcal{H}^2_{\text{loc}}$  there exists a continous local martingale $(X_t)_{t \in  [0,T]}$ such that 
  for any localizing sequence $(\nu_n)_{n \in  \mathbb{N}}$ of $f$ it holds 
  \begin{align*}
    \int_0^{t} f(*,s) \cha_{[0,\nu_n]}(s) dB_s \xrightarrow{n\to \infty}  X_t
  .\end{align*}
  for $t \in  [0,T]$. In particular this is well defined and $(X_t)$ does not depend on the choice of localizing sequence.
\end{Theorem}
\begin{proof}
  Remember that any proof involving local Martingales or $\mathcal{H}^2_{\text{loc}}$ processes we need to work through 
  a localizing sequence. Let $f \in  \mathcal{H}_\text{loc}^2$ and $(\nu_n)$ be a corresponding localizing sequence (it exists),
  define the localized integral process 
  \begin{align*}
    X_t^n = \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s)dB_s 
  .\end{align*}
  we first show that $X_t^n$ has a continuous limit $(X_t)$ i.e 
  \begin{align*}
    &X_t = \lim_{n\to \infty}X_t^n\\
    &X_t = \lim_{n\to \infty} \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s) dB_s
  .\end{align*}
  The expression above is only useful if we view it on the set such that 
  \begin{align*}
    &t \mapsto X_t^n(\omega ) \text{ is continous} \\
    &\min \{n \in  \mathbb{N} :  \nu_n(\omega ) = T\} <\infty
  .\end{align*}
  The first gives us that 
  \begin{align*}
    t \mapsto \int_0^{t} f(*,s)\cha_{[0,\nu_n]}(s) dB_s
  .\end{align*}
  Show $X$ is continuous \\ 
  For $\epsilon > 0 $ and $t \in  [0,T]$ find $\delta  > 0 $ such that
  \begin{align*}
    s \in  B_\delta(t) \implies \abs{X_t(\omega ) - X_s(\omega )} < \epsilon
  .\end{align*}
  suppose  w.l.o.g $s<t$
  \begin{align*}
    \abs{X_t(\omega ) -X_s(\omega )} &= \abs*{\lim_{n\to \infty} \left(  \int_0^{t} f\cha_{[0,\nu_n]}  - \int_0^{s} f\cha_{[0,\nu_n]} \right)  } \\
                                     &=  \abs*{\lim_{n\to \infty} \int_s^{t} f\cha_{[0,\nu_n]} dB_s} \\ 
                                     &\le \lim_{n\to \infty} \int_s^t \abs{f\cha_{[0,\nu_n]}}  dB_s
  .\end{align*}
  Since $X_t^n$ is continuous the integral can be made arbitrarily small and $f*\cha_{[0,\nu_n]} \in  \mathcal{H}^2$.
  Thus the limit exists and is continuous, for the independence of localizing sequence we get immediately by prop 3.10 (identity) that for $\tau_n \coloneqq  \nu_n \land \tilde{\nu }_n $ 
  \begin{align*}
    X^n \cha_{[0,\tau_m]} =   \tilde{X}^n \cha_{[0,\tau_m ]}
  .\end{align*}
  where 
  \begin{align*}
    \tilde{X}^n = \int_0^{*}  f(*,s)\cha_{[0,\tilde{\nu }_n ]}(s)
  .\end{align*}
then
\begin{align*}
  \lim_{n\to \infty}X^{n}  = \lim_{n\to \infty}\tilde{X}^{n}  
.\end{align*}
on $[0,\tau_m]$ and $\tau_m \uparrow T$.\\[1ex]
It remains to show that $(X_t)$ is a local martingale, 
this is simply to show that $f \in \mathcal{H}^2$ then we already know that $\int  f$ is a martingale,
\begin{align*}
  \sigma_n = \inf \{t \in [0,T]  : \int_0^{t} f^2(*,s) \ge n \}   \land T
.\end{align*}
clearly this is a localizing sequence for $f$ then 
\begin{align*}
  X_{t \land \sigma_n} = \int_{0}^{t} \underbrace{f(*,s)\cha_[0,\sigma_n]}_{\in  \mathcal{H}^2} dB_s
.\end{align*}
and we conclude with Theorem 3.7
\end{proof}
\begin{Theorem}[3.17]
  Let $f,g \in  \mathcal{H}^2_{\text{loc}}$   and $\nu $ be a stopping time such that 
  \begin{align*}
    f\cha_{[0,\nu ]} = g \cha_{[0,\nu ]}
  .\end{align*}
  then 
  \begin{align*}
    \int_0^{t} f(*s)dB_s \cha_{[0,\nu ]}  \myS{\P}{=} \int_0^{t} g(*s)dB_s \cha_{[0,\nu ]}  
  .\end{align*}
\end{Theorem}
\begin{proof}
 You know the drill, localizing sequence and then apply the result for the normal, we have that 
 \begin{align*}
   \tau_n  = \inf \{t \in  [0,T] : \int_0^{t} f(*,s)^2 ds \ge  n \lor \int_0^{t} g(*,s)^2 ds \ge n   \}  \land T
 .\end{align*}
 Clearly $f\cha_{[0,\tau_n]} \in  \mathcal{H}^2$ since we stop the moment one of the integral processes exceeds $n$ then we have 
 \begin{align*}
   X^{n}  &= \int_0^{*}  \underbrace{f(*,s)\cha_{[0,\tau_n]}}_{\in  \mathcal{H}^2} \\
   Y^{n}  &= \int_0^{*}  \underbrace{g(*,s)\cha_{[0,\tau_n]}}_{\in  \mathcal{H}^2} 
 .\end{align*}
 then prop 3.10 gives 
 \begin{align*}
   X^{n}\cha_{[0,\nu ]} = Y^{n} \cha_{[0,\nu ]}
 .\end{align*}
 Theorem 3.15 gives us the existence of the limit 
 \begin{align*}
   \lim_{n\to \infty}X^{n} 
 .\end{align*}
\end{proof}
\begin{Theorem}[3.17 Riemann sum approximation]
 If $f : \mathbb{R} \to \mathbb{R} $  is a continuous function and  $t_i = \frac{i}{n}T$ then for $n\to \infty$ we have
 \begin{align*}
   \sum_{i=1}^{n} f(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) \xrightarrow{\P} \int_0^{T} f(B_s) dB_s
 .\end{align*}
\end{Theorem}
\begin{proof}
By Remark 3.12. we know that for any continuous function $g : \mathbb{R} \to  \mathbb{R}$ 
\begin{align*}
  f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} 
.\end{align*}
This follows since for a.s. $\omega  \in  \Omega $ the map 
\[\phi(\omega ) : [0,T] \to  \mathbb{R} : t \mapsto B_t(\omega )\]
is bounded. This gives us that 
\begin{align*}
  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))} = \sup_{x \le \abs{m}} \abs{g(x)} \le  C
.\end{align*}
Where the last inequality follows from the fact that $g$ is continuous and attains a maximum on the compact set $[-m,m]$ then we can check that 
for 
\begin{align*}
  \omega  \in  \{\phi \text{ is bounded } \}  
.\end{align*}
The integral 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &\le  \int_0^{T}  \abs{g(B_t(\omega ))}\abs{g(B_t(\omega ))} dt\\
                                   &\le  \underbrace{\sup_{t \in [0,T]} \abs{g(B_t(\omega ))}}_{\le C} \int_0^{T}  \abs{g(B_t(\omega ))} dt\\
                                   &\le  C^2T
.\end{align*}
Since $\P(\{\phi \text{ is bounded } \}) = 1$ we get  immediately 
\begin{align*}
  \P(\int_0^{T} g^2(B_t(\omega ))  < \infty) = 1
.\end{align*}
This tells us that  for any continuous $f$ and Brownian motion $B$ 
\begin{align*}
  f(B) \in  \mathcal{H}_{\text{loc}}^2
.\end{align*}
we can rewrite $\{\phi \text{ is bounded } \}$ as a stopping time instead and get 
\begin{align*}
  \tau_m = \inf \{t \in [0,T] :  \abs{B_t} \ge  m\}  
.\end{align*}
which is a localizing sequence for $f(B)$ since by similar argument to above we have 
\begin{align*}
  \abs{f(B_{* \land \tau_m})} \le  \sup_{\abs{x} \le m} \abs{f(x)} < \infty
.\end{align*}
and we get 
\begin{align*}
  f_m = f*\cha_{[-m,m]}  = f\rvert_{[-m,m]}
.\end{align*}
Where 
\begin{align*}
  f_m(B) \in  \mathcal{H}^2
.\end{align*}
By definition of the It\^o integral for $f \in \mathcal{H}^2$ we already get that  
\begin{align*}
  I(f_m^{(n)} ) = \sum_{i=1}^{n} a_i (B_{t_{i}}  - B_{t_{i-1}}) \xrightarrow{L^2} \int_0^{T}  f_m(B_t) dt 
.\end{align*}
where $L^2$ convergence implies $\P$ convergence. \\[1ex]
Thus our goal in Step 2 is to show that  in fact
\begin{align*}
  f_m^{(n)}   = \sum_{i=1}^{n}  f_m(B_{t_{i-1}})(\omega )\cha_{(t_{i-1},t_i]}(s) 
.\end{align*}
we clearly have
\begin{align*}
  f_m^{(n)}   \in  \mathcal{H}_{0}^2
.\end{align*}
Then it  remains to show $f_m^{(n)} \xrightarrow{\mathcal{H}^{2} } f_m $ 
% \begin{align*}
%   \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[\int_0^{T} 2\sum_{i=1}^{n} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_0^{T} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 \cha_{(t_{i-1},t_i]}(s)]\marginnote{$t_i$ is partition of $[0,T]$}\\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
%                                           &\le   2\sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
%                                           &\le   2 \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
% .\end{align*}
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(B_s))^2 ] \\
                                          &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - \sum_{i=1}^{n} f_m(B_s) \cha_{t_{{i-1}},t_i})^2 ] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 \\
                                          &+ \underbrace{\sum_{i,j=1}^{n} \underbrace{(f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))(f_m(B_{t_{j-1}})-f_{m}(B_s)\cha_{(t_{j-1},t_j]}(s))}_{{[t_{i-1}},t_i] \cap [t_{j-1},t_{j}] = \emptyset }}_{=0}  ds \bigg] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 ds] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(B_s))^2 ] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
                                          &\le   \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
                                          &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
.\end{align*}
Where we can bound 
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 
.\end{align*}
further by considering that $f$ is continuous and thus for 
\begin{align*}  
  \mu_{f_m}(h)\coloneqq  \sup \{\abs{f_m(x)-f_m(y)} : x,y \in  \mathbb{R} \text{ with } \abs{x-y} \le h \}
.\end{align*}
we get that
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2  \le \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})  
.\end{align*}
putting it together
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
                                          &\le  \frac{T}{n} \sum_{i=1}^{n} \E[ \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  \frac{T}{n} \E[n* \mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  T \E[\mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
.\end{align*}
Since $f_m$ is continuous the modulus of continuity must tend to 0 as $n\to \infty$.
Thus we have shown that $f_m^{(n)} \xrightarrow{\mathcal{H}^2} f_m  \implies I(f_m^{(n)} ) \xrightarrow{L^2} I(f_m)$\\
Now on the set $\{\tau_m = T\}  $ we have 
\begin{align*}
  f(B) = f_m(B) 
.\end{align*}
and  by persistence of identity also 
\begin{align*}
  \int_0^{T} f(B_s) dB_s = \int_0^{T}   f_m(B_s) dB_s
.\end{align*}
For 
\begin{align*}
  A_{n,\epsilon } = \{\abs{\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  - \int_0^{T} f(B_s) dB_s } \ge  \epsilon\}  
.\end{align*}
Then we get 
\begin{align*}
\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  \xrightarrow{\P} \int_0^{T} f(B_s) dB_s 
.\end{align*}
if $\P(A_{n,\epsilon}) \to 0$ 
\begin{align*}
  \P(A_{n,\epsilon}) &= \P(A_{n,\epsilon} \cap \{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \marginnote{This inequality is just $\P(A) \le  \P(B)$ if $A \subset B$ } \\
                     &\le  \P(\{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \\ 
                     &\xrightarrow{n\to \infty} 0 
.\end{align*}
\end{proof}
\spewnotes
\begin{remark}[3.12]
  For any continuous $g: \mathbb{R}\to \mathbb{R}$  we have $f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} $  since $B$ is a.s. pathwise
  bounded on $[0,T]$
\end{remark}
\begin{proof}
 Consider $\omega \in  \Omega $ a.s., then 
 \begin{align*}
   \sup_{t \in  [0,T]}\abs{g(B_t(\omega ))} \le C 
 .\end{align*}
 for some $C\ge 0$, then we have 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &=  \int_0^{T}  g(B_t(\omega ))g(B_t(\omega)) dt\\
                                   &\le \int_0^{T}  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}* \abs{g(B_t(\omega))} dt\\
                                   &\le  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}\int_0^{T} \abs{g(B_t(\omega))} dt\\
                                   &\le C^2*T 
.\end{align*}
\end{proof}
\begin{Theorem}[3.18]
 For any twice continuous differentiable function $f : \mathbb{R} \to \mathbb{R}$  we have 
 \begin{align*}
   f(B_t) = f(0) + \int_0^{t}  f'(B_s)dB_s + \frac{1}{2} \int_0^{t} f^{''}(B_s) ds  \quad \P \text{-a.s.}
 .\end{align*}
\end{Theorem}
\begin{proof}
 The main tools used are a second order Taylor expansion then using our Riemann Sum convergence for the first integral 
 and for the second we have a normal (pointwise) Riemann sum.\\
 We write 
 \begin{align*}
   f(B_t) - f(0) &= \sum_{i=0}^{n-1} f(B_{t_i})-f(B_(t_{i-1})))\\
                 &= \sum_{i=0}^{n-1} f'(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) + \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})^2 + \sum_{i=0}^{n} r(B_{t_i},B_{t_{i-1}}) 
 .\end{align*}
 The first sum converges by 3.17  to 
 \begin{align*}
  \sum_{i=0}^{n-1} f'(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) \to  \int f'(B_s) dB_s
 .\end{align*}
 The second one we write as 
 \begin{align*}
   \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})^2 = \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})((B_{t_i}-B_{t_{i-1}})^2-(t_i-t_{i-1})) + \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(t_i-t_{i-1})
 .\end{align*}
 Then the second term is the integral we want i.e.
 \begin{align*}
   \frac{1}{2}\sum_{i=0}^{n-1} f''(B_{t_{i-1}})(t_i-t_{i-1}) \to  \frac{1}{2}\int f''(B_{s}) ds
 .\end{align*}
 Such that we need to show the first part converges against 0 $\P$-a.s., we do so by showing it converges to 0 in $L^2$ instead
 where we again make use of the independence of Brownian increments , and that they have mean 0, plus the fact that 
 \begin{align*}
   \E[(B_{t_i}-B_{t_{i-1}})^2] = t_i - t_{i-1}
 .\end{align*}
 the remainder term is a little more complicated, we rewrite 
 \begin{align*}
   r(x,y) &= \int_{x}^{y} (y-u)(f''(u)-f''(x)) du\\
          &= (y-x)^2 \int_0^{1} (1-t)(f''(x+t(y-x))f''(x))dt
 .\end{align*}
 then  if $f$ has compact support it holds 
 \begin{align*}
  \abs{r(x,y)} \le \abs{y-x}^2 \abs{h(x,y)}
 .\end{align*}
 for bounded $h$ with $h(x,x) = 0$ and compact support (that is $\supp f$),
 we show that the error term converges to 0 in $L^{1} $ by bounding $h$, that consists of splitting up $\Omega $ as follows
 \begin{align*}
  \Omega  = \{\abs{x-y} < \delta  \}  \cup \{\abs{x-y} \ge \delta \}  
 .\end{align*}
 on the first set we know by continuity $h(x,y) < \epsilon$ on the second we bound by using the $\|h\|_{\infty}$ which exists
 since $h$ continuous and of compact support,bounding the probability of the second set by Markov inequality ($f(x) = x^2$).
 Since we can choose $\delta $ as we want we may take $\epsilon=0$\\[1ex]
 Now we need to argue that we are allowed to assume $f$ compact support, this follows by similar argument to Riemann approximation.
\end{proof}
\begin{Theorem}[3.21]
  For any $f \in  \mathcal{C}^{1,2}([0,T]\times \mathbb{R}) $  we have
  \begin{align*}
    f(t,B_t) = f(0,0) + \int_{0}^{t} f_x(s,B_s) dB_s + \int_0^{t} f_t(s,B_s) + \frac{1}{2} f_{x x}(s,B_s) ds    
  .\end{align*}
  for $t \in [0,T]$ , $\P$-a.s.
\end{Theorem}
\begin{proof}
 We do a second order Taylor expansion in space and 1 in time  
\end{proof}
\begin{align*}
  f(t,B_t) - f(0,0) &= \sum_{i=1}^{n} f(t_{i},B_{t_i}) - f(t_{i-1},B_{t_{i-1}})\\
                    &= \sum_{i=1}^{n} f(t_{i-1},B_{t_{i}})-f(t_{i-1},B_{t_i}) + \sum_{i=1}^{n} f(t_{i},B_{t_{i-1}}) - f(t_{i},B_{t_{i-1}})\\
                    &= \sum_{i=1}^{n} f_t(t_{i-1},B_{t_{i}})(t_i -t_{i-1}) \\
                    &+ \sum_{i=1}^{n} f_x(t_i,B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})\\
                    &+ \sum_{i=1}^{n} \frac{1}{2}f_{x x}(t_i,B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})^2\\
                    &+ \sum_{i=1}^{n} r(t_{i-1},t_i,B_{t_{i-1}},B_{t_i})
.\end{align*}
where 
\begin{align*}
  r(s,t,x,y) = \int_s^{t} (f_t(r,y)-f_{t}(s,y))  dr + \int_x^y (y-u)(f_{x x}(s,u) - f_{x x}(s,x) du
.\end{align*}
Now if $f$ has compact we get a bound again and use the same arguments
\begin{Lemma}
  Proof that if $X_n \xrightarrow{L^{p} } X$  then $X_n \xrightarrow{\P}  X$
\end{Lemma}
\begin{proof}
 Let us first remember what the two convergences mean, for the first we have that
 \begin{align*}
   \|X_n - X\|_{L^{p} }^{p}  = \int_{\Omega} \abs{X_n - X}^{p}  \to 0
 .\end{align*}
 The second means that for $\forall  \epsilon >0$ : 
 \begin{align*}
  \P(\abs{X_n-X} \ge \epsilon) \to  0
 .\end{align*}
 Using Markov 
 \begin{align*}
   \P(\abs{X_n-X} \ge \epsilon) \le \frac{\E[\abs{X_n-X}^p]}{\epsilon^p} \to 0
 .\end{align*}
\end{proof}
\begin{Lemma}
  If $ X_n \xrightarrow{L^p} X$ then $X_n \xrightarrow{L^q} X$ for $p>q$
\end{Lemma}
\begin{proof}
 We recall Hölder for $p,q>1 : \frac{1}{p}+\frac{1}{q} = 1$ it holds
 \begin{align*}
   \E[XY] \le \E[X^{p} ]^{\frac{1}{p}} \E[X^{q} ]^{\frac{1}{q}} 
 .\end{align*}
 We want to get :
 \begin{align*}
   \E[\abs{X_n-X}^{q}] = \int_{\Omega } \abs{X_n-X}^{q} d\P  \le  \int_{\Omega } \abs{X_n-X}^{p} d\P * 1 \to 0
 .\end{align*}
 i.e we need $r,s\ge 1$ such that $\frac{1}{r}+\frac{1}{s} = 1$ and 
 \begin{align*}
  q*r = p \implies r = \frac{p}{q}
 .\end{align*}
 and 
 \begin{align*}
   1&=\frac{1}{s}+\frac{q}{p} \\
   \frac{p-q}{q} &= \frac{1}{s}
 .\end{align*}
 Note that any finite measure space suffices.
\end{proof}
\begin{Definition}[4.1]
  A stochastic process $X = (X_t)_{t \in  [0,T]}$  is called It\^o process if there is a a.s. 
  representation
  \begin{align*}
    X_t = X_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} b(*,s) dB_s  
  .\end{align*}
  where $X_{0} \in  \mathbb{R}$ and $a,b : \Omega \times [0,T] \to \mathbb{R}$ are adapted, measurable processes satisfying 
  \begin{align*}
    \P(\int_0^{t} \abs{a(\omega ,s)} ds  <\infty) = 1 \text{ and }  \P(\int_0^{t} \abs{b(\omega ,s)}^2 ds  <\infty) = 1
  .\end{align*}
\end{Definition}
\begin{remark}
  The stochastic integral process $A = (\int_0^{t} a(*,s) ds )_{t \in [0,T]}$  is continuous, adapted process of finite variation.
\end{remark}
\begin{proof}
  We show that it has finite variation, let $\Pi_n \subset  \Pi_{[0,T]}$  such that $\abs{\Pi_n} \to 0$,
  and $\omega  \in  \{\int_0^{t} \abs{a(\omega ,s)} ds  <\infty\}$
  \begin{align*}
    \lim_{n\to \infty} \sum_{J \in  \Pi_n} \Delta_{J \cap [0,T]} A &=  \lim_{n\to \infty} \sum_{J \in  \Pi_n : J = (s,t]} \abs*{\int_{s}^{t} a(\omega ,r) dr }\\
                                                                   &\le  \lim_{n\to \infty} \sum_{J \in  \Pi_n : J = (s,t]} \int_{s}^{t} \abs{a(\omega ,r)} dr \\
  .\end{align*}
  We can show this for any process $X_s$ such that $X_s$ is FV since 
  \begin{align*}
    X = X_{1}-X_{2}
  .\end{align*}
  By definition of FV where  $X_{1},X_{2}$are monotone increasing then 
  \begin{align*}
    \int_0^{t} a(*,s) dX_s &= \int_0^{t} a^{+}(*,s) - a^{-}(*,s)   dX_s \\
                           &= \int_0^{t} a^{+} dX^{(1)}_s + \int_{0}^{t}  a^{-}(*,s) dX^{(2)}_s - (\int_0^{t} a^{+} dX^{(2)}_s + \int_{0}^{t}  a^{-}(*,s) dX^{(1)}_s)
  .\end{align*}
  Which is the difference of two monotone increasing functions for $X_s = s$ we could just pick $s-0$
\end{proof}
\begin{Lemma}[4.2]
  Let $X  = (X_t)_{t \in  [0,T]}$  be an It\^o process with representation
  \begin{align*}
    X_t = X_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} b(*,s) dB_s = \tilde{X}_0 +\int_0^{t} \tilde{a}(*,s) ds + \int_0^{t} \tilde{b}(*,s) dB_s
  .\end{align*}
  for $t \in  [0,T]$ and $X_{0} = \tilde{X}_0 $. Then we have $a = \tilde{a}  $ and $b = \tilde{b} $ $\P \otimes \lambda $-a.s.
\end{Lemma}
\begin{proof}
 We have by linearity 
 \begin{align*}
  0 =  \int_0^{t} a(*,s)-\tilde{a}  ds + \int_0^{t} b(*,s)-\tilde{b}  dB_s
 .\end{align*}
 this implies 
 \begin{align*}
   -\int_0^{t} a(*,s)-\tilde{a}  ds &=\underbrace{\int_0^{t} b(*,s)-\tilde{b}  dB_s}_{B_t}
 .\end{align*}
 Right is a local martingale, and left is a process of finite variation. Then we know that 
\begin{align*}
  B_t = 0 \quad \P\text{-a.s.}
.\end{align*}
Then we have  by Ito-isometry that 
\begin{align*}
  0 = \|B\|_{\mathcal{H}^2} = \E[\int (b(*,s)-b(*,s))^2 ds ] = \int \E[(b(*,s)-b(*,s))^2] ds 
.\end{align*}
Which gives the implication, since we can split up the integral into two parts, 1 where they are equal i.e 0 and the other one where
they are not equal to each other, for $a$ we do the same
\end{proof}
\begin{Prop}[4.3]
  For any It\^o process $X = (X_t)$ with rep
  \begin{align*}
    X_t = X_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} b(*,s) dB_s
  .\end{align*}
 the quadratic variation of $X$ is given by 
 \begin{align*}
   \braket{X}_t = \lim_{n\to \infty} \sum_{J \in  \Pi_n}(\Delta_{J \cap [0,t]} X)^2 = \int_0^{t} b^2(*,s)  ds
 .\end{align*}
\end{Prop}
\begin{proof}
 We have that 
 \begin{align*}
  X_t = A_t + M_t
 .\end{align*}
 For $A_t$ continuous adapted and of Finite variation and $M_t$ a local martingale,
 we first note that 
 \begin{align*}
  (\Delta_{J \cap [0,T]} X)^2 =   (\Delta_{J \cap [0,T]} A)^2 + 2(\Delta_{J \cap [0,T]} M)(\Delta_{J \cap [0,T]} A) + (\Delta_{J \cap [0,T]} M)^2
 .\end{align*}
 We have 
 \begin{align*}
   \lim_{n\to \infty}\sum_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} A)^2  = \lim_{n\to \infty}\sum_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} A) (\Delta_{J \cap [0,T]} A) \le  \lim_{n\to \infty}  \sup_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} A) \sum_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} A) \to 0
 .\end{align*}
 Since the first part is a bounded constant, and the second goes to 0 since $A$ is of FV.\\
 By the same logic we have that the middle term goes to 0 and all that remains is 
 \begin{align*}
   \braket{X}_t = \lim_{n\to \infty}\sum_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} X)^2  = \lim_{n\to \infty}\sum_{J \in  \Pi_n} (\Delta_{J \cap [0,T]} M)^2 = \braket{M}_t
 .\end{align*}
Thus we have to show 
\begin{align*}
  \braket{M}_t =   \int_0^{t} b^2(*,s)  ds
.\end{align*}
For that we know it suffices to show that 
\begin{align*}
  (M_t^2 - \int_0^{t} b^2(*,s)  ds)_{t \in  [0,T]}
.\end{align*}
is a Martingale, first take a localizing sequence $\tau $ for $M_t$ (for notation sake we leave it out)
\begin{align*}
  &\E[M_t^2 - \int_0^{t} b^2(*,s)  -  M_s^2 + \int_0^{s} b^2(*,s) ds  ] \\
  &=\E[M_t^2-M_s^2 - \int_s^{t} b^2 ds ] \\
  &= \E[(M_t-M_s)^2 - \int_s^{t} b^2 ds]
.\end{align*}
We have $M_t - M_s$ (localized) is a martingale such that we can use Ito isometry 
\begin{align*}
  (M_t^{\tau_n} - M_s^{\tau_n})\cha_A &=  (M_t^{\tau_n} - M_s)\cha_A \cha_{\tau_n \ge s} \\
                                      &\int_s^{t} \cha_A\cha_{\tau_n\ge s}b()
.\end{align*}
which is in $\mathcal{H}^2$ then we have directly 
\begin{align*}
  \E[(M_{t\land \tau_n}-M_{s \land \tau_n})^2 \cha_A] &= \E[(\int_0^{T} \cha_A\cha_{\ldots } b dB_r)^2]\\
                                                      &=  \E[(\int_0^{T} \cha_A\cha_{\ldots } b^2 dr\\
                                                      &= \E[\cha_A \int_{s\land \tau_n}^{t \land t }b^2 dr ]
.\end{align*}
which with the before discussion concludes the proof
\end{proof}
\begin{Lemma}[4.5]
  Let $X = (X_t)_{t \in [0,T]}$  be an Ito process with rep 
  \begin{align*}
    X_t = X_0 + \int_0^{t} b(*,s) dB_s
  .\end{align*}
  Then the integral process $(\int_0^{t} f(*,s) dX_s )$ is a continous local martingale  and 
  If $\E[\int_0^{T} f^2 b^2 ds] <\infty$ then Ito isometry holds true
\end{Lemma}
\begin{proof}
 For (i)  we have by definition $f \in \mathcal{L}(X)$ that 
 \begin{align*}
   \int_0^{t} f(*,s)dX_s = \int_0^{t} \underbrace{f(*,s)b(*,s)}_{\in  \mathcal{H}^2_{\text{loc}}}dB_s   
 .\end{align*}
 Thus its  a continuous local martingale\\
 For (ii) we see that
 \begin{align*}
   \E[(\int_0^{T} f dX_t)^2 ] &= \E[\int_0^{T} f^2b^2(*,s)ds ]\\
                              &= \E[\int_0^{T} f^2(*,s) d\braket{X}_s]
 .\end{align*}
\end{proof}
\begin{Lemma}[4.7.]
  Let $g \in  \mathcal{C}(\mathbb{R}^{2} ) $  , $(Z_t)_{t \in  [0,T]}$ be a continuous adapted $\mathbb{R}^{2} $-valued stochastic process
  and $(X_t)_{t \in  [0,T]}$ be an It\^o process. For any $t \in  [0,T]$ and $t_i = t \frac{i}{n}$ we have 
  \begin{align*}
    \sum_{i=1}^{n} g(Z_{t_{i-1}})(X_{t_i}-X_{t_{i-1}})^2 \to \int_0^{t} g(Z_s)d\braket{X}_s 
  .\end{align*}
\end{Lemma}
\begin{proof}
 We compare this to the Riemann approximation theorem , which for $f \in  \mathcal{C}$ gives us 
 \begin{align*}
   \sum_{i=1}^{n}f(B_{t_{i-1}}) (B_{t_i}-B_{t_{i-1}}) \to  \int_0^{t} f(B_s) dB_s 
 .\end{align*}
 usually when we deal with stochastic integrals it is most convenient to show $L^2$ convergence since we can use Ito-isometry,
 in the riemann proof, the main step was showing that our $f_n \in \mathcal{H}_{0}^2$ sequence actually looks like the one above, to 
 that end we localized it onto a compact support and showed that convergence.\\[1ex]
Let us first note that 
\begin{align*}
  X_t = M_t + A_t
.\end{align*}
where $M_t$ is a local martingale and $A_t$ is of finite variation , that means 
\begin{align*}
  \sum_{i=1}^{n}g(Z_{t_{i-1}})(X_{t_i} - X_{t_{i-1}})^2 =  \sum_{i=1}^{n}g(Z_{t_{i-1}})(M_{t_i} - M_{t_{i-1}})^2 + \sum_{i=1}^{n}g(Z_{t_{i-1}})(A_{t_i} - A_{t_{i-1}})^2 + 2 \sum_{i=1}^{n}g(Z_{t_{i-1}})(M_{t_i} - M_{t_{i-1}})(A_{t_i} - A_{t_{i-1}})
.\end{align*}
Where similar to the proof for quadratic variation we know that only the first term doesn't go to 0, \textcolor{Red}{Important} the above argument is only valid if $g$ is bounded 
otherwise we cannot pull it out.
\begin{align*}
  \lim_{n\to \infty} \sum_{i=1}^{n}g(Z_{t_{i-1}})(X_{t_i} - X_{t_{i-1}})^2 = \lim_{n\to \infty}  \sum_{i=1}^{n}g(Z_{t_{i-1}})(M_{t_i} - M_{t_{i-1}})^2
.\end{align*}
Now if $g $ is bounded and $X$ is a bounded martingale with bounded variation then DCT yields
\begin{align*}
  \int_0^{t} g(Z_s) d\braket{X}_s &= \int_0^{t} \lim_{n\to \infty} \sum_{i=1}^{n}g(Z_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) d\braket{X}_s\\
                                  &= \lim_{n\to \infty} \int_0^{t} \sum_{i=1}^{n}g(Z_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) d\braket{X}_s\\
                                  &= \lim_{n\to \infty}  \sum_{i=1}^{n}g(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}})
.\end{align*}
We show that 
\begin{align*}
  \|\sum_{i=1}^{n}g(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}}) - \sum_{i=1}^{n}g(Z_{t_{i-1}}) (X_{t_i} -X_{t_{i-1}})^2\|_{L^2} \to 0
.\end{align*}
we show this goes to $0$ by using Cauchy-Schwartz and $(a+b)^2 \le 2a^2+2b^2$
\begin{align*}
  &\E[(\sum_{i=1}^{n}g(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}}) - \sum_{i=1}^{n}g(Z_{t_{i-1}}) (X_{t_i} -X_{t_{i-1}})^2)^2]\\
  &= \E[(\sum_{i=1}^{n}g(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)^2]\\
.\end{align*}
and we note that for $i\neq j$ and assume $j<i$ then
\begin{align*}
  &\E[g(Z_{t_{i-1}})g(Z_{t_{j-1}})(\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)(\ldots )]\\
  &=\E[g(Z_{t_{i-1}})g(Z_{t_{j-1}})(\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)(\ldots )]\\ 
  &= \E[\E[g(Z_{t_{i-1}})g(Z_{t_{j-1}})(\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)(\ldots )|\mathcal{F}_{t_{i-1}}]]\\ 
  &= \E[ g(Z_{t_{i-1}})g(Z_{t_{j-1}})\E[(\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)|\mathcal{F}_{t_{i-1}}](\ldots )]\\ 
  &= 0
.\end{align*}
since the conditional expectation is 0, this means the cross terms get eliminated and we have 
\begin{align*}
  &\E[(\sum_{i=1}^{n}g(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}}) - \sum_{i=1}^{n}g(Z_{t_{i-1}}) (X_{t_i} -X_{t_{i-1}})^2)^2]\\
  &= \E[\sum_{i=1}^{n}g^2(Z_{t_{i-1}}) (\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)^2]\\
  &\le \|g\|^2_{\infty} \sum_{i=1}^{n}\E[(\braket{X}_{t_i} - \braket{X}_{t_{i-1}}-(X_{t_i} -X_{t_{i-1}})^2)^2]\\
  &\le 2\|g\|^2_{\infty} \sum_{i=1}^{n}\left(  \E[(\braket{X}_{t_i} - \braket{X}_{t_{i-1}})^2]+\E[(X_{t_i} -X_{t_{i-1}})^4]\right) \\
.\end{align*}
then arguing using the modulus of continuity twice and that $X$ is bounded such that DCT applies we get that everything goes to 0\\
Last step is just the argument that all the assumptions above are well formed by localizing we  get
\begin{align*}
  \tau_m \coloneqq  \inf \{t \in [0,T] : \abs{X_t}\ge m \text{ or } \abs{\braket{X}_t} \ge  m \text{ or } \abs{Z_t} \ge m\}  \land T
.\end{align*}
which gives us again that the two coincide on $\{\tau_m = T\}  $ and step 2 gives us the $L^2$ convergence for $X^{m} $
\end{proof}
\begin{Theorem}][4.8]
  Let $f : \mathbb{R}^{2} \to  \mathbb{R} $ be a twice continuous differentiable function, and $X,Y$ be two It\^o processes with representation 
  \begin{align*}
    X_t = X_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} b(*,s)dB_s \quad \text{ and } \quad Y_t = Y_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} \beta(*,s) dB_s
  .\end{align*}
  Then we have 
  \begin{align*}
    f(X_t,Y_t) &= f(0,0) + \int_0^{t} f_x(X_t,Y_t) dX_t + \int_0^{t} f_y(X_t,Y_t) dY_t + \frac{1}{2}\int_0^{t} f_{x x}(X_t,Y_t) d\braket{X}_t + \frac{1}{2}\int_0^{t} f_{y y}(X_t,Y-t) d\braket{Y}_t \\
               &+ \int_0^{t} f_{xy}(X_t,Y_t) d\braket{X,Y}_t 
  .\end{align*}
  where 
  \begin{align*}
    \braket{X,Y}_t = \frac{1}{4}(\braket{X+Y}_t - \braket{X-Y}_t)
  .\end{align*}
\end{Theorem}
\begin{example}[5.1]
 Consider the SDE 
 \begin{align*}
  dX_t = \mu X_t dt + \sigma X_t dB_t
 .\end{align*}
 then we can solve this SDE by making the ansatz $X_t = f(B_t,t)$ and using It\^os formula
 \begin{align*}
   df(B_t,t) &= f(0,0) + f_x(B_t)dB_t + (\frac{1}{2}f_{x x}+f_{t}) dt\\
           &\triangleq  \mu  f(B_t) dt + \sigma f(B_t)dB_t
 .\end{align*}
 This implies 
 \begin{align*}
  f_x(B_t) = \sigma  f
 .\end{align*}
 Such that 
 \begin{align*}
  f = \exp(\sigma*x + g(t))
 .\end{align*}
 then 
 \begin{align*}
   g'(t)*f + \frac{1}{2} \sigma^2 f &= \mu  f\\
                                g'(t)   &= \mu  - \frac{\sigma^2}{2}\\
                                g(t)&= (\mu  - \frac{\sigma^2}{2})t + g_{0}
 .\end{align*}
 Which gives the solution 
 \begin{align*}
  X_t = \exp(\sigma B_t + (\mu -\frac{\sigma^2}{2})t + g_{0})
 .\end{align*}
\end{example}
\begin{Prop}[5.2.]
 In general a linear SDE has the form
 \begin{align*}
  X_t = (\alpha (t)X_t + \beta(t) )dt + (\phi(t)X_t + \psi(t))dB_t
 .\end{align*}
 and a solution is given by 
 \begin{align*}
  X_t  = x_{0}\exp(Y_t) + \int_0^{t} \exp(Y_t-Y_s)(\beta(s)-\psi(s)\phi(s))  ds + \int_0^{t}  \exp(Y_t-Y_s)\psi(s)dB_s
 .\end{align*}
 Where
 \begin{align*}
  Y_t = \int_0^{t} \phi(s) dB_s + \int_0^{t}(\alpha(s)-\frac{1}{2}\phi^2(s))   ds
 .\end{align*}
\end{Prop}
\begin{proof}
 This proof is straight forward computation, since if 
 \begin{align*}
  H_t \coloneqq  \exp(Y_t)
 .\end{align*}
 and 
 \begin{align*}
  Z_t = x_{0} + \int_0^{t} H^{-1}_s(\beta(s)-\psi(s)\phi(s)) ds + \int_0^{t}H_s^{-1}\psi(s)dB_s    
 .\end{align*}
 then we observe that 
 \begin{align*}
  X_t = H_tZ_t
 .\end{align*}
 we use It'os formula for Ito processes 
 \begin{align*}
   H_t = f(Y_t) &= f(0) + \int_0^{t} f_y(Y_t) d_Ys + \frac{1}{2} \int_0^{t} f_{yy}(Y_t) d\braket{Y_s}  \\
                &= 1 + \int_0^{t} \exp(Y_s) d_Ys + \frac{1}{2 } \int_0^{t} \exp(Y_s) d\braket{Y}_s   
 .\end{align*}
\end{proof}
\begin{Lemma}[5.5]
  For $T \in  (0,\infty)$  let $g : [0,T] \to \mathbb{R}$ be a bounded and measurable function.
  If there are constants $A,B \in  \mathbb{R}$ such that  
  \begin{align*}
    g(t) \le  A + B \int_0^{t} g(s) ds 
  .\end{align*}
  then $g(t) \le  A e^{Bt} $ for all $t \in [0,T]$
\end{Lemma}
\begin{Theorem}[5.3.]
 Let $\mu ,\sigma $  be two measurable functions satisfying the following conditions 
 \begin{align*}
  \abs{\mu(t,x) - \mu(t,y)} + \abs{\sigma(t,x)-\sigma(t,y)} \le  L \abs{x-y}
 .\end{align*}
 and 
 \begin{align*}
  \abs{\mu(t,x)} + \abs{\sigma(t,x)} \le L(1+\abs{x}^2)
 .\end{align*}
 then there exists a solution $X_t$  to 
 \begin{align*}
  dX_t = \mu(t,X_t) dt + \sigma(t,X_t) dB_t 
 .\end{align*}
\end{Theorem}
\begin{proof}
  Let us first summarize the proof steps and the difficulties 
  \begin{enumerate}
    \item Uniqueness for uniqueness we want indistinguishable but grönwall only yields modification, 
      such that we argue with continuity that they are also indistinguishable (continuous lets you rewrite the sets as finite unions.)
    \item Existence, for existence we use a picard iteration,  but similarly to the proof that the stochastic integral is a martingale 
      we need to argue a strong Cauchy case i.e. that the supremum over all t is bounded, this relies on borell cantelli.
    \item further more showing that the limit is in $L^2$ is not completly tribial
  \end{enumerate}
 We proof first uniqueness in $L^2$ by checking  for two solutions $X_t , Y_t$,
 \begin{align*}
   \E[\abs{X_t-Y_t}^2]  &=\E[\abs*{\int_0^{t} \mu(s,X_s)-\mu(s,Y_s) ds + \int_0^{t} \sigma(s,X_s) -\sigma(s,Y_s) dB_s }^2 ]\\
                        &\le 2\E[(\int_0^{t}\mu(s,X_s) - \mu(s,Y_s) ds )^2] + 2\E[(\int_0^{t}\sigma (s,X_s) - \sigma(s,Y_s) dB_s )^2]\\
                        &\le 2t\E[(\int_0^{t}(\mu(s,X_s) - \mu(s,Y_s))^2 ds] + 2\E[(\int_0^{t}(\sigma (s,X_s) - \sigma(s,Y_s))^2 ds ]\\
                        &\le 2(t+1)L\E[\int_0^{t} \abs{X_s-Y_s}^2 ds ]
 .\end{align*}
 Where the first part is Hölder with 1 and $p=q=2$ and the second follows by Ito Isometry, and last step is Lipschitz
 then we have that $A=0$ in grönwall and the entire thing is $0$
\end{proof}

