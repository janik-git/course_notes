
\begin{Lemma}[3.2 It\^o s isometry simple version]
 For $f \in  \mathcal{H}_0^{2} $  we have 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \|I(f)\|_{L^2}
 .\end{align*}
\end{Lemma}
\begin{proof}
 We have 
 \begin{align*}
   \|I(f)\|_{L^2} &= \E[(\sum_{i=1}^{n} a_i (B_{t_i} - B_{t_{i-1}}))^2]\\
                  &= \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2 + \sum_{i\neq j}^{n} a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ]\\
                  &=  \E[\sum_{i=1}^{n} a_i^2(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[\E[a_i^2(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2 | \mathcal{F}_{t_{i-1}}]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2\E[(B_{t_i}-B_{t_{i-1}})^2]]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2]\E[(B_{t_i}-B_{t_{i-1}})^2]\\
                  &=  \sum_{i=1}^{n} \E[a_i^2](t_i-t_{i-1})\\
 .\end{align*}
 Since  w.l.o.g take $i<j$
 \begin{align*}
   \E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}})  ] &= \E[\E[a_ia_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
                                                              &= \E[a_i \underbrace{\E[(B_{t_{i}}-B_{t_{i-1}})]}_{=0}\E[a_j(B_{t_{i}}-B_{t_{i-1}})(B_{t_{j}}-B_{t_{j-1}}) | \mathcal{F}_{t_{i-1}}] ]\\
 .\end{align*}
 But 
 \begin{align*}
   \|f\|_{\mathcal{H}^2} = \E[\int_0^{T} f^2 ds ] = \sum_{i=1}^{n}\E[a_i^2](t_i-t_{i-1})
 .\end{align*}
\end{proof}
\begin{Prop}[3.5]
  For every $f \in  \mathcal{H}^2$  there exists a sequence $(f_n)_{n \in  \mathbb{N}}) \subset  \mathcal{H}_0^{2} $ such that 
  \begin{align*}
    \|f_n - f\|_{\mathcal{H}^2}\xrightarrow{n\to \infty} 0
  .\end{align*}
\end{Prop}
\begin{proof}
 The rough outline of the proof can be split up as follows
 \begin{enumerate}
  \item Show that we can assume $f$ is bounded 
  \item Show that we can assume $f$ is bounded adapted and continuous
  \item Construct simple sequence that approximates $f$
 \end{enumerate}
 For step 1 we can simply consider that for any (potentially ) unbounded function $f$ 
 \begin{align*}
  f_n = -n \lor (f \land n)
 .\end{align*}
 is bounded and apply DCT
 \begin{align*}
   \lim_{n\to \infty}\|f_n - f\|_{\mathcal{H}^2} = \lim_{n\to \infty}\E[\int_0^{T}(f_n-f)^2 dt ] =  \E[\int_0^{T} \lim_{n \to \infty}(f_n-f)^2 dt ] = 0
 .\end{align*}
 Thus we may assume $f$ bounded \\[1ex]
 For step 2 we can use that we can get the height of a rectangle by dividing by its base s.t.
 \begin{align*}
   f_n(*,t) \coloneqq  \frac{1}{(t-(t-\frac{1}{n})_+)} \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds =  n \int_{(t-\frac{1}{n})_+}^{t} f(*,s) ds
 .\end{align*}
 Now we note that since we work with random variables we condition $\omega $ on the set where
 \begin{align*}
   \lim_{n\to \infty}  f_n(\omega ,t) = f(\omega ,t)
 .\end{align*}
 We need this set to have measure 1, such that 
 \begin{align*}
   A \coloneqq  \{(\omega ,t) \in  \Omega  \times  [0,T] : \lim_{n\to \infty} f_n(\omega ,t) \neq  f(\omega ,t)\}  
 .\end{align*}
 Has measure zero with respect to $\P \otimes \lambda $, we have by the fundamental theorem of calculus that
 \begin{align*}
   \lambda(\{t \in  [0,T] : (\omega,t) \in  A\}) = 0
 .\end{align*}
Or rather Lebesgue Differentiation theorem, otherwise we'd need the condition that $f(\omega ,*)$ only has countable many discontinuities.
Thus we get that we may assume $f$ is bounded and continuous.\\[1ex]
We now construct our simple function $f_n$ as 
\begin{align*}
  f_{n,s}(\omega ,t) \coloneqq  f(\omega ,(s+\phi_n(t-s)_+))
.\end{align*}
where
\begin{align*}
  \phi_n = \sum_{i=1}^{2^n} \frac{j-1}{2^{n} }\cha_{(\frac{j-1}{2^{n}  },\frac{j}{2^{n} }]}
.\end{align*}
makes our time interval discrete (standard argument really), then we wanna show 
\begin{align*}
  \|f_n-f\|_{\mathcal{H}^2}&= \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] \to 0
.\end{align*}
We have 
\begin{align*}
  \E[\int_0^{T} \abs{f_{n,s}-f}^2 dt ] &\le \E[\int_0^{T} \int_0^{1} \abs{f_{n,s}-f}^2 ds dt]\\
                                       &= \E[\int_0^{T} \int_0^{1} \abs{f(*,(s+\phi_n(t-s))_+)-f}^2 ds dt]\\
                                       &=  \sum_{j\in \mathbb{Z}}\E[\int_0^{T} \int_{[t-\frac{j}{2^{n} },t-\frac{j-1}{2^{n} }] \cap [0,1]} \abs{f(*,(s+\frac{j-1}{2^{n} })-f}^2 ds dt]\\
                                       &\le (2^{n}+1 )2^{-n}  \int_{(0,1]} \E[\int_0^{T}\abs{f(*,t-2^{-n}h )-f(*,t)}^2 dt ]dh 
.\end{align*}
Where we can show that the Expectation term goes to 0
\end{proof}
\begin{align*}
  \int_0^{T}  \abs{f_n(*,(t-h)_+)-f(*,(t-h)_+)}^2 dt = \int_0^{h} \abs{f_n(*,0)-f(*,0)}^2 dt + \int_h^{t} \ldots \le h\abs{f_n(*,0)-f(*,0)}^2 + \|f-f_n\|_{\mathcal{H}^2}
.\end{align*}
\begin{Theorem}[3.7.]
  For any $f \in  \mathcal{H}^2$ there is a continuous martingale $X = (X_t)_{t \in  [0,T]}$ with 
  respect to $\mathcal{F}_t$ such that for all $t \in  [0,T]$
  \begin{align*}
    X_t = I(f \cha_{[0,t]})
  .\end{align*} 
\end{Theorem}
\begin{proof}
 We first consider the simple case with 
 \begin{align*}
   f_n = \sum_{i=0}^{m_n - 1} a_i^{n}  \cha_{(t_i^{n},t_{i+1}^{n}  ]}
 .\end{align*}
 Then 
 \begin{align*}
   \E[X_{t}^{n} - X_{s}^{n} | \mathcal{F}_s  ] &= \E[\sum_{t_i > s} a_i(B_{t_{i+1}}-B_{t_i})]\\
                                               &=  0
 .\end{align*}
Our end goal is to use a triangular inequality to use the simple case to bound the normal one i.e.
\begin{align*}
  \abs{\E[X_t-X_s|\mathcal{F}_s]} &= \abs{\E[X_t-X_{t}^{n}+X_{t}^{n} -X_s +X_{s}^{n} -X_{s}^{n}  |\mathcal{F}_s]}\\
                                  &\le \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \underbrace{\abs{\E[X_{t}^{n} -X_{s}^{n}  |\mathcal{F}_s]}}_{=0} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}
                                  &= \abs{\E[X_t-X_{t}^{n}|\mathcal{F}_s]} + \abs{\E[X_{s}^{n}-X_s|\mathcal{F}_s ]}\\
                                  &\myS{Jen}{\le } \E[\abs{X_t-X_{t}^{n}}|\mathcal{F}_s] + \E[\abs{X_{s}^{n}-X_s}|\mathcal{F}_s ]\\
.\end{align*}
We consider $A \in  \mathcal{F}_s$
\begin{align*}
  \E[\abs{X_t-X_{t}^{n}}\cha_A] + \E[\abs{X_{s}^{n}-X_s}\cha_A ] &\le \E[\abs{X_t-X_{t}^{n}}] + \E[\abs{X_{s}^{n}-X_s} ]\\
                                                                 &\le 2*\|f-f_n\|_{\mathcal{H}^2}
.\end{align*}
So we have $\E[X_t |\mathcal{F}_s] = X_s$\\[1ex]
For $(f_n)_{n \in  \mathbb{N}} \subset \mathcal{H}^2_0 $ we have 
\begin{align*}
  \lim_{n\to \infty}f_n = f \in \mathcal{H}^2
.\end{align*}
And 
\begin{align*}
  \lim_{n \to \infty} I(f_n) = I(f) \in L^2
.\end{align*}
Which should be equivalent to for fixed $t \in  [0,T]$
\begin{align*}
  X_t^n &\xrightarrow{L^2}  X_t \\
  I(f_n\cha_{[0,t]}) &\xrightarrow{L^2} I(f\cha_{[0,t]})
.\end{align*}
We need to make the argument uniform in $t \in  [0,T]$, which is Step 2 in the script i guess.\\[1ex]
We have
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  \epsilon)  \le  \epsilon^{-2} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2}
.\end{align*}
since  for all $p>1$
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t} \ge  \epsilon) \frac{1}{\epsilon^p} \E[\abs{X}_T^p]
.\end{align*}
By choosing a subequence we can get 
\begin{align*}
  \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} )  \le  2^{2k} \E[\abs{X_T^n - X_T^m}^2] = \epsilon^{-2} \|f_n-f_m\|^2_{\mathcal{H}^2} \le  2^{-k} 
.\end{align*}
Then we can apply Borel-Cantelli since
\begin{align*}
  \sum_{k=0}^{\infty}   \P(\sup_{0\le t\le T} \abs{X_t^n - X_t^m} \ge  2^{-k} ) < \infty
.\end{align*}
and get $\Omega_0 \in  \mathcal{F}$ such that $\P(\Omega_0) = 1$ and $X^{n_k} $ is a pathwise cauchy sequence.
\end{proof}
\begin{Theorem}[3.17 Riemann sum approximation]
 If $f : \mathbb{R} \to \mathbb{R} $  is a continuous function and  $t_i = \frac{i}{n}T$ then for $n\to \infty$ we have
 \begin{align*}
   \sum_{i=1}^{n} f(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}}) \xrightarrow{\P} \int_0^{T} f(B_s) dB_s
 .\end{align*}
\end{Theorem}
\begin{proof}
By Remark 3.12. we know that for any continuous function $g : \mathbb{R} \to  \mathbb{R}$ 
\begin{align*}
  f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} 
.\end{align*}
This follows since for a.s. $\omega  \in  \Omega $ the map 
\[\phi(\omega ) : [0,T] \to  \mathbb{R} : t \mapsto B_t(\omega )\]
is bounded. This gives us that 
\begin{align*}
  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))} = \sup_{x \le \abs{m}} \abs{g(x)} \le  C
.\end{align*}
Where the last inequality follows from the fact that $g$ is continuous and attains a maximum on the compact set $[-m,m]$ then we can check that 
for 
\begin{align*}
  \omega  \in  \{\phi \text{ is bounded } \}  
.\end{align*}
The integral 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &\le  \int_0^{T}  \abs{g(B_t(\omega ))}\abs{g(B_t(\omega ))} dt\\
                                   &\le  \underbrace{\sup_{t \in [0,T]} \abs{g(B_t(\omega ))}}_{\le C} \int_0^{T}  \abs{g(B_t(\omega ))} dt\\
                                   &\le  C^2T
.\end{align*}
Since $\P(\{\phi \text{ is bounded } \}) = 1$ we get  immediately 
\begin{align*}
  \P(\int_0^{T} g^2(B_t(\omega ))  < \infty) = 1
.\end{align*}
This tells us that  for any continuous $f$ and Brownian motion $B$ 
\begin{align*}
  f(B) \in  \mathcal{H}_{\text{loc}}^2
.\end{align*}
we can rewrite $\{\phi \text{ is bounded } \}$ as a stopping time instead and get 
\begin{align*}
  \tau_m = \inf \{t \in [0,T] :  \abs{B_t} \ge  m\}  
.\end{align*}
which is a localizing sequence for $f(B)$ since by similar argument to above we have 
\begin{align*}
  \abs{f(B_{* \land \tau_m})} \le  \sup_{\abs{x} \le m} \abs{f(x)} < \infty
.\end{align*}
and we get 
\begin{align*}
  f_m = f*\cha_{[-m,m]}  = f\rvert_{[-m,m]}
.\end{align*}
Where 
\begin{align*}
  f_m(B) \in  \mathcal{H}^2
.\end{align*}
By definition of the It\^o integral for $f \in \mathcal{H}^2$ we already get that  
\begin{align*}
  I(f_m^{(n)} ) = \sum_{i=1}^{n} a_i (B_{t_{i}}  - B_{t_{i-1}}) \xrightarrow{L^2} \int_0^{T}  f_m(B_t) dt 
.\end{align*}
where $L^2$ convergence implies $\P$ convergence. \\[1ex]
Thus our goal in Step 2 is to show that  in fact
\begin{align*}
  f_m^{(n)}   = \sum_{i=1}^{n}  f_m(B_{t_{i-1}})(\omega )\cha_{(t_{i-1},t_i]}(s) 
.\end{align*}
we clearly have
\begin{align*}
  f_m^{(n)}   \in  \mathcal{H}_{0}^2
.\end{align*}
Then it  remains to show $f_m^{(n)} \xrightarrow{\mathcal{H}^{2} } f_m $ 
% \begin{align*}
%   \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[\int_0^{T} 2\sum_{i=1}^{n} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_0^{T} (f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(s))^2 \cha_{(t_{i-1},t_i]}(s)]\marginnote{$t_i$ is partition of $[0,T]$}\\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(s))^2 ] \\
%                                           &\le   \E[2\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
%                                           &\le   2\sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
%                                           &\le   2 \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
% .\end{align*}
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - f_m(B_s))^2 ] \\
                                          &= \E[\int_0^{T} (\sum_{i=1}^{n} f_m(B_{t_{i-1}})\cha_{(t_{i-1},t_i]}(s) - \sum_{i=1}^{n} f_m(B_s) \cha_{t_{{i-1}},t_i})^2 ] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 \\
                                          &+ \underbrace{\sum_{i,j=1}^{n} \underbrace{(f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))(f_m(B_{t_{j-1}})-f_{m}(B_s)\cha_{(t_{j-1},t_j]}(s))}_{{[t_{i-1}},t_i] \cap [t_{j-1},t_{j}] = \emptyset }}_{=0}  ds \bigg] \\
                                          &= \E\bigg[\int_0^{T} \sum_{i=1}^{n} (f_m(B_{t_{i-1}})-f_{m}(B_s)\cha_{(t_{i-1},t_i]}(s))^2 ds] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} (f_m(B_{t_{i-1}}) - f_m(B_s))^2 ] \\
                                          &\le   \E[\sum_{i=1}^{n} \int_{t_{i-1}}^{t_i} \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ds] \\
                                          &\le   \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 \int_{t_{i-1}}^{t_i}  ds] \\
                                          &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
.\end{align*}
Where we can bound 
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 
.\end{align*}
further by considering that $f$ is continuous and thus for 
\begin{align*}  
  \mu_{f_m}(h)\coloneqq  \sup \{\abs{f_m(x)-f_m(y)} : x,y \in  \mathbb{R} \text{ with } \abs{x-y} \le h \}
.\end{align*}
we get that
\begin{align*}
  \sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2  \le \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})  
.\end{align*}
putting it together
\begin{align*}
  \E[\int_0^{T}  (f_m^{(n)} - f_m )^2 ds] &\le    \frac{T}{n} \sum_{i=1}^{n} \E[\sup_{r \in [t_{i-1},t_{i}]}(f_m(B_{t_{i-1}})-f_m(B_r))^2 ] \\
                                          &\le  \frac{T}{n} \sum_{i=1}^{n} \E[ \mu_{f_m}(\sup_{r \in [t_{i-1},t_i]} \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  \frac{T}{n} \E[n* \mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
                                          &\le  T \E[\mu_{f_m}(\sup_{r \in [t_{i-1},t_i],i\le n\}  } \abs{B_{t_{i-1}}-B_r})^2] \\
.\end{align*}
Since $f_m$ is continuous the modulus of continuity must tend to 0 as $n\to \infty$.
Thus we have shown that $f_m^{(n)} \xrightarrow{\mathcal{H}^2} f_m  \implies I(f_m^{(n)} ) \xrightarrow{L^2} I(f_m)$\\
Now on the set $\{\tau_m = T\}  $ we have 
\begin{align*}
  f(B) = f_m(B) 
.\end{align*}
and  by persistence of identity also 
\begin{align*}
  \int_0^{T} f(B_s) dB_s = \int_0^{T}   f_m(B_s) dB_s
.\end{align*}
For 
\begin{align*}
  A_{n,\epsilon } = \{\abs{\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  - \int_0^{T} f(B_s) dB_s } \ge  \epsilon\}  
.\end{align*}
Then we get 
\begin{align*}
\sum_{i=1}^{n} f(B_{t_{i-1}})*(B_{t_i}-B_{t_{i-1}}))  \xrightarrow{\P} \int_0^{T} f(B_s) dB_s 
.\end{align*}
if $\P(A_{n,\epsilon}) \to 0$ 
\begin{align*}
  \P(A_{n,\epsilon}) &= \P(A_{n,\epsilon} \cap \{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \marginnote{This inequality is just $\P(A) \le  \P(B)$ if $A \subset B$ } \\
                     &\le  \P(\{\tau_m < T\} ) + \P(A_{n,\epsilon} \cap \{\tau_m = T\} ) \\ 
                     &\xrightarrow{n\to \infty} 0 
.\end{align*}
\end{proof}
\spewnotes
\begin{remark}[3.12]
  For any continuous $g: \mathbb{R}\to \mathbb{R}$  we have $f(\omega ,t) = g(B_t(\omega )) \in  \mathcal{H}^{2}_{\text{loc}} $  since $B$ is a.s. pathwise
  bounded on $[0,T]$
\end{remark}
\begin{proof}
 Consider $\omega \in  \Omega $ a.s., then 
 \begin{align*}
   \sup_{t \in  [0,T]}\abs{g(B_t(\omega ))} \le C 
 .\end{align*}
 for some $C\ge 0$, then we have 
\begin{align*}
  \int_0^{T}  g^2(B_t(\omega )) dt &=  \int_0^{T}  g(B_t(\omega ))g(B_t(\omega)) dt\\
                                   &\le \int_0^{T}  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}* \abs{g(B_t(\omega))} dt\\
                                   &\le  \sup_{t \in  [0,T]} \abs{g(B_t(\omega ))}\int_0^{T} \abs{g(B_t(\omega))} dt\\
                                   &\le C^2*T 
.\end{align*}
\end{proof}
\begin{example}[5.1]
 Consider the SDE 
 \begin{align*}
  dX_t = \mu X_t dt + \sigma X_t dB_t
 .\end{align*}
 then we can solve this SDE by making the ansatz $X_t = f(B_t,t)$ and using It\^os formula
 \begin{align*}
   df(B_t,t) &= f(0,0) + f_x(B_t)dB_t + (\frac{1}{2}f_{x x}+f_{t}) dt\\
           &\triangleq  \mu  f(B_t) dt + \sigma f(B_t)dB_t
 .\end{align*}
 This implies 
 \begin{align*}
  f_x(B_t) = \sigma  f
 .\end{align*}
 Such that 
 \begin{align*}
  f = \exp(\sigma*x + g(t))
 .\end{align*}
 then 
 \begin{align*}
   g'(t)*f + \frac{1}{2} \sigma^2 f &= \mu  f\\
                                g'(t)   &= \mu  - \frac{\sigma^2}{2}\\
                                g(t)&= (\mu  - \frac{\sigma^2}{2})t + g_{0}
 .\end{align*}
 Which gives the solution 
 \begin{align*}
  X_t = \exp(\sigma B_t + (\mu -\frac{\sigma^2}{2})t + g_{0})
 .\end{align*}
\end{example}
\begin{Definition}
 In general a linear SDE has the form
 \begin{align*}
  X_t = (\alpha (t)X_t + \beta(t) )dt + (\phi(t)X_t + \psi(t))dB_t
 .\end{align*}
 and a solution is given by 
 \begin{align*}
  X_t  = x_{0}\exp(Y_t) + \int_0^{t} \exp(Y_t-Y_s)(\beta(s)-\psi(s)\phi(s))  ds + \int_0^{t}  \exp(Y_t-Y_s)\psi(s)dB_s
 .\end{align*}
 Where
 \begin{align*}
  Y_t = \int_0^{t} \phi(s) dB_s + \int_0^{t}(\alpha(s)-\frac{1}{2}\phi^2(s))   ds
 .\end{align*}
\end{Definition}
