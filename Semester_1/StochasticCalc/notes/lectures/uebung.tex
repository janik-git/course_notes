\section*{Sheet 1}
\subsection*{1.1}
Mostly just calculating and showing the three properties 
\begin{enumerate}
 \item $W_0 = 0$
 \item For any partition $(t_i)_{i \in  \mathbb{N}}$ it holds that $W_0, W_{t_1} - W_{t_0},\ldots W_{t_n}-W_{t_{n-1}}$ are independent random variables
 \item The increment's are normally distributed i.e $W_t - W_s \sim \mathcal{N}(0,\abs{t-s})$
\end{enumerate}
For (iv) pick $\pm B_t$ as a counterexample , then the variance doesn't match for the increments
\subsection*{1.2}
\begin{exercise}
Let $(X_t)_{t \in  [0,\infty)}$ be a right-continuous real-valued, stochastic process adapted to the filtration $(\mathcal{F}_{t})_{t \in  [0,\infty)}$ and 
let $A \subset  \mathbb{R}$. Prove that the hitting time 
\begin{align*}
  \tau_A \coloneqq  \inf \{t\ge 0 : X_t \in  A\}  
.\end{align*}
is a stopping time if 
\begin{enumerate}
  \item $A$ is open and $(\mathcal{F}_{t})_{t \in  [0,\infty)}$  is right-continuous
  \item $A$ is closed and $(X_t)_{t \in  [0,\infty)}$  is continuous
\end{enumerate}  
\end{exercise}
\begin{proof}
  First we note that if $A$ is open then $\tau_A = t$ does not imply $X_t \in  A$, and that since $X_t$ is right-continuous we have for any 
  $\omega  \in  \{\tau_A =  t\}$ that 
  \begin{align*}
    t \mapsto X_t(\omega )
  .\end{align*}
  is right-continuous i.e for any $\epsilon > 0$ there $\exists \delta  > 0$ such that 
  \begin{align*}
    s \in [t,t+\delta ]  \implies \abs{X_s - X_{t}} < \epsilon
  .\end{align*}
  i.e if $X_t \in  A$ then a small Ball (to the right) around $t$ is also in $A$. This lets us do 
  \begin{align*}
    \{\tau_A \le  t\}   = \{\tau_A < t\}   \cup \{\tau_A = t\}   
  .\end{align*}
  Where
  \begin{align*}
    \{\tau_A < t\}   = \bigcup_{s < t} \{X_s \in A\}   \myS{Cont.}{=} \bigcup_{s < t , s \in \mathbb{Q}} \{X_s \in  A\}   
  .\end{align*}
  where the last union is over finite set each in $\mathcal{F}_s$ (X is adapted) such that 
  \begin{align*}
    \{\tau_A < t\}   \in  \mathcal{F}_t
  .\end{align*}
  For $\{\tau_A = t\}  $ we consider 
  \begin{align*}
    \{\tau_A \le  t\}   = \bigcap \{\tau_A < t+\frac{1}{n}\}   
  .\end{align*}
  Which by right right-continuity and again a continuous argument lie in $\mathcal{F}_t^+ = \mathcal{F}_t$ \\[1ex]
  I am unsure why $X$  continuous is necessary since since $X_t$ at any $\omega$ is already uniquely determined by its paths.
  We consider 
  \begin{align*}
    d(x,A) = \inf_{y \in  A} \abs{x-y}
  .\end{align*}
  Then 
  \begin{align*}
    \{\tau_A = t\}  = \{d(X_t,A) = 0\}  
  .\end{align*}
  we show that 
  \begin{align*}
    A_n = \{y \in  \mathbb{R} : d(y,A) < \frac{1}{n}\}  
  .\end{align*}
  Then 
  \begin{align*}
    \bigcap A_n = A
  .\end{align*}
  Since $A$ is closed, then we want to show 
  \begin{align*}
    \{\tau_A \le  t\}  =  \bigcap_{n \in  \mathbb{N}} \{\tau_{A_n} \le  t\}  
  .\end{align*}
  And first note that $\tau_{A_n} \le  \tau_{A_{n+1}} \le  \tau_A $ \\
  We show that  for $T = \sup_{n} \tau_{A_n}$ 
  \begin{align*}
    \tau_A \le  T
  .\end{align*}
  Then we get the convergence, we do so by showing that $X_T \in  A$  then by definition $\tau_A \le  T$. 
  \begin{align*}
    d(X_T,A) = \inf_{y \in  A} \abs{X_T - y} \le  \abs{X_T -X_{t_n}} + \abs{X_{t_n} - y}  \le  \abs{X_T -X_{\tau _n}} + \frac{1}{n}
  .\end{align*}
  And since $X$ is continuous we get that there $\exists N \in  \mathbb{N}$ such that for $n\ge N$ 
  \begin{align*}
    \abs{X_T - X_{t_n}} < \frac{1}{n}
  .\end{align*}
  in fact left continuous would have been enough (for this argument) we still need right continuous such that we can apply (i) to 
  \begin{align*}
    \{\tau_A \le t\}   = \bigcap_{n \in  \mathbb{N}} \{\tau_{A_n} < t\}  
  .\end{align*}
\end{proof}
\subsection*{Excercise 1.3}
\begin{exercise}
 Let $X$ and $X_n , n \in  \mathbb{N}$  be random variables on a probability space $(\Omega ,\mathcal{F},\P)$. Prove the following 
 statements 
 \begin{enumerate}
   \item If $(X_n)_{n \in  \mathbb{N}}$ is uniformly integrable and $X_n \to  X$ $\P$-a.s. then $X_n \to  X$ in $L^1$
   \item If $X$ is integrable, then the family $\{\E[X|\mathcal{G}] \ : \ G \subseteq \mathcal{F} \}  $ is uniformly integrable
 \end{enumerate}
\end{exercise}
\begin{proof}
  For (i)  alternative statement is if $\|X_n\| \to  \|X\|$ and $X_n \to X$  a.s. then
  \begin{align*}
    \lim_{n \to \infty} \E[\abs{X - X_n}]  = 0
  .\end{align*}
  We consider 
  \begin{align*}
    \abs{X-X_n} = \le  \abs{X} + \abs{X_n}
  .\end{align*}
 I.e 
 \begin{align*}
  \abs{X} + \abs{X_n} - \abs{X-X_n} \ge 0
 .\end{align*}
 Such that  by Fatou 
 \begin{align*}
   0 \le \E[\lim_{n\to \infty} \abs{X} + \abs{X_n} - \abs{X-X_n}]  &=   \E[2 \abs{X}] \le  \liminf \E[\abs{f} + \abs{f_n} - \abs{f-f_n}]\\
                                                                   &= \liminf (\E[\abs{f}] + \E[f_n]) - \limsup \E[\abs{f-f_n}]
 .\end{align*}
 Then we get by rearanging
 \begin{align*}
   \limsup \E[\abs{f-f_n}] \le  \liminf (\E[\abs{f}] + \E[f_n]) - \E[2\abs{f}]  = 0
 .\end{align*}
 \\[1ex]
 Now consider 
 \begin{align*}
   \E[\abs{f-f_n}] = \E[\abs{f-f_n}*\cha_{\abs{f-f_n}\ge c}] +  \E[\abs{f-f_n}*\cha_{\abs{f-f_n}  < c}]
 .\end{align*}
 The last is bounded by 
 \begin{align*}
   \E[\abs{f-f_n}*\cha_{\abs{f-f_n}\ge c}] +  \E[\abs{f-f_n}*\cha_{\abs{f-f_n}  < c}] <  \E[\abs{f-f_n}*\cha_{\abs{f-f_n}\ge c}] + c*\P(\Omega)
 .\end{align*}
 By convergence in measure there exists $n \in  \mathbb{N}$ such that $\P(\abs{f-f_n} \ge c)<\delta $ where $c$ is choses n such that $c*\P(\Omega ) < \frac{\epsilon}{2} $ 
 Now we prove $\abs{f-f_n}$ is uniformly integrable, we have 
 \begin{align*}
  \int_A \abs{f} \le \liminf \int_A \abs{f_n} \le  \epsilon
 .\end{align*}
 for $\P(A) < \delta $ then 
 \begin{align*}
  \abs{f-f_n} < \abs{f} + \abs{f_n}
 .\end{align*}
 i.e 
 \begin{align*}
  \int_A \abs{f-f_n} \le  \int_A \abs{f} + \abs{f_n} \le  \epsilon
 .\end{align*}
 Such that $\abs{f-f_n}$ is uniformly integrable.
\end{proof}
Let us summarize, in the hitting time exercise we know finite unions of open sets are in the $\sigma$-algebra such that we always want to rewrite it as that case,
the right continuity of $X$ allows us to show that any infinite union (over time) can be written as a finite one. the right continuity is useful be cause 
\begin{align*}
  [1,2+\frac{1}{n}) \to [1,2]
.\end{align*}
And we need that $\{\tau  < t + \frac{1}{n}\}  $ are contained in $\mathcal{F}_t$. \\[1ex]
In the closed case we argue that first
\begin{align*}
  A = \bigcap A_n \coloneqq  \{y \in  \mathbb{R} : d(y,A) < \frac{1}{n}\}  
.\end{align*}
this follows since $A$ is closed, then we want the following convergence
\begin{align*}
  \{\tau_A \le  t\}  = \bigcap \{\tau_{A_n} < t\}  
.\end{align*}
We use the left continuity from $X$ to prove that 
\begin{align*}
  \sup \tau_{A_n} \le  \tau_A  \text{ and } \tau_A \le \sup_{\tau_{A_n}}
.\end{align*}
then since clearly $\tau_{A_n} \le  \tau_{A_n+1}$ the following holds
\begin{align*}
  \lim_{n \to \infty} \tau_{A_n} = \tau_A
.\end{align*}
the first direction holds immediately since for any $n$ we must have that
\begin{align*}
  \tau_{A_n} \le   \tau_A
.\end{align*}
And for the second we consider 
\begin{align*}
    d(X_T,A) = \inf_{y \in  A} \abs{X_T - y} \le  \abs{X_T -X_{t_n}} + \abs{X_{t_n} - y}  \le  \abs{X_T -X_{\tau _n}} + \frac{1}{n}
.\end{align*}
which goes to $0$ for $n \to \infty$

\section*{2}
\subsection*{2.1}
Do not forget to show the integrability of the processes, besides that its just using smart 0 to get the result one wants,
at (iii) one has to recognize that the mean of functions of equal distribution are the same and then 
\begin{align*}
  B_s  =B_s - B_0 \sim \mathcal{N}(0,s)
.\end{align*}
\subsection*{2.2}
For (i) the direction  indistinguishable $\implies$  modification is trivial, for the other way we recognize that 
\begin{align*}
  A = \{X_t = Y_t \ ,\ \forall t \in [0,T] \cap \mathbb{Q} \}   = \bigcap_{t \in [0,T] \cap \mathbb{Q}} \{X_t = Y_t\} 
.\end{align*}
Then $A^{c} $ is a null set by property of being a union of null sets.\\
I.e for $\omega  \in  A$ we already have for rational times $t$
\begin{align*}
  X_t(\omega ) = Y_t(\omega )
.\end{align*}
For real times $t$ we argue by 
\begin{align*}
  X_t(\omega ) = \lim_{k\to \infty} X_{q_k}(\omega ) = \lim_{k\to \infty} Y_{q_k}(\omega ) = Y_t(\omega )
.\end{align*}
\textcolor{Red}{BIG ISSUE WITH THE ABOVE} \\
When talking in the language of probability one always needs to consider that everything is only defined up to null sets, i.e the $\omega  \in  A$ is not guaranteed 
to also be in $\omega  \myS{?}{\in } \{X \text{right continuous}\}  $. which is why we need to consider $\omega  \in  A \cap B \cap C$ where $B,C$ guarantee 
that we can perform the operations.
\subsection*{2.3}
(i) is an ok assumption to make since we otherwise consider the shifted process $M_t = M_t - \E[M_t]$,
(ii) The telescoping argument here is fairly important, since its a common tool, and then we can proceed from there by letting go $n\to \infty$ see 
\begin{align*}
  M_t - M_0 = \sum_{i=1}^{n} (M_{t_i}-M_{t_{i-1}}) 
.\end{align*}
As $n\to \infty$ 
\begin{align*}
  M_t - M_0 = \int_0^{t} dM_t  
.\end{align*}
same argument is used later in proving Itos formula. \\[1ex]
For (iii) we just consider the limit to $n \to \infty$ and then argue by DCT (we can bound like the following )
\begin{align*}
  (M_{t_i} - M_{t_{i-1}})^2 = (M_{t_i} - M_{t_{i-1}})(M_{t_i} - M_{t_{i-1}}) \le \sup_{i} (M_{t_i} - M_{t_{i-1}}) * \abs{M_{t_i} - M_{t_{i-1}}}
.\end{align*}
For (iv) we use fatou, also a natural bound
\begin{lemma}
  Let $(X_t)_{t \in  [0,T]}$  be an It\^o process with representations 
  \begin{align*}
    X_t = X_{0} + \int_0^{t} a(*,s) ds + \int_0^{t} b(*,s) dB_s = \tilde{X}_0 + \int_0^{t} \tilde{a}(*,s) ds + \int_0^{t} \tilde{b}(*,s)dB_s      
  .\end{align*}
  $X_0 = \tilde{X}_0 $, then $a = \tilde{a} $ and $b = \tilde{b} $
\end{lemma}
\begin{proof}
 We have 
 \begin{align*}
  0 =  \int_0^{t} a(*,s) - \tilde{a}(*,s)  +   \int_0^{t} b(*,s)-\tilde{b}(*,s)  dB_s
 .\end{align*}
 Which follows by taking the difference, i.e 
 \begin{align*}
   \int_0^{t} a(*,s) - \tilde{a}(*,s)  = -  \int_0^{t} b(*,s)-\tilde{b}(*,s)  dB_s
 .\end{align*}
  This is a local martingale that is continuous and of finite variation
\end{proof}
Let us prove that 
\begin{align*}
  (\int_0^{t} a(*,s) ds )
.\end{align*}
is of finite variation
\begin{proof}
 Define  
\begin{align*}
  A_t = \int_0^{t} a(*,s) ds 
.\end{align*}
We consider 
\begin{align*}
  \lim_{n \to \infty} \sum_{i=1}^{n} A_{t_i} - A_{t_{i-1}} &=  \lim_{n \to \infty} \sum_{i=1}^{n} \int_0^{t_i} a(*,s) ds - \int_0^{t_{i-1}} a(*,s) ds  \\
                                                           &=  \lim_{n \to \infty} \sum_{i=1}^{n} \int_{t_{i-1}}^{t_i}  a(*,s) ds\\
                                                           &\le   \lim_{n \to \infty} \sum_{i=1}^{n} \sup_{t \in [t_{i-1},t_i]} \abs{a(*,s)}(t_{i}-t_{i-1}) \\
                                                           &\le   \lim_{n \to \infty} \sum_{i=1}^{n} \sup_{t \in [0,T]} \abs{a(*,s)}(t_{i}-t_{i-1}) \\
                                                           &\le   \lim_{n \to \infty} C \sum_{i=1}^{n} (t_{i}-t_{i-1}) \\
                                                           &< \infty
.\end{align*}
\begin{align*}
  \braket{A}_t = \lim_{n\to \infty} \sum_{i=1}^{n} (A_{t}-A_{t-1})^2  &= \lim_{n \to \infty} \sum_{i=1}^{n} (\int_0^{t_i} a(*,s) ds - \int_0^{t_{i-1}} a(*,s) ds)^2  \\
                                                                      &= \lim_{n \to \infty} \sum_{i=1}^{n} (\int_{t_{i-1}}^{t_i} a(*,s) ds)^2  \\
                                                                      &\le  \lim_{n \to \infty} \sum_{i=1}^{n} (t_i-t_{i-1})\int_{t_{i-1}}^{t_i} a(*,s)^2 ds  \\
                                                                      &=    (T) \lim_{n\to \infty} \sum_{i=1}^{n}
.\end{align*}
\begin{align*}
  (\int_{t_{i-1}}^{t_i} \abs{a(*,s)*1} ds)^2 \le (\int_{t_{i-1}}^{t_i} \abs{a(*,s)}^2 ds) ds *\int_{t_{i-1}}^{t_i}  1^2 = (t_{i}-t_{i-1})\int_{t_{i-1}}^{t_i} \abs{a(*,s)}^2 ds ds
.\end{align*}
\end{proof}
\begin{lemma}
Show 
\begin{align*}
  \abs{g}_t  = \sup_{\Pi } \sum_{J \in  \Pi }\abs{\Delta_{J \cap [0,t]} g} = \lim_{n\to \infty} \sum_{J \in  \Pi_n} \abs{\Delta_{J \cap [0,t]} g}
.\end{align*}
For a zero sequence of partitions 
\end{lemma}
\begin{proof}
  Let $(\Pi )_{n \in  \mathbb{N}}$ be a zero-sequence of partitions and define
 \begin{align*}
   \abs{g}_t^n =  \sum_{J \in  \Pi^n}\abs{\Delta_{J \cap [0,t]} g} 
 .\end{align*}
 then showing that 
 \begin{align*}
   \abs{g}_t^{n+1} \ge    \abs{g}_t^{n}
 .\end{align*}
And 
\begin{align*}
  \sup_{\Pi } \sum_{J \in  \Pi }\abs{\Delta_{J \cap [0,t]} g} \ge  \abs{g}_t^{n}
.\end{align*}
Gives
\begin{align*}
  \lim_{n \to \infty} \abs{g}_t^{n} = \sup_{\Pi } \sum_{J \in  \Pi }\abs{\Delta_{J \cap [0,t]} g}
.\end{align*}
\end{proof}
\newpage
\begin{exercise}
 Show that the difference between the Lefthand Limit and Right hand limit is not necessarily the same for a Continuos function
 $f \in  \mathcal{C}^{1} $ and a Process $B_t$ , with partition $\Delta_n = \{0=t_0<\ldots , < t_n = T\}  $
 \begin{align*}
   L_n &= \sum_{i=1}^{n} f(B_{t_{i-1}})(B_{t_i}-B_{t_{i-1}})\\
   R_n &= \sum_{i=1}^{n} f(B_{t_i})(B_{t_i}-B_{t_{i-1}})
 .\end{align*}
\end{exercise}
\begin{proof}
 We have 
 \begin{align*}
   \abs{L_n - R_n} &= \abs{\sum_{i=1}^{n} (f(B_{t_{i-1}})-f(B_{t_i}))(B_{t_i}-B_{t_{i-1}})} \\ 
                   &\le   \sqrt{ \sum_{i=1}^{n} f(B_{t_{i-1}}-f(B_{t_i}))^2}*\sqrt{\sum_{i=1}^{n} (B_{t_i}-B_{t_{i-1}})^2} \\
                   &\le   C * \sqrt{\sum_{i=1}^{n} (B_{t_i}-B_{t_{i-1}})^2}
 .\end{align*}
 If the quadratic variation of $B_t$ is nonzero and finite , then there exists cases where $\abs{L_n - R_n} \neq 0$
\end{proof}
\begin{exercise}
 Stochastic Integral as limit of simple functions is well defined, i.e. does not depend on the choice of limiting sequence. 
\end{exercise}
\begin{proof}
  Let $f \in  \mathcal{H}^{2} $ and take two sequences $(f_n),(g_n) \subset  \mathcal{H}^{2}_0 $ such that 
  \begin{align*}
    \|f_n - f\|_{\mathcal{H}^2} \to  0 \text{ and } \|g_m - f\|_{\mathcal{H}^2} \to  0
  .\end{align*}
  We check 
  \begin{align*}
    \|f_n - g_m\| = \|f_n - f + f-g_m\| &=   \E[\int_0^{t} (f_n - f + f-g_m)^2  ]\\
                                        &\le \E[\int_0^{t} 2*(f_n-f)^2 + 2*(f-g_m)^2 ] \\
                                        &= 2*(\|f_n - f\| + \|f-g_m\|) \xrightarrow{n,m\to \infty} 0
  .\end{align*}
\end{proof}
\begin{exercise}
 Calculate 
 \begin{align*}
  \int_a^{b} B(t)dB(t) 
 .\end{align*}
\end{exercise}
\begin{proof}
 We prove this by picking $f_n$ such that 
 \begin{align*}
  f_n \to  B(t)
 .\end{align*}
 then we have 
 \begin{align*}
   \int_{a}^{b}  B(t) dB(t) = \lim_{n \to \infty} I(f_n) = \lim_{n \to \infty} \sum_{i=1}^{n}  f_n(B(t_{i-1}))(B(t_i)-B(t_{i-1}))
 .\end{align*}
 Clearly 
 \begin{align*}
   f_n(t,\omega ) = B(t_{i-1})*\cha_{[t_{i-1},t_i)}
 .\end{align*}
 Converges to $B(t)$ for a partition $\Delta_n$, we get
 \begin{align*}
   \int_{a}^{b}  B(t) dB(t) = \lim_{n\to \infty} \sum_{i=1}^{n} f_n(B(t_{i-1}))(B(t_i)-B(t_{i-1})) = B(t_{i-1}(B(t_i)-B(t_{i-1})))
 .\end{align*}
 Where we note that 
 \begin{align*}
   L_n &= \sum_{i=1}^{n} f_n(B(t_{i-1}))(B(t_i)-B(t_{i-1}))\\
  R_n &= \sum_{i=1}^{n} f_n(B(t_{i}))(B(t_i)-B(t_{i-1}))
 .\end{align*}
 Such that 
 \begin{align*}
   R_n - L_n &=  \sum_{i=1}^{n} (f_n(B(t_{i}))-f_n(B_{t_{i-1}}))(B(t_i)-B(t_{i-1})) = \sum_{i=1}^{n} (B(t_i)-B(t_{i-1}))^2 \\
   R_n + L_n &=  \sum_{i=1}^{n} (f_n(B(t_{i}))+f_n(B_{t_{i-1}}))(B(t_i)-B(t_{i-1})) = \sum_{i=1}^{n} (B(t_i)^2-B(t_{i-1})^2) = B(b)^2 - B(a)^2\\
 .\end{align*}
 Then 
 \begin{align*}
   L_n =  \frac{1}{2} (B(b)^2-B(a)^2 -\sum_{i=1}^{n} (B(t_i)-B(t_{i-1}))^2 )
 .\end{align*}
 As $n\to \infty$
 \begin{align*}
   \lim_{n\to \infty} L_n  = \frac{1}{2}(B(b)^2-B(a)^2 -  (b-a))
 .\end{align*}
 Plugging in $b=t$ and $a=0$
 \begin{align*}
  \frac{1}{2}(B(t)^2-t)
 .\end{align*}
\end{proof}
\begin{exercise}
  Show quadratic variation of $B(t)$  over $[a,b]$ is $b-a$
\end{exercise}
\begin{proof}
  First way is by $L^2$ convergence 
 \begin{align*}
   \phi_n = \sum_{i=1}^{n} \underbrace{(B(t_i)-B(t_{i-1}))^2 - (\sum_{i=1}^{n}  t_i-t_{i-1})}_{X_i}
 .\end{align*} 
 Then 
 \begin{align*}
   \E[\phi_n^2] = \sum_{i,j=1}^{n} \E[X_iX_j]
 .\end{align*}
 For $i\neq j$ 
 \begin{align*}
   \E[X_iX_j] &=  \E[((B(t_i)-B(t_{i-1}))^2 - (t_i-t_{i-1}))((B(t_j)-B(t_{j-1}))^2 - (t_j-t_{j-1}))]\\
              &= \E[(B(t_i)-B(t_{i-1}))^2(B(t_j)-B(t_{j-1}))^2] \ldots  
 .\end{align*}
\end{proof}

